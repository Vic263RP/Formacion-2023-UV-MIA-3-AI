{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_v1_batch_LunarLander.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNOEGKuM67qyNYuL7CFYhVY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CWsxkki8jh8A"},"source":["# DQN v1 batch"]},{"cell_type":"code","source":["!pip3 install box2d-py\n","!pip3 install gym[Box_2D]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1Q9jfiM0fSa","executionInfo":{"status":"ok","timestamp":1652218163113,"user_tz":-120,"elapsed":14617,"user":{"displayName":"Raul Vica","userId":"03347371194950353448"}},"outputId":"6a67c8e7-3eae-4c04-aa77-f8962a3bf290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"]}]},{"cell_type":"code","metadata":{"id":"fSeTTLadjXBg"},"source":["import gym\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","import matplotlib.pyplot as plt\n","import random\n","from collections import deque\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wn0-uTtqkPeC"},"source":["Disable GPU computation for local devices\n"]},{"cell_type":"code","metadata":{"id":"z-dueBMmj8QY"},"source":["# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Utils function to transform discrete states into one-hot vector"],"metadata":{"id":"2gyC2ETeG6AC"}},{"cell_type":"code","source":["def discrete_input(state_discrete: tuple, env_dim: gym.spaces.tuple.Tuple):\n","    one_hot_state = []\n","    for i_pos, dim in zip(state_discrete, env_dim):\n","        temp = np.zeros(dim.n)\n","        temp[i_pos] = 1\n","        one_hot_state.append(temp)\n","\n","    return np.concatenate(one_hot_state)"],"metadata":{"id":"irCOH5cNG49D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gvh2VHsje3w"},"source":["## Gym selection and basic configurations"]},{"cell_type":"code","metadata":{"id":"3A722SC0km68"},"source":["gym_name_list = [\n","    {\n","        'name': 'CartPole-v0',\n","        'goal': 180,\n","        'v_min': 0,\n","        'v_max': 210,\n","        'ep': 50\n","    },\n","    {\n","        'name': 'MountainCar-v0',\n","        'goal': -150, \n","        'v_min': -210,\n","        'v_max': 0,\n","        'ep': 20\n","    },\n","    {\n","        'name': 'Blackjack-v0',\n","        'goal': 0.10,\n","        'v_min': -20,\n","        'v_max': 20,\n","        'ep': 1000\n","    },\n","    {\n","        'name': 'LunarLander-v2',\n","        'goal': 200,\n","        'v_min': 250,\n","        'v_max': -250,\n","        'ep': 50\n","    }\n","]\n","\n","env_i = 3 #@param {type:\"slider\", min:0, max:3, step:1}\n","\n","save_model: bool = False # @param {type:\"boolean\"}\n","show_plots: bool = True # @param {type:\"boolean\"}\n","render_env: bool = False # @param {type:\"boolean\"}\n","seed = 42 # @param {type:\"integer\"}\n","\n","max_steps_per_episode = 400 # @param {type:\"integer\"}\n","\n","stopping_reward_criteria = gym_name_list[env_i]['goal']\n","\n","gym_name = gym_name_list[env_i]['name']\n","\n","env = gym.make(gym_name)  # Create the environment\n","\n","env.seed(seed)\n","\n","if isinstance(env.observation_space, gym.spaces.tuple.Tuple):\n","    env = gym.wrappers.TransformObservation(env, lambda obs: discrete_input(obs, env.observation_space))\n","    num_inputs = sum([x.n for x in env.observation_space])  # 4\n","else:\n","    num_inputs = env.observation_space.shape[0]  # 4\n","num_actions = env.action_space.n  # 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWXm85B4lIJ7"},"source":["## Algorithm hyper-parameters"]},{"cell_type":"code","metadata":{"id":"VqDFkMCflFY-"},"source":["# Factor of the ema that displays that tracks the averaged rewards\n","ema_ratio = 0.01  # @param {type:\"number\"}\n","\n","# Ratio between generating experiences and sampling for training\n","training_ratio: int = 4 # @param  {type:\"integer\"}\n","\n","# Size of the batch when sampling experiences\n","batch_size: int = 32 # @param {type:\"integer\"}\n","\n","# Size of the buffer that stores the experiences\n","mem_length: int = 4096 # @param {type:\"integer\"}\n","\n","# Discount factor for estimaing the futures rewards\n","gamma: float = 0.99  # @param {type:\"number\"}\n","\n","# Initial and last probability for choosing exploration instead of explotation\n","epsilon: float = 1.0 # @param {type:\"number\"}\n","epsilon_min: float = 0.05 # @param {type:\"number\"}\n","\n","# This is an estimation of the training iterations to tune the epsilon decay\n","approx_iterations: float = 5e6 # @param {type:\"number\"}\n","\n","# The epsilon_decay reduce the exploration probability after each iteration\n","epsilon_decay: float = (epsilon_min / epsilon) ** (1 / approx_iterations)\n","\n","# Factor of the ema that controls the updating weights of the target network\n","tau: float = 0.125 # @param {type:\"number\"}\n","\n","# The usual factor that controls the amount of change the weights are updated\n","learning_rate = 0.05 # @param {type:\"number\"}\n","\n","# For enabling the double dqn learning when choosing next Q-values\n","double_dqn_learning: bool = True # @param {type:\"boolean\"}\n","\n","# Factor to define heuristically the size of the hidden layer.\n","hidden_size_factor = 16 # @param {type:\"integer\"}\n","num_hidden = num_inputs * num_actions * hidden_size_factor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OT3aznNck22R"},"source":["## Load DQN models as Q-table approximators\n","We start with double DQN, where:\n","*   q_model estimates the Q-values used for action selection.\n","*   t_model is responsible for estimating the target Q values on training."]},{"cell_type":"code","metadata":{"id":"Hq3oaCbNk17Y"},"source":["q_model = keras.Sequential(layers=[layers.Input(shape=(num_inputs,)),\n","                                   layers.Dense(num_hidden, activation=\"relu\"),\n","                                   layers.Dense(num_actions)],\n","                           name=\"q_model\")\n","\n","t_model = keras.Sequential(layers=[layers.Input(shape=(num_inputs,)),\n","                                   layers.Dense(num_hidden, activation=\"relu\"),\n","                                   layers.Dense(num_actions)],\n","                           name=\"t_model\")\n","\n","optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","# loss_function = keras.losses.Huber()\n","loss_function = keras.losses.MeanSquaredError()\n","\n","\n","# Preprocessing to convert the state into a tensor\n","def state_to_tensor(state):\n","  state = tf.convert_to_tensor(state)\n","  state = tf.expand_dims(state, 0)\n","  return state\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just checking the model can be saved. Not needed for the training."],"metadata":{"id":"rRTM_7B7B7Hn"}},{"cell_type":"code","source":["# implementation = \"DQN_v1_LunarLanding_v2\"\n","# episode_count = 0\n","# model_folder = os.path.join(\"./models\", gym_name, implementation, \"Ep_\" + str(episode_count).zfill(5), \"model\")\n","# if not os.path.exists(model_folder):\n","#   os.makedirs(model_folder)\n","# q_model.save_weights(filepath=model_folder, save_format=\"tf\")"],"metadata":{"id":"uvsu574Kg4TV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train model (agent) from past experiences"],"metadata":{"id":"9xK4l1ECOJkO"}},{"cell_type":"code","source":["def train_agent(samples):\n","  state_batch = tf.concat([s for s, a, r, n_s, d in samples], axis=0)\n","  action_batch = tf.concat([a for s, a, r, n_s, d in samples], axis=0)\n","  reward_batch = tf.cast(tf.concat([r for s, a, r, n_s, d in samples], axis=0),\n","                         dtype=tf.float32)\n","  next_state_batch = tf.concat([n_s for s, a, r, n_s, d in samples], axis=0)\n","  not_done_batch = tf.concat([float(not d) for s, a, r, n_s, d in samples], axis=0)\n","\n","  # Create a mask so we only calculate loss on the updated Q-values\n","  masks = tf.one_hot(action_batch, num_actions)\n","\n","  # Build the updated Q-values for the sampled future states\n","  # Use the target model for stability\n","  future_t = t_model(next_state_batch)\n","\n","  if double_dqn_learning:\n","      future_q = q_model(next_state_batch)\n","      best_future_action = tf.argmax(future_q, axis=-1)\n","      next_action_mask = tf.one_hot(best_future_action, num_actions)\n","      future_q_action = tf.reduce_sum(tf.multiply(future_t, next_action_mask), axis=1)\n","  else:\n","      future_q_action = tf.reduce_max(future_t, axis=1)\n","\n","  # Q value = reward + discount factor * expected future reward\n","  updated_q_values = reward_batch + gamma * tf.multiply(future_q_action, not_done_batch)\n","\n","  with tf.GradientTape() as tape:\n","      q_values = q_model(state_batch)\n","\n","      # Apply the masks to the Q-values to get the Q-value for action taken\n","      q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n","      loss = loss_function(q_action, updated_q_values)\n","\n","      # Backpropagation\n","      grads = tape.gradient(loss, q_model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, q_model.trainable_variables))\n","\n","  return loss.numpy()"],"metadata":{"id":"wtxV91tzOKt9","executionInfo":{"status":"ok","timestamp":1652220608675,"user_tz":-120,"elapsed":234,"user":{"displayName":"Raul Vica","userId":"03347371194950353448"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["### Transfer weights to target model"],"metadata":{"id":"AMqQQHH5OyHY"}},{"cell_type":"code","source":["def learning_transfer(q_model, t_model, tau):\n","  weights = q_model.get_weights()\n","  target_weights = t_model.get_weights()\n","  for j in range(len(target_weights)):\n","      target_weights[j] = weights[j] * tau + target_weights[j] * (1 - tau)\n","  t_model.set_weights(target_weights)\n"],"metadata":{"id":"mpPuNp2_O40V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Epsilon-greedy function to select an action"],"metadata":{"id":"WeATMsVgNywe"}},{"cell_type":"code","source":["def agent_best_action(state):\n","  return np.argmax(q_model(state, training=False))\n","\n","def select_action(state, epsilon):\n","  epsilon *= epsilon_decay\n","  epsilon = max(epsilon_min, epsilon)\n","  \n","  if np.random.random() < epsilon:\n","      action = env.action_space.sample()\n","  else:\n","      action = agent_best_action(state)\n","  return action, epsilon"],"metadata":{"id":"UGjx7uYhN5Ku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2H04rFNqdNg"},"source":["## Tensorboard configuration"]},{"cell_type":"code","metadata":{"id":"JyP6U0yKq-Ez"},"source":["implementation = \"DQN_v1_LunarLanding_v2\"\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","train_log_dir = os.path.join(\"logs\", gym_name, implementation, \"T_\" + current_time)\n","summary_writer = tf.summary.create_file_writer(train_log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":874},"id":"VkJuXmNph9wb","executionInfo":{"status":"ok","timestamp":1652220979337,"user_tz":-120,"elapsed":227,"user":{"displayName":"Raul Vica","userId":"03347371194950353448"}},"outputId":"451f6616-af3d-439c-cf07-03dcd4e53d56"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# from tensorboard import notebook\n","# notebook.list() # View open TensorBoard instances\n","\n","# # Control TensorBoard display. If no port is provided, \n","# # the most recently launched TensorBoard is used\n","# notebook.display(port=6006, height=1000) \n","\n","%tensorboard --logdir ./logs\n","\n","# # IF TENSORBOARD DOES NOT LOAD TRY THIS:\n","# %reload_ext tensorboard\n","# %tensorboard --logdir ./logs"],"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"output_type":"display_data","data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 400), started 0:49:27 ago. (Use '!kill 400' to kill it.)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"e0Budp8owKeX"},"source":["## Main learning loop\n","It consist in main two steps:\n","\n","1.   Collect experiences from the environment in episodes, with exploitation/exploration trade-off\n","2.   Sample past experiences to train the model using the Bellman equation.\n","\n","Other sections are the mean reward tracking, stop-learning trigger, display status.\n"]},{"cell_type":"code","metadata":{"id":"QG_ZMJ0NwVNl","colab":{"base_uri":"https://localhost:8080/","height":550},"outputId":"e293afd7-49a9-46aa-9069-8e7ff6496fd0","executionInfo":{"status":"error","timestamp":1652220818761,"user_tz":-120,"elapsed":206384,"user":{"displayName":"Raul Vica","userId":"03347371194950353448"}}},"source":["memory = deque(maxlen=mem_length)\n","running_reward = None\n","episode_count = 0\n","epoch = 0\n","historic_reward = []\n","while True:  # Run until solved\n","\n","  state = env.reset()\n","  state = state_to_tensor(state)\n","\n","  episode_reward = 0\n","  for time_step in range(1, max_steps_per_episode):\n","    # env.render(); Adding this line would show the attempts\n","    # of the agent in a pop up window.\n","\n","    action, epsilon = select_action(state, epsilon)\n","\n","    # Apply the sampled action in our environment\n","    next_state, reward, done, _ = env.step(action)        \n","    next_state = state_to_tensor(next_state)\n","\n","    memory.append([state, action, reward, next_state, done])\n","    episode_reward += reward\n","\n","    # ##### TRAIN MODEL ###############\n","    if len(memory) >= 2 * batch_size and time_step % training_ratio == 0:\n","      samples = random.sample(memory, batch_size)\n","\n","      loss = train_agent(samples)\n","\n","      # Transfer weights to target model\n","      learning_transfer(q_model, t_model, tau)\n","\n","      if running_reward is not None:\n","        with summary_writer.as_default():\n","          tf.summary.scalar('loss', loss, step=epoch)\n","          tf.summary.scalar('ema_reward', running_reward, step=epoch)\n","          tf.summary.scalar('epsilon', epsilon, step=epoch)\n","        epoch += 1\n","    state = next_state\n","\n","    if done:\n","      break\n","\n","  if running_reward is None:\n","    running_reward = episode_reward\n","\n","  # Update running reward to check condition for solving\n","  running_reward = ema_ratio * episode_reward + (1 - ema_ratio) * running_reward\n","  historic_reward.append(running_reward)\n","\n","  # Log details\n","  episode_count += 1\n","  if episode_count % gym_name_list[env_i]['ep'] == 0 and 'loss' in locals():\n","    template = \"running reward: {:.2f} at episode {} with epsilon {:.2f} and loss {:.2f}\"\n","    print(template.format(running_reward, episode_count, epsilon, loss))\n","\n","  # Condition to consider the task solved\n","  if running_reward > stopping_reward_criteria:\n","    print(\"Solved at episode {}!\".format(episode_count))\n","    break\n","\n","  if show_plots and episode_count % 10000000 == 0:\n","    plt.plot(historic_reward)\n","    plt.show()"],"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["running reward: -137.19 at episode 50 with epsilon 0.98 and loss 905.21\n","running reward: -156.45 at episode 100 with epsilon 0.98 and loss 5952.27\n","running reward: -176.62 at episode 150 with epsilon 0.97 and loss 2913.18\n","running reward: -170.99 at episode 200 with epsilon 0.97 and loss 13543.19\n","running reward: -177.38 at episode 250 with epsilon 0.97 and loss 3829.33\n","running reward: -182.64 at episode 300 with epsilon 0.96 and loss 27144.95\n","running reward: -186.00 at episode 350 with epsilon 0.96 and loss 99537.86\n","running reward: -197.66 at episode 400 with epsilon 0.96 and loss 2528225.25\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-c463cffdcbc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;31m# Transfer weights to target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0mlearning_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-663ce5f80964>\u001b[0m in \u001b[0;36mlearning_transfer\u001b[0;34m(q_model, t_model, tau)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mtarget_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0mweight_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[0;31m# Perform any layer defined finalization of the layer state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   4017\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4019\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4020\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4021\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    951\u001b[0m           self.handle, value_tensor, name=name, **kwargs)\n\u001b[1;32m    952\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_lazy_read\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mdeleter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_deleter\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_graph_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mparent_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         unique_id=self._unique_id)\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, dtype, shape, in_graph_mode, deleter, parent_op, unique_id)\u001b[0m\n\u001b[1;32m   2134\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0mhandle_deleter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m         graph_element=graph_element)\n\u001b[0m\u001b[1;32m   2137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, shape, dtype, handle, constraint, synchronization, aggregation, distribute_strategy, name, unique_id, handle_name, graph_element, initial_value, initializer_op, is_initialized_op, cached_value, save_slice_info, handle_deleter, caching_device, in_graph_mode, **unused_kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         variables.validate_synchronization_aggregation_trainable(\n\u001b[1;32m    440\u001b[0m             synchronization, aggregation, trainable, name))\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_synchronization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynchronization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"3RoY6P3OzRyy"},"source":["## Save model weights for a later use (optional)."]},{"cell_type":"code","metadata":{"id":"We2l8xxlznvz"},"source":["if save_model:\n","  model_folder = os.path.join(\"./models\", gym_name, implementation, \"Ep_\" + str(episode_count).zfill(5), \"model\")\n","  if not os.path.exists(model_folder):\n","    os.makedirs(model_folder)\n","  q_model.save_weights(filepath=model_folder, save_format=\"tf\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwgdrS-SzuCZ"},"source":["## Play with the trained agent"]},{"cell_type":"code","metadata":{"id":"akp6d5gdz2zj"},"source":["episodes = 100 # @param {type:\"integer\"}\n","deterministic = True # @param {type:\"boolean\"}\n","\n","agent_rewards = []\n","for env_i in range(episodes):\n","  state = env.reset()\n","  episode_reward = 0\n","\n","  for time_step in range(1, max_steps_per_episode):\n","    if render_env and gym_name != 'Blackjack-v0':\n","      env.render()  # Show the attempts of the agent in a pop up window.\n","\n","    state = state_to_tensor(state)\n","\n","    if deterministic:\n","      action = agent_best_action(state)\n","    else:\n","      action, epsilon0 = select_action(state, 0.2)\n","\n","    # Apply the sampled action in our environment\n","    state, reward, done, _ = env.step(action)\n","    episode_reward += reward\n","\n","    if done:\n","      break\n","  agent_rewards.append(episode_reward)\n","  \n","print(f\"After 100 episodes the mean reward is {np.mean(agent_rewards)}\")\n","\n","if show_plots:\n","  num_bins = 50\n","  x = np.array(agent_rewards)\n","  fig, ax = plt.subplots()\n","\n","  # the histogram of the data\n","  n, bins, patches = ax.hist(x, num_bins, density=1)\n","\n","  ax.set_xlabel('Episode rewards')\n","  ax.set_ylabel('Probability density')\n","  ax.set_title(f'Mean {np.mean(x).round(2)} +/- {np.std(x).round(2)}')\n","\n","  # Tweak spacing to prevent clipping of ylabel\n","  fig.tight_layout()\n","  plt.show()\n","\n","print(\"End of script!\")"],"execution_count":null,"outputs":[]}]}