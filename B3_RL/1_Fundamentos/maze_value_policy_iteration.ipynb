{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo del laberinto. Implementación del algoritmo de Evaluación de Políticas.\n",
    "\n",
    "Vamos a revisar esta implementación del ejercicio del laberinto que hemos visto en las diapositivas y que procede del libro de Sutton & Barto. Aquí vamos a utilizar la convergencia iterativa de las ecuaciones de Bellman para resolver el mismo ejercicio del laberinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Definimos las estructuras de datos:  \n",
    " * Definimos las coordenadas de las posiciones especiales A, B, A', B'. \n",
    "   Ten en cuenta que los índices comienzan por 0 en python\n",
    " * Definimos  la variable DISCOUNT que es nuestra Gamma de teoria a un valor 0.9\n",
    " * La variable WORL_SIZE nos define un tablero o matriz de 5x5\n",
    " * El vector Actions define con dos códigos el cambio de índices en el tablero al moverse \n",
    "   ejemplo: estoy en [2,2], elijo la accion 'left', con  lo que sumo [2,2] + [0,-1] = [2,1] y estoy \n",
    "   una posición a la izquierda\n",
    " * La constante ACTION_PROB es la probabilidad de cada acción\n",
    " \n",
    "    -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "WORLD_SIZE = 5\n",
    "A_POS = [0, 1]\n",
    "A_PRIME_POS = [4, 1]\n",
    "B_POS = [0, 3]\n",
    "B_PRIME_POS = [2, 3]\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# left, up, right, down\n",
    "ACTIONS = [np.array([0, -1]),\n",
    "           np.array([-1, 0]),\n",
    "           np.array([0, 1]),\n",
    "           np.array([1, 0])]\n",
    "ACTIONS_FIGS=[ '←', '↑', '→', '↓']\n",
    "\n",
    "\n",
    "ACTION_PROB = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función 'step' implementa el paso elemental $s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$\n",
    "Si el estado actual es 'A' o 'B', salto a los puntos 'A'' o 'B'' y devuelvo la recompensa +10 ó +5 \n",
    "Sino, hago el movimiento, sumándole al estado la acción (como hemos explicado antes) y compruebo si\n",
    "me he salido fuera del tablero.\n",
    "    -Si he salido, devuelvo -1 y vuelvo al estado anterior\n",
    "    -Si no he salido, devuelvo 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    if state == A_POS:\n",
    "        return A_PRIME_POS, 10\n",
    "    if state == B_POS:\n",
    "        return B_PRIME_POS, 5\n",
    "\n",
    "    next_state = (np.array(state) + action).tolist()\n",
    "    x, y = next_state\n",
    "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "        reward = -1.0\n",
    "        next_state = state\n",
    "    else:\n",
    "        reward = 0\n",
    "    return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos funciones (celdas de código) siguientes no tienen interés algorítmico. Son para dibujar la imagen de los resultados del tablero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = image.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(image):\n",
    "\n",
    "        # add state labels\n",
    "        if [i, j] == A_POS:\n",
    "            val = str(val) + \" (A)\"\n",
    "        if [i, j] == A_PRIME_POS:\n",
    "            val = str(val) + \" (A')\"\n",
    "        if [i, j] == B_POS:\n",
    "            val = str(val) + \" (B)\"\n",
    "        if [i, j] == B_PRIME_POS:\n",
    "            val = str(val) + \" (B')\"\n",
    "        \n",
    "        tb.add_cell(i, j, width, height, text=val,\n",
    "                    loc='center', facecolor='white')\n",
    "        \n",
    "\n",
    "    # Row and column labels...\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "\n",
    "    ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_policy(optimal_values):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = optimal_values.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(optimal_values):\n",
    "        next_vals=[]\n",
    "        for action in ACTIONS:\n",
    "            next_state, _ = step([i, j], action)\n",
    "            next_vals.append(optimal_values[next_state[0],next_state[1]])\n",
    "\n",
    "        best_actions=np.where(next_vals == np.max(next_vals))[0]\n",
    "        val=''\n",
    "        for ba in best_actions:\n",
    "            val+=ACTIONS_FIGS[ba]\n",
    "        \n",
    "        # add state labels\n",
    "        if [i, j] == A_POS:\n",
    "            val = str(val) + \" (A)\"\n",
    "        if [i, j] == A_PRIME_POS:\n",
    "            val = str(val) + \" (A')\"\n",
    "        if [i, j] == B_POS:\n",
    "            val = str(val) + \" (B)\"\n",
    "        if [i, j] == B_PRIME_POS:\n",
    "            val = str(val) + \" (B')\"\n",
    "        \n",
    "        tb.add_cell(i, j, width, height, text=val,\n",
    "                loc='center', facecolor='white')\n",
    "\n",
    "    # Row and column labels...\n",
    "    for i in range(len(optimal_values)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
    "                   edgecolor='none', facecolor='none')\n",
    "\n",
    "    ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este es el meollo\n",
    "La función 'figure_3_2()' implementa el algoritmo 'Iterative Policy Evaluation'\n",
    "La función figure_3_5() implementa el método de optimización del algoritmo 'Value Iteration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta función EVALUA la política consistente en asignar 0.25 de probabilidad cada acción\n",
    "def figure_3_2():\n",
    "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    while True:\n",
    "        # keep iteration until convergence\n",
    "        new_value = np.zeros_like(value)\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    # Backup de la función de Bellman para la politica descrita\n",
    "                    new_value[i, j] += ACTION_PROB * (reward + DISCOUNT * value[next_i, next_j])\n",
    "        if np.sum(np.abs(value - new_value)) < 1e-4:\n",
    "            draw_image(np.round(new_value, decimals=2))\n",
    "            plt.savefig('imagesfigure_3_2.png')\n",
    "            plt.close()\n",
    "            break\n",
    "        value = new_value\n",
    "\n",
    "\n",
    "#Value Iteration. Esta función es UNA OPTIMIZACION, es decir , encuentra la política óptima\n",
    "def figure_3_5():\n",
    "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    while True:\n",
    "        # keep iteration until convergence\n",
    "        new_value = np.zeros_like(value)\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                values = []\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    # value iteration\n",
    "                    #Fijaros que aquí estamos guardandonos todos los backups \n",
    "                    #correspondientes a las 4 acciones. NO ACTUALIZAMOS LA FUNCION DE VALOR TODAVIA\n",
    "                    values.append(reward + DISCOUNT * value[next_i, next_j])\n",
    "                    \n",
    "                #ESTA INSTRUCCION RECOGE EL MAXIMO DE LOS POSIBLES RETORNOS. \n",
    "                #ESTA INSTRUCCION ES LA QUE HACE POSIBLE LA OPTIMIZACION\n",
    "                new_value[i, j] = np.max(values)\n",
    "        if np.sum(np.abs(new_value - value)) < 1e-4:\n",
    "            draw_image(np.round(new_value, decimals=2))\n",
    "            plt.savefig('figure_3_5.png')\n",
    "            plt.close()\n",
    "            draw_policy(new_value)\n",
    "            plt.savefig('figure_3_5_policy.png')\n",
    "            plt.close()\n",
    "            break\n",
    "        value = new_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    figure_3_2()\n",
    "    #figure_3_5()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.\n",
    "Completa el código siguiente para implementar el algoritmo de Policy Iteration que maximiza la funcion de valor. Este algoritmo esta separado en dos partes: \n",
    "- 1. Policy evaluation (que obtiene la funcion de valor de la politica actual en cada iteracion) Nota: Daros cuenta que la politica actual CAMBIA en cada iteracion debido a la segunda parte del algoritmo.\n",
    "- 2. Policy Improvement (que construye una politica greedy a partir de la funcion de valor actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    #Ahora, como la politica no es constante como en \"figure_3_2()\", debemos implementar una\n",
    "    #estructura de datos para representar a la politica en cada iteracion.\n",
    "    #Esta estructura es una matriz de tamaño WORLD_SIZEx WORLD_SIZE de enteros donde\n",
    "    #almacenaremos los indices de la accion para cada estado {0,1,2,3}\n",
    "    #Igual que la instruccción de arriba carga en 'policy una matriz de enteros con \n",
    "    #valores aleatorios de 0 a 3 en cada casilla\n",
    "    policy = ??\n",
    "    \n",
    "    #bandera booleana que nos permite acabar con el bucle que le sigue\n",
    "    policy_estable = False\n",
    "    while policy_estable == False:\n",
    "        # keep iteration until convergence\n",
    "        #---Comentario del Ejercicio 2---\n",
    "        #Parte 1 de Policy_Evaluation para la evaluación de la politica definida en 'policy'\n",
    "        new_value = np.zeros_like(value)\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                \n",
    "                #ATENCION: Aqui ya hay una diferencia con la policy evaluation de \"figure_3_2()\"\n",
    "                #La accion la escogemos de la matriz 'policy', porque cambia en cada iteracion\n",
    "                #Usa el indice almacenado en 'policy' para elegir la accion de 'ACTIONS'\n",
    "                action = ??  #pi(s)\n",
    "                \n",
    "                #Ejecutamos ahora el paso usando la función 'step' con la acción elegida\n",
    "                (next_i, next_j), reward = ??\n",
    "                # Actuaiza el backup de la ecuación de Bellman\n",
    "                #Fijate que usamos otra estructura para almacenar el valor nuevo\n",
    "                #porque necesitamos conservar la vieja función de valor hasta que todos\n",
    "                #los valores se hayan actualizado correctamente\n",
    "                \n",
    "                #No uses ACTION_PROB en esta actualizacion porque ESTAMOS ELIGIENDO LA POLITICA GREEDY\n",
    "                new_value[i, j] = ??\n",
    "        if np.sum(np.abs(value - new_value)) < 1e-4:\n",
    "            #draw_image(np.round(new_value, decimals=2))\n",
    "            #plt.savefig('imagesfigure_3_2.png')\n",
    "            #plt.close()\n",
    "            break\n",
    "        #Aqui actualizo la función de valor    \n",
    "        value = new_value\n",
    "      \n",
    "\n",
    "    #Parte 2 de policy_Improvement para la mejora de la politica\n",
    "    \n",
    "    \n",
    "        # keep iteration until convergence\n",
    "        policy_estable = True\n",
    "        \n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                b = policy[i,j]\n",
    "                #Ahora me quedo con la accion que maximiza la ecuacion de Bellman\n",
    "                #Nota: El codigo que sigue esta pensado en C++. No sigue la \n",
    "                #filosofia de python\n",
    "                #¿Podrias tú hacerlo mejor?\n",
    "                #Basta con almacenar en un vector los cálculos de los valores de (s,a_i)\n",
    "                #y luego asignar a la politica el índice de la acción del valor máximo\n",
    "                \n",
    "                \n",
    "                max_action_value =-1000 #inicializo el valor de accion a uno muy bajo\n",
    "                for action in ACTIONS:\n",
    "                    #Ejecuta el paso desde 's_ij' con esta acción\n",
    "                    (next_i, next_j), reward = ??\n",
    "                    \n",
    "                    #Calcula el retorno de esa acción (la formula la misma que en la parte 1)\n",
    "                    aux = ??\n",
    "                    \n",
    "                    #Esto es lo que debes de cambiar, si te parece cutre. Sino, el codigo funciona\n",
    "                    if aux > max_action_value:\n",
    "                        max_action_value = aux\n",
    "                        #Guardo en la politica el indice de la accion  máxima {o0, o1, o2, o3}\n",
    "                        policy[i,j] = next((i for i, val in enumerate(ACTIONS) if np.all(val == action)), -1)  \n",
    "                    \n",
    "                #Condición de parada    \n",
    "                if b != policy[i,j]:\n",
    "                    policy_estable = False\n",
    "                    \n",
    "       \n",
    "     #volvemos al while inicial               \n",
    "                    \n",
    "    draw_image(np.round(value, decimals=2))\n",
    "    plt.savefig('images_policy_iteration.png')\n",
    "    plt.close()\n",
    "    draw_policy(value)\n",
    "    plt.savefig('figure_policy_iteration.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui deberias comentar la función main que esta dos cajas más arriba y ponerla aquí para ejecutar tu nueva función\n",
    "\n",
    "#Igual que antes, esta función te genera dos imagenes, una con la funcion de valor máxima y otra con la \n",
    "#política óptima. Compara la solución con la proporcionada con el algoritmo de value iteration\n",
    "    policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.\n",
    "Si te fijas en el resultado que te da el algoritmo implementado \"policy_iteration()\"  de la caja anterior, observarás que,aunque se aproxima al verdadero valor óptimo de cada estado, no llega a alcanzar totalmente el verdadero valor óptimo (compáralo con la diapo 67 de la presentación donde está la solución). Esto es debido a que la parte del algoritmo donde se calcula el valor de V para la política actual no se calcula más que mediante una única iteración sobre los estados, por eficiencia. Esto es posible hacerlo ya que el algoritmo garantiza igualmente la convergencia.  Modifica esta parte (señalada como ---Comentario del Ejercicio 2---) para que sea igual que la función \"figure_3_2()\" que implementa el cálculo más aproximado y compara los resultados que obtienes ahora con el anterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
