{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling para *Big Data*\n",
    "Vamos a ver cómo realizar un modelado de temática en grandes volúmenes de texto con la librería `gensim`  \n",
    "\n",
    "Utilizaremos el conjunto de datos *Lee* de `Gensim` (es una versión abreviada del conjunto http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF).  \n",
    "\n",
    "Para visualizar gráficamente los tópicos es necesario instalar la librería `pyLDAvis` dentro del entorno de Anaconda con el comando:\n",
    "```python\n",
    "conda install -c conda-forge pyldavis \n",
    "```\n",
    "\n",
    "### Cargamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/IPython/core/pylabtools.py:77: DeprecationWarning: backends is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n",
      "  warnings.warn(\n",
      "/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/IPython/core/pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spacy para lematizar\n",
    "import spacy\n",
    "\n",
    "# herramientas de dibujado\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos un generador para obtener los documentos del Corpus línea a línea desde el archivo del conjunto de ejemplo y convertirlos en un listado de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "stop_words = nlp.Defaults.stop_words #listado de stop-words\n",
    "\n",
    "def lemmatize_corpus(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    \"\"\"Función que devuelve el lema de una string,\n",
    "    excluyendo las palabras cuyo POS_TAG no está en la lista\"\"\"\n",
    "    text_out = [t.lemma_.lower() for t in nlp(text)\n",
    "                if t.pos_ in allowed_postags\n",
    "                and len(t.lemma_)>3\n",
    "                and not t.is_stop]\n",
    "    return text_out\n",
    "            \n",
    "def build_texts(fname):\n",
    "    \"\"\"\n",
    "    Generador que devuelve el texto tokenizado a partir de un archivo\n",
    "    línea a línea\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield lemmatize_corpus(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_data_file = data_dir + os.sep + 'lee_background.cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/gensim/test/test_data/lee_background.cor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lee_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto=build_texts(lee_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object build_texts at 0x7fa1ccdc8ba0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hundred', 'people', 'force', 'vacate', 'home', 'southern', 'highlands', 'south', 'wales', 'strong', 'wind', 'today', 'push', 'huge', 'bushfire', 'town', 'hill', 'blaze', 'goulburn', 'south', 'west', 'sydney', 'force', 'closure', 'hume', 'highway', 'aedt', 'marked', 'deterioration', 'weather', 'storm', 'cell', 'move', 'east', 'blue', 'mountains', 'force', 'authority', 'decision', 'evacuate', 'people', 'home', 'outlying', 'street', 'hill', 'south', 'wales', 'southern', 'highland', 'estimate', 'resident', 'leave', 'home', 'nearby', 'mittagong', 'south', 'wales', 'rural', 'fire', 'service', 'weather', 'condition', 'cause', 'fire', 'burn', 'finger', 'formation', 'ease', 'fire', 'unit', 'hill', 'optimistic', 'defend', 'property', 'blaze', 'burn', 'year', 'south', 'wales', 'fire', 'crew', 'call', 'fire', 'gunning', 'south', 'goulburn', 'detail', 'available', 'stage', 'fire', 'authority', 'close', 'hume', 'highway', 'direction', 'fire', 'sydney', 'west', 'long', 'threaten', 'property', 'cranebrook', 'area', 'rain', 'fall', 'part', 'illawarra', 'sydney', 'hunter', 'valley', 'north', 'coast', 'bureau', 'meteorology', 'claire', 'richards', 'rain', 'little', 'ease', 'fire', 'burn', 'state', 'fall', 'isolated', 'area', 'generally', 'fall', 'millimetre', 'place', 'significant', 'millimetre', 'relief', 'rain', 'concern', 'fact', 'probably', 'hamper', 'effort', 'firefighter', 'wind', 'gust', 'associate', 'thunderstorm']\n"
     ]
    }
   ],
   "source": [
    "print(next(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indian', 'security', 'force', 'shoot', 'dead', 'suspect', 'militant', 'night', 'long', 'encounter', 'southern', 'kashmir', 'shootout', 'take', 'place', 'dora', 'village', 'kilometer', 'south', 'kashmiri', 'summer', 'capital', 'srinagar', 'death', 'come', 'pakistani', 'police', 'arrest', 'dozen', 'militant', 'extremist', 'group', 'accuse', 'stage', 'attack', 'india', 'parliament', 'india', 'accuse', 'pakistan', 'base', 'lashkar', 'taiba', 'jaish', 'mohammad', 'carry', 'attack', 'december', 'behest', 'pakistani', 'military', 'intelligence', 'military', 'tension', 'soar', 'raid', 'side', 'mass', 'troop', 'border', 'trade', 'diplomatic', 'sanction', 'yesterday', 'pakistan', 'announce', 'arrest', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'karachi', 'likely', 'raid', 'launch', 'group', 'militant', 'organisation', 'accuse', 'targette', 'india', 'military', 'tension', 'india', 'pakistan', 'escalate', 'level']\n"
     ]
    }
   ],
   "source": [
    "for t in texto:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el diccionario y el corpus para Topic Modeling\n",
    "Las dos entradas para el modelo LDA son un diccionario de `gensim` y un corpus de texto.  \n",
    "Preparamos el diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOW_Corpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: en cada iteración devuelve el vector bag-of-words\n",
    "    del siguiente documento en el corpus.\n",
    "    El corpus es el listado de críticas alojadas en el directorio\n",
    "    pasado como argumento al instanciar la clase.\n",
    "    \n",
    "    Procesa un documento cada vez, así\n",
    "    nunca carga el corpus entero en RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        #creamos bigramas y trigramas\n",
    "        self.bigram = gensim.models.Phrases(build_texts(self.filename), min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "        #optimizamos una vez entreando\n",
    "        self.bigram_mod = gensim.models.phrases.Phraser(self.bigram)\n",
    "\n",
    "        self.trigram = gensim.models.Phrases(self.bigram_mod[build_texts(self.filename)], min_count=5, threshold=50)  \n",
    "        self.trigram_mod = gensim.models.phrases.Phraser(self.trigram)\n",
    "        #crea el diccionario = mapeo de documentos a sparse vectors\n",
    "        self.diccionario = gensim.corpora.Dictionary(self.trigram_mod[map(lambda x: self.bigram_mod[x], build_texts(self.filename))])\n",
    "\n",
    "    def __len__(self):\n",
    "        #necesitamos saber la longitud del corpus para visualizar con pyLDAvis\n",
    "        return self.diccionario.num_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        __iter__ es un iterable => BOW_Corpus es un streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in build_texts(self.filename):\n",
    "            # transforma cada doc (lista de tokens) en un vector sparse uno a uno\n",
    "            yield self.diccionario.doc2bow(self.trigram_mod[self.bigram_mod[tokens]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow = BOW_Corpus(lee_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bow.diccionario.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5136"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_bow.diccionario.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rural_fire_service',\n",
       " 'leader_yasser_arafat',\n",
       " 'industrial_relations_commission',\n",
       " 'foreign_minister_alexander_downer',\n",
       " 'president_george_bush',\n",
       " 'woomera_detention_centre',\n",
       " 'hamas_islamic_jihad']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in corpus_bow.diccionario.token2id if re.match(r'\\w+_\\w+_\\w+', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k in corpus_bow.diccionario.token2id if re.match(r'\\w+_\\w+', k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 2), (6, 1), (7, 1), (8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 3), (33, 1), (34, 7), (35, 1), (36, 3), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 3), (47, 3), (48, 1), (49, 2), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 2), (68, 1), (69, 1), (70, 2), (71, 1), (72, 3), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 4), (80, 2), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 3), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 2), (96, 2), (97, 1)]\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "for c in corpus_bow:\n",
    "    print(c)\n",
    "    print(len(c))\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo LDA\n",
    "Es un modelo generativo que considera cada documento como una mezcla de temas donde cada tema tiene una distribución de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"palestinian\" + 0.011*\"people\" + 0.009*\"israeli\" + 0.006*\"report\" + '\n",
      "  '0.006*\"government\" + 0.005*\"fire\" + 0.005*\"arafat\" + 0.005*\"security\" + '\n",
      "  '0.005*\"area\" + 0.005*\"attack\"'),\n",
      " (1,\n",
      "  '0.009*\"australia\" + 0.007*\"year\" + 0.006*\"government\" + 0.006*\"claim\" + '\n",
      "  '0.005*\"force\" + 0.005*\"union\" + 0.005*\"attack\" + 0.004*\"people\" + '\n",
      "  '0.004*\"plan\" + 0.004*\"security\"'),\n",
      " (2,\n",
      "  '0.011*\"palestinian\" + 0.006*\"group\" + 0.006*\"fire\" + 0.006*\"attack\" + '\n",
      "  '0.005*\"arrest\" + 0.005*\"government\" + 0.005*\"year\" + 0.005*\"israeli\" + '\n",
      "  '0.005*\"people\" + 0.005*\"kill\"'),\n",
      " (3,\n",
      "  '0.007*\"australia\" + 0.007*\"year\" + 0.006*\"test\" + 0.006*\"force\" + '\n",
      "  '0.006*\"australian\" + 0.006*\"israeli\" + 0.006*\"palestinian\" + 0.004*\"time\" + '\n",
      "  '0.004*\"government\" + 0.003*\"kill\"'),\n",
      " (4,\n",
      "  '0.008*\"year\" + 0.007*\"official\" + 0.006*\"australia\" + 0.005*\"fire\" + '\n",
      "  '0.004*\"attack\" + 0.004*\"area\" + 0.004*\"people\" + 0.004*\"palestinian\" + '\n",
      "  '0.004*\"think\" + 0.003*\"come\"'),\n",
      " (5,\n",
      "  '0.007*\"people\" + 0.006*\"australian\" + 0.005*\"police\" + 0.005*\"palestinian\" '\n",
      "  '+ 0.005*\"government\" + 0.004*\"year\" + 0.004*\"arafat\" + 0.004*\"time\" + '\n",
      "  '0.004*\"attack\" + 0.003*\"force\"'),\n",
      " (6,\n",
      "  '0.006*\"australia\" + 0.006*\"year\" + 0.005*\"government\" + 0.005*\"people\" + '\n",
      "  '0.005*\"road\" + 0.005*\"force\" + 0.004*\"metre\" + 0.003*\"power\" + '\n",
      "  '0.003*\"president\" + 0.003*\"come\"'),\n",
      " (7,\n",
      "  '0.006*\"good\" + 0.006*\"australian\" + 0.006*\"year\" + 0.005*\"attack\" + '\n",
      "  '0.005*\"cent\" + 0.004*\"know\" + 0.004*\"union\" + 0.004*\"tell\" + 0.003*\"qantas\" '\n",
      "  '+ 0.003*\"river\"'),\n",
      " (8,\n",
      "  '0.012*\"australian\" + 0.006*\"afghanistan\" + 0.005*\"force\" + '\n",
      "  '0.005*\"australia\" + 0.004*\"test\" + 0.004*\"know\" + 0.004*\"police\" + '\n",
      "  '0.004*\"expect\" + 0.003*\"government\" + 0.003*\"report\"'),\n",
      " (9,\n",
      "  '0.007*\"australia\" + 0.006*\"year\" + 0.005*\"australian\" + 0.005*\"pakistan\" + '\n",
      "  '0.004*\"people\" + 0.004*\"afghanistan\" + 0.004*\"federal\" + 0.004*\"attack\" + '\n",
      "  '0.003*\"qantas\" + 0.003*\"team\"')]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "ldamodel = LdaModel(corpus=corpus_bow, num_topics=10, id2word=corpus_bow.diccionario)\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los temas  \n",
    "Podemos visualizarlo gráficamente la distribución de los documentos del Corpus por temas con la librería `pyLDAvis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pyLDAvis/__init__.py\", line 44, in <module>\n    from pyLDAvis._display import *\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pyLDAvis/_display.py\", line 11, in <module>\n    from pyLDAvis._prepare import PreparedData\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pyLDAvis/_prepare.py\", line 9, in <module>\n    import pandas as pd\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n    from pandas.core.api import (\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/api.py\", line 47, in <module>\n    from pandas.core.groupby import (\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n    from pandas.core.groupby.generic import (\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/groupby/generic.py\", line 68, in <module>\n    from pandas.core.frame import DataFrame\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/frame.py\", line 149, in <module>\n    from pandas.core.generic import (\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/generic.py\", line 193, in <module>\n    from pandas.core.window import (\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/window/__init__.py\", line 1, in <module>\n    from pandas.core.window.ewm import (\n  File \"/home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/core/window/ewm.py\", line 11, in <module>\n    import pandas._libs.window.aggregations as window_aggregations\nImportError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/vic_263/anaconda3/envs/ia/lib/python3.10/site-packages/pandas/_libs/window/aggregations.cpython-310-x86_64-linux-gnu.so)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mgensimvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mldamodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_bow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_bow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiccionario\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(vis_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/pyLDAvis/gensim_models.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/pyLDAvis/_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    430\u001b[0m term_frequency \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(term_topic_freq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43m_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_term_dists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_topic_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m token_table \u001b[38;5;241m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    436\u001b[0m topic_coordinates \u001b[38;5;241m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/pyLDAvis/_prepare.py:273\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    262\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab[term_ix],\n\u001b[1;32m    263\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m: term_topic_freq\u001b[38;5;241m.\u001b[39mloc[original_topic_id, term_ix],\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency[term_ix],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglift\u001b[39m\u001b[38;5;124m'\u001b[39m: log_lift\u001b[38;5;241m.\u001b[39mloc[original_topic_id, term_ix]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    268\u001b[0m                        })\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloglift\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     ])\n\u001b[0;32m--> 273\u001b[0m top_terms \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_relevance_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_ttd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_lift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_job_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    276\u001b[0m topic_dfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(topic_top_term_df, \u001b[38;5;28menumerate\u001b[39m(top_terms\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39miterrows(), start_index))\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([default_term_info] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(topic_dfs))\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ia/lib/python3.10/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "vis_data = gensimvis.prepare(ldamodel, corpus_bow, corpus_bow.diccionario)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus_bow:\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducimos el vocabulario\n",
    "corpus_bow.diccionario.filter_extremes(no_above=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_bow.diccionario.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus_bow:\n",
    "    print(len(c))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "ldamodel = LdaModel(corpus=corpus_bow, num_topics=10, id2word=corpus_bow.diccionario)\n",
    "vis_data = gensimvis.prepare(ldamodel, corpus_bow, corpus_bow.diccionario)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in ldamodel[corpus_bow]:\n",
    "    print(l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel[corpus_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
