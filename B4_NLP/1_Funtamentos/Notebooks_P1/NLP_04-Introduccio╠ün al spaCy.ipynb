{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la librería spaCy\n",
    "En este *notebook* vamos a describir el uso de la librería `spaCy` para el Procesado de Lenguaje Natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la librería y el modelo de lenguaje para el español. Vemos las principales características de la librería y del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos los modelos de lenguaje instalados (compatible con todas las versiones de `spaCy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.info('es_core_news_md')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Información del modelo\n",
    "Podemos ver todas las características del modelo cargado con su atributo `meta` (diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.meta['pipeline']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo\n",
    "Para cada modelo, `spaCy` tiene un vocabulario de palabras conocidas (*lexemes*) que almacena en un `stringStore` global"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada modelo tiene un conjunto de lexemas del idioma definidos en su Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTamaño vocabulario: \", len(nlp.vocab)) #esta longitud es falsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.vocab.strings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` no precargar en memoria todo el vocabulario, sino que carga los lexemas conforme los va necesitando:  \n",
    "> *To reduce the initial loading time, the lexemes in `nlp.vocab` are no longer loaded on initialization for models with vectors. As you process texts, the lexemes will be added to the vocab automatically, just as in small models without vectors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in nlp.vocab.vectors:\n",
    "    _ = nlp.vocab[w]\n",
    "\n",
    "print(\"\\nTamaño vocabulario: \", len(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"ciudad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"ciudad\"].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prop for prop in dir(spacy.lexeme.Lexeme) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"adiós\"].has_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"jhgsfhgjsf\" in nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"jhgsfhgjsf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"jhgsfhgjsf\" in nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"adiós\"].orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings[15970597814306712899]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado de texto\n",
    "spaCy ejecuta todos los análisis del texto con una sola instrucción. Esta instrucción ejecuta un *pipeline* (procesado secuencial) que implementa:  \n",
    "\n",
    "- División en tokens  \n",
    "- Lematizado\n",
    "- Análisis gramatical\n",
    "- Análisis de dependencias\n",
    "- *Name Entity Recognition* (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de texto\n",
    "texto = \"La casa de Juan es muy bonita.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es procesar el texto y generar un objeto de tipo `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prop for prop in dir(spacy.tokens.doc.Doc) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploramos el documento\n",
    "\n",
    "Al analizar un texto, spaCy lo divide en una lista de `tokens`, que se acceden iterando sobre el objeto `Doc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta división es distinta de la simple división por espacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada token tiene una serie de atributos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([prop for prop in dir(spacy.tokens.token.Token) if not prop.startswith('_')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada propiedad tiene dos atributos:  \n",
    "- `propiedad`: ID único o *hash* que identifica el valor en un diccionario común a todos los Docs (`stringstore`)\n",
    "- `propiedad_`: valor de la propiedad  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los atributos de algunos de los tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=doc[3]\n",
    "print(\"TOKEN:\", token)\n",
    "print(\"original:\", token.orth, token.orth_)\n",
    "print(\"lowercased:\", token.lower, token.lower_)\n",
    "print(\"lemma:\", token.lemma, token.lemma_)\n",
    "print(\"shape:\", token.shape, token.shape_)\n",
    "print(\"prefix:\", token.prefix, token.prefix_)\n",
    "print(\"suffix:\", token.suffix, token.suffix_)\n",
    "print(\"log probability:\", token.prob)\n",
    "print(\"Brown cluster id:\", token.cluster)\n",
    "print(\"POS:\", token.pos, token.pos_)\n",
    "print(\"tag:\", token.tag, token.tag_)\n",
    "print(\"morphology:\", token.morph)\n",
    "print(\"Dependency parsing:\", token.dep, token.dep_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las propiedades más importantes son (https://spacy.io/api/token#attributes):  \n",
    "* `orth_`: texto del token\n",
    "* `lemma_`: lema (palabra base)\n",
    "* `shape_`: forma ortográfica del token\n",
    "* `pos_`: Part-of-Speech (genérico)\n",
    "* `tag_`: POS detallado\n",
    "* `morph`: Análisis morfológico\n",
    "* `dep_`: Tipo de dependencia del token (análisis de dependencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "datos = map(lambda t: {'token': t.orth_,\n",
    "                       'lema': t.lemma_,\n",
    "                       'shape': t.shape_,\n",
    "                       'POS': t.pos_,\n",
    "                       'Descripción TAG': spacy.explain(t.tag_),\n",
    "                       'dependencia': t.dep_,\n",
    "                       'Descripción dep': spacy.explain(t.dep_),\n",
    "                       'Morfología': t.morph}, doc)\n",
    "\n",
    "pd.DataFrame(datos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencia entre token y lexema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_md')\n",
    "parsedData = nlp_en(\"I run a long run\")\n",
    "datos = map(lambda t: {'token': t.orth_,\n",
    "                       'lema': t.lemma_,\n",
    "                       'shape': t.shape_,\n",
    "                       'POS': t.pos_,\n",
    "                       'Morfología': t.morph,\n",
    "                       'dependencia': t.dep_,\n",
    "                       'Descripción dep': spacy.explain(t.dep_)},\n",
    "                        parsedData)\n",
    "\n",
    "pd.DataFrame(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[1]==parsedData[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[1].orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings[parsedData[1].orth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[1].orth==parsedData[4].orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[1].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData[4].pos_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis gramatical\n",
    "Los documentos SpaCy también dividen en texto en oraciones (*sentences*) que son objetos del tipo `spacy.tokens.span.Span`. Podemos iterar con el generador `doc.sents` usando `next()`, `list()`, un bucle o con una comprensión de lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Al Sr. Daniel siempre le gustaron las catedrales. La de su ciudad era tan alta, \\\n",
    "que al mirarla desde su pequeña estatura, tenía que torcer el cuello de tal forma que \\\n",
    "le costaba no marearse. Lo que más temía era que sus pies se despegasen de la tierra \\\n",
    "y la Catedral le arrastrara con ella hasta los cielos. Aun así, un espíritu \\\n",
    "aventurero le llevaba cada tarde hasta la Plaza Mayor.\"\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = doc.sents\n",
    "frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(next(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(\"Oración {}:\\n{}\\n\".format(i,sent))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada sentencia también tiene sus propios atributos, distintos de los de los tokens. Para spaCy las oraciones son objetos de tipo `spacy.tokens.span.Span`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([prop for prop in dir(spacy.tokens.span.Span) if not prop.startswith('_')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas propiedades de `Span` son distintas que las de cada `Token`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prop for prop in dir(spacy.tokens.span.Span) if\n",
    " not prop.startswith('_') and not prop in dir(spacy.tokens.token.Token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase=next(doc.sents)\n",
    "frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in doc[frase.start:frase.end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(frase.root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Obtén la palabra raíz -atributo `root`- de cada oración del texto anterior y muéstrala.  \n",
    "La respuesta es  \n",
    "```python  \n",
    "[gustaron, alta, era, llevaba]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech (POS)\n",
    "La librería `spaCy` determina el tipo gramatical (POS) de cada palabra en nuestro texto. Creamos un diccionario con los distintos POS de nuestro texto de ejemplo, usando el *hash* de cada POS como clave del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{w.pos: (w.pos_, spacy.explain(w.pos_)) for w in doc} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada tipo gramatical (POS) se subdivide en distintas etiquetas según su análisis mnorfológico(atributo `morph`).  \n",
    "Por ejemplo en nuestro texto tenemos los siguientes `tag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(set([(w.pos_, w.morph) for w in doc]), columns=['POS', 'morph']).sort_values(by='POS')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Crea una lista de Python con todas las palabras del texto `doc` que sean del tipo NOMBRE y además sean de género Femenino.  \n",
    "Ayuda: tendrás que usar una comprensión de lista filtrando mediante una función de búsqueda de texto (o patrón regular) sobre el valor del atributo `morph` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos extraer el género de todos los sustantivos a partir de su atributo `morph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "[(w, re.findall('Gender=(\\w+)', str(w.morph))) for w in doc if w.pos_=='NOUN']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de dependencias (dependency parsing)\n",
    "La librería `spaCy` también analiza las relaciones entre palabras de una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"El perro de Juan se comió mi bocadillo.\")\n",
    "dependencias = [(t.text, t.dep_, spacy.explain(t.dep_)) for t in doc]\n",
    "pd.DataFrame(list(dependencias), columns=['texto', 'dependencia', 'explicación'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada palabra tiene un tipo de dependencia determinado dentro de la frase. \n",
    "La lista completa está en https://spacy.io/docs/api/annotation  \n",
    "La dependencia `root` coincide con el atributo `root` de cada sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=next(doc.sents)\n",
    "sent.root"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada raíz tiene una serie de hijos (tokens que dependen gramaticalmente de esa raíz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sent.root.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1].dep_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos extender el análisis a cada palabra del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sent: \n",
    "    print(word, ': ', str(list(word.children)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, cada palabra tiene una palabra de la que depende (`.head`) y unas palabras que dependen de ella (`children`) a izquierda (`.lefts`) y a derecha (`.rights`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencias = map(lambda t: {\n",
    "    'Palabra': t.orth_,\n",
    "    'tipo de dependencia': f\"{t.dep_} ({spacy.explain(t.dep_)})\",\n",
    "    'HEAD': t.head.orth_},\n",
    "    doc)\n",
    "\n",
    "pd.DataFrame(dependencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencias = map(lambda token: {\n",
    "    'dep. izquierdas': [t.orth_ for t in token.lefts],\n",
    "    'palabra[tipo de dependencia]': f\"{token.orth_} [{token.dep_}]\",\n",
    "    'dep.derechas': [t.orth_ for t in token.rights]},\n",
    "    doc)\n",
    "\n",
    "pd.DataFrame(dependencias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar gráficamente las dependencias con el módulo de visualización `displaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance':120})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se pueden obtener los **sintagmas nominales** (*noun phrases*) de la oración. Cada NP tiene un sustantivo como raíz, acompañado ocasionalmente de las palabras que describen el sustantivo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = map(lambda chunk: {'NP': chunk.text,\n",
    "                            'root': chunk.root.text,\n",
    "                            'Dep.': chunk.root.dep_,\n",
    "                            'head': chunk.root.head.text},\n",
    "             doc.noun_chunks)\n",
    "\n",
    "pd.DataFrame(chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda de patrones de token\n",
    "Spacy tiene una clase `Matcher` que permite buscar tokens con un patrón definido en los objetos `Doc`.  \n",
    "Se puede buscar por el texto del token o por los atributos del token.  \n",
    "Ref: https://spacy.io/usage/rule-based-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "#inicializamos sobre el vocabulario\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos un patrón de texto a buscar\n",
    "patron = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}] #Patrón: texto 'iPhone' seguido de texto 'X'\n",
    "matcher.add(\"iphone_x\", [patron])\n",
    "\n",
    "#procesamos un documento con el patrón\n",
    "doc = nlp(\"El iPhone X salió después del iPhone 8, pero nunca sacaron el iPhone 9\")\n",
    "\n",
    "#llamamos al matcher\n",
    "matches = matcher(doc)\n",
    "\n",
    "#iteramos sobre los resultados\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches #lista de 'matches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings[1738708750870670527]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar por atributos del token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patron = [{\"TEXT\": \"iPhone\"}, {\"IS_DIGIT\": True}] #Patrón: texto 'iPhone' seguido token con la atributo 'IS_DIGIT' a True\n",
    "matcher.add(\"iphone_NN\", [patron])\n",
    "#llamamos al matcher\n",
    "matches = matcher(doc)\n",
    "\n",
    "#iteramos sobre los resultados\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end] #span del match en el documento\n",
    "    string_id = nlp.vocab.strings[match_id] #identificador del match\n",
    "    print(f\"{string_id}: {matched_span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"A mí me gusta el baile y a Pedro le gustaba tocar la trompeta pero no le gusta María y el gusta el iPhone X\")\n",
    "\n",
    "patron = [{\"LEMMA\": \"gustar\"}, {\"POS\": \"DET\", \"OP\": \"?\"}, {\"POS\": {\"REGEX\": \"NOUN|PROPN\"}}]\n",
    "matcher.add(\"gustar_nombre\", [patron])\n",
    "#llamamos al matcher\n",
    "matches = matcher(doc)\n",
    "\n",
    "#iteramos sobre el resultado\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    print(f\"{string_id}: {matched_span.text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "Crea un nuevo patrón para el lema \"gustar\" seguido de un verbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "Busca todas las secuencias de texto formadas por nombre seguido de al menos un adjetivo en el texto siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"En el agua muerta, de una brillantez de estaño, permanecía inmóvil la barca-correo: \\\n",
    "un gran ataúd cargado de personas y paquetes, con la borda casi a flor de agua. La vela triangular, \\\n",
    "con remiendos obscuros, estaba rematada por un guiñapo incoloro que en otros tiempos había sido una \\\n",
    "bandera española destartalada y delataba el carácter oficial de la vieja embarcación.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# búsqueda de patrones de doc\n",
    "También existe una clase para buscar secuencias de tokens (objetos tipo `doc`) dentro de un texto procesado. Se usa la clase `PhraseMatcher`, como explica https://spacy.io/usage/rule-based-matching#phrasematcher.  \n",
    "Es útil para buscar un listado de palabras o frases en un texto.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "paises = [\"Alemania\", \"Países Bajos\", \"Estados Unidos\"]\n",
    "#definimos patrones como objetos 'doc'\n",
    "patterns = [nlp.make_doc(text) for text in paises] #sólo tokenizamos\n",
    "matcher.add(\"Países\", patterns)\n",
    "\n",
    "doc = nlp(\"El presidente de Estados Unidos, Barack Obama, \"\n",
    "          \"visitó al caciller de Alemania en Berlín\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
    "colores = [\"azul\", \"amarillo\", \"rojo\", \"verde\"]\n",
    "#definimos patrones como objetos 'doc'\n",
    "patterns = [nlp(text) for text in colores] #hay que ejecutar todo el pipeline\n",
    "matcher.add(\"Colores\", patterns)\n",
    "\n",
    "doc = nlp(\"Me gusta el azul pero los amarillos me parecen todos feos\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
