{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-battlefield",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "## WE en spaCy\n",
    "Los word embeddings (o word vectors) son representaciones numéricas de las palabras, generadas con una reducción de dimensionalidad sobre una matriz de co-ocurrencia sobre un corpus enorme. Spacy utiliza los word vectors de GloVe, (*Stanford's Global Vectors for Word Representation*). Estos vectores se pueden utilizar para calcular la similaridad semántica entre palabras o documentos.\n",
    "\n",
    "El vocabulario por defecto en el modelo spaCy del idioma inglés (`en_core_web_sm`) es muy pequeño. Hay que cargar en_core_web_md (`python -m spacy download en_core_web_md`) para tener un conjunto de word vectors mayor. El modelo de tamaño medio en español (`python -m spacy download es_core_news_md`) contiene vectores también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.vectors_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "madrid = nlp.vocab[\"Madrid\"]\n",
    "madrid.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(madrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"me voy a Madrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92de316-e88d-4240-ac2c-a25eed9a438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9c851e6",
   "metadata": {},
   "source": [
    "El vector del lexema 'Madrid' es igual al token 'Madrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(madrid.vector == doc[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf072d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "madrid.vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.get_vector(\"Madrid\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b783116",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[3].vector[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef4676fc",
   "metadata": {},
   "source": [
    "Similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "toledo = nlp.vocab[\"Toledo\"]\n",
    "madrid.similarity(toledo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "manzana = nlp.vocab[\"manzana\"]\n",
    "madrid.similarity(manzana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "manzana.similarity(nlp.vocab[\"pera\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-diesel",
   "metadata": {},
   "source": [
    "### Visualización de word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemas = [nlp.vocab[orth] for orth in nlp.vocab.vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lexemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemas_vector = [nlp.vocab[orth] for orth in nlp.vocab.vectors if nlp.vocab[orth].has_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lexemas_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [t.text for t in np.random.choice(lexemas, 25, replace=False)]\n",
    "word_vectors = np.array([nlp(word).vector for word in words])\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-girlfriend",
   "metadata": {},
   "source": [
    "### Visualización t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2, init='random', learning_rate='auto')\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_vectors)\n",
    "labels = words\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-alabama",
   "metadata": {},
   "source": [
    "### Visualización PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = [t.vector for t in np.random.choice(lexemas, 10000, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = ['manzana', 'pera', 'Madrid', 'Toledo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vectors = np.array([nlp(word).vector for word in palabras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = pca.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue',alpha=0.05)\n",
    "\n",
    "labels = palabras\n",
    "T = pca.transform(palabras_vectors)\n",
    "plt.scatter(T[:, 0], T[:, 1], c='lime', edgecolors='darkgreen')\n",
    "\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-leave",
   "metadata": {},
   "source": [
    "### Visualización t-SNE extendida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_all = [t.text for t in np.random.choice(lexemas, 10000, replace=False)] + palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vectors = np.array([nlp(word).vector for word in palabras_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=5, init='random', learning_rate='auto')\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(palabras_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', alpha=0.05)\n",
    "\n",
    "labels = palabras\n",
    "\n",
    "plt.scatter(T[-len(palabras):, 0], T[-len(palabras):, 1], c='lime', edgecolors='darkgreen')\n",
    "for label, x, y in zip(labels, T[-len(palabras):, 0], T[-len(palabras):, 1]):\n",
    "    plt.annotate(label, xy=(x+0.1, y+0.1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-mount",
   "metadata": {},
   "source": [
    "# Word embeddings con Gensim\n",
    "Cargamos un conjunto de WE ya pre-entrenado con la API de Gensim:\\\n",
    "https://radimrehurek.com/gensim/downloader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "list(api.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.info('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_data in sorted(api.info()['models'].items()):\n",
    "    print(f\"\"\"{model_name} ({model_data.get('num_records', \"None\")} records):\n",
    "    {model_data['description']}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos el modelo deseado con\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-novel",
   "metadata": {},
   "source": [
    "Podemos usar los modelos cargados para ver los vectores de una palabra, buscar palabras similares o calcular analogías.\\\n",
    "Los modelos cargados son objetos de clase `models.keyedvectors` (https://radimrehurek.com/gensim/models/keyedvectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b190e-89e4-4057-a3d9-33ee4df630f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f408f",
   "metadata": {},
   "source": [
    "Podemos listar todas las palabras del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e692e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = model.index_to_key\n",
    "\n",
    "np.random.choice(palabras, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af536ead",
   "metadata": {},
   "source": [
    "Cada palabra tiene su vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra_rara = 'zamburiña'\n",
    "try:\n",
    "    vector = model[palabra_rara]\n",
    "except KeyError:\n",
    "    print(f\"La palabra '{palabra_rara}' no aparece en este modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a67350-627e-4f56-9c5f-2dbed2fcc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('apple','pear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4c8f0-2e5c-4d02-8730-c03d2dfef36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('banana','pear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.doesnt_match(\"apple pear banana city\".split())) #palabra que no encaja en el contexto del resto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-composition",
   "metadata": {},
   "source": [
    "### Analogías de word vectors con Gensim\n",
    "![rel](word2vec-king-queen-composition.png)  \n",
    "Si *hombre* es a *rey*, entonces *mujer* es a *??*\\\n",
    "Se calcula como la palabra más cercana al vector (rey - hombre) + mujer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hombre es a rey como mujer es a XX\n",
    "# rey - hombre + mujer \n",
    "#https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar_cosmul\n",
    "model.most_similar(positive=['king','woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a51e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = [model[t] for t in np.random.choice(model.index_to_key, 10000, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = ['man', 'woman', 'king', 'queen', 'son', 'daughter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda70574",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vectors = np.array([model[word] for word in palabras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = pca.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue',alpha=0.05)\n",
    "\n",
    "labels = palabras\n",
    "T = pca.transform(palabras_vectors)\n",
    "plt.scatter(T[:, 0], T[:, 1], c='lime', edgecolors='darkgreen')\n",
    "\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-platform",
   "metadata": {},
   "source": [
    "### Carga de otros modelos pre-entrenados en Gensim\n",
    "En lugar de usar su API cargamos los modelos en formato texto. Hay varios modelos en Español en https://github.com/dccuchile/spanish-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga de vectores en formato TXT\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wordvectors_file_vec = '~/Downloads/fasttext-sbwc.100k.vec'\n",
    "cantidad = 100000\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors['rey'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar(positive=['rey','mujer'],negative=['hombre'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar(positive=['yerno','mujer'],negative=['hombre'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correr -> corrían como saltar -> XX\n",
    "wordvectors.most_similar(positive=['corrían','saltar'],negative=['correr'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francia -> París como España -> XX\n",
    "wordvectors.most_similar(positive=['parís','españa'],negative=['francia'], topn=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ideal-stanley",
   "metadata": {},
   "source": [
    "### Modelos FastText\n",
    "Los modelos de FastText se pueden cargar en formato texto (sólo palabras pre-entrenadas) o como modelo binario (calcula nuevas palabras a partir de su n-grama de caracteres)  \n",
    "Modelos pre-entrenados de FastText: https://github.com/mquezada/starsconf2018-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar(['adiós'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra_rara = 'pequeñín'\n",
    "try:\n",
    "    vector = wordvectors[palabra_rara]\n",
    "except KeyError:\n",
    "    print(f\"La palabra '{palabra_rara}' no aparece en este modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(wordvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectores de FastText desde el formato binario (lento, requiere mucha memoria)\n",
    "# descargado de https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "# ¡ojo, ocupan 4,5 GB!\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "wordvectors_file = '/Users/jovifran/Downloads/cc.es.300.bin'\n",
    "wordvectors = load_facebook_vectors(wordvectors_file) #carga vectores pre-entrenados sólo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd437e65-a7a9-41e2-8dde-ced6fb54bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors['adiós'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08acd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'pequeñín' in wordvectors.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar('pequeñín')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031f750-8ef7-473a-b019-c0216a9bc7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#'neorevolucionario' in wordvectors.vocab #versión gensim <4.0\n",
    "'neorevolucionario' in wordvectors.key_to_index #veersión gensim >=4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors['neorevolucionario'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar('neorevolucionario')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028af6a4",
   "metadata": {},
   "source": [
    "Se puede usar como corrector ortográfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a30cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'ayedo' in wordvectors.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677b3f2-7441-482b-91e2-a09372a5583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar('ayedo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e41f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-saver",
   "metadata": {},
   "source": [
    "## Entrenamiento de vectores propios\n",
    "En lugar de usar vectores preentrenados los podemos entrenar con el modelo `word2vec` de Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_doc_tokenize(doc):\n",
    "    '''Función que normaliza un texto cogiendo sólo\n",
    "    las palabras en minúsculas mayores de 3 caracteres'''\n",
    "    # separamos en tokens\n",
    "    tokens = nlp.make_doc(doc)\n",
    "    # filtramos stopwords\n",
    "    filtered_tokens = [t.lower_ for t in tokens if\n",
    "                       len(t.text)>3 and\n",
    "                       not t.is_space and\n",
    "                       not t.is_punct]\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cañas y barro.txt', 'r', encoding = 'utf-8') as f:\n",
    "    texto = f.readlines()\n",
    "TOKENIZED_CORPUS = list(map(normalizar_doc_tokenize, texto))\n",
    "len(TOKENIZED_CORPUS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8697808b",
   "metadata": {},
   "source": [
    "Consideramos cada línea como un documento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3277ba-f864-4e1b-8940-3b3b24e4d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_CORPUS[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-conditioning",
   "metadata": {},
   "source": [
    "Calculamos los vectores de las palabras de nuestro corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(TOKENIZED_CORPUS, #lista de documentos como lista de tokens\n",
    "                               vector_size=50,          #tamaño del vector\n",
    "                               window=5,         #nº de términos adyacentes que usamos para el cálculo\n",
    "                               min_count=10,      #nº mínimo de apariciones del término para contarlo\n",
    "                               epochs = 50)\n",
    "\n",
    "#una vez entrenado el modelo nos quedamos con los vectores calculados\n",
    "#si no se van a actualizar los vectores con nuevos documentos\n",
    "model = model.wv\n",
    "len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4399824-813f-4ff2-904d-c90e7ce5c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-lottery",
   "metadata": {},
   "source": [
    "Este modelo funciona como los que hemos utilizado anteriormente en la librería `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-lemon",
   "metadata": {},
   "source": [
    "# Vectores de documento (modelos semánticos)\n",
    "Los vectores de documento recogen el sentido semántico de todo el documento como un vector de dimensines únicas.\n",
    "## Modelos basados en *word embeddings*\n",
    "Calcula el promedio de los *word embeddings* del documento para obtener un vector con sentido semántico de todo el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "#Librería spaCy\n",
    "#El atributo vector del Doc o Span calcula el promedio de sus vectores de palabra\n",
    "\n",
    "doc1 = nlp(\"Me gustan las patatas fritas y las hamburguesas.\")\n",
    "doc2 = nlp(\"La comida rápida sabe muy bien.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d893339",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1[0].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293a6be-617e-4598-a549-ab5f5df3a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ca73c",
   "metadata": {},
   "source": [
    "En la librería `spaCy` el objeto `Doc` contiene el vector del documento como promedio de sus `word embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b30ec-8a4b-4376-b72e-9e04a9cbe89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc1[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1[2:4].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitud de dos documentos\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similitud de tokens y spans\n",
    "patatas_fritas = doc1[3:5]\n",
    "hamburguesas = doc1[7]\n",
    "print(patatas_fritas, \"<->\", hamburguesas, patatas_fritas.similarity(hamburguesas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b81980-338e-4d2d-b690-89ea3123a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "comida_rapida = doc2[1:3]\n",
    "print(patatas_fritas, \"<->\", comida_rapida, patatas_fritas.similarity(comida_rapida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "muy_bien = doc2[4:6]\n",
    "print(patatas_fritas, \"<->\", muy_bien, patatas_fritas.similarity(muy_bien))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec349afd",
   "metadata": {},
   "source": [
    "### Modelo Sentence embeddings (SBERT)\n",
    "Usamos un modelo *transformer* entrenado en texto en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome',\n",
    "             'The dog plays in the garden',\n",
    "             'A woman listens to music',\n",
    "             'The new movie is so great']\n",
    "\n",
    "#Compute embedding\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea138bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences[i+1:]):\n",
    "        print(f\"Score: {cosine_scores[i][i+j+1]:.4f}:\\t{s1}, {s2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(embeddings[0].reshape(-1,384).cpu(),cmap=\"Greys\",center=0,square=False)\n",
    "plt.gcf().set_size_inches(10,1)\n",
    "plt.axis('off')\n",
    "plt.title(sentences[0])\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(embeddings[2].reshape(-1,384).cpu(),cmap=\"Greys\",center=0,square=False)\n",
    "plt.gcf().set_size_inches(10,1)\n",
    "plt.axis('off')\n",
    "plt.title(sentences[2])\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(embeddings[5].reshape(-1,384).cpu(),cmap=\"Greys\",center=0,square=False)\n",
    "plt.gcf().set_size_inches(10,1)\n",
    "plt.axis('off')\n",
    "plt.title(sentences[5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8483ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo multilingüe\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0400c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['el gato juega en el jardín',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome',\n",
    "             'The dog plays in the garden',\n",
    "             'una mujer escucha música',\n",
    "             'la nueva película es asombrosa']\n",
    "\n",
    "#Compute embedding\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7091540",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences[i+1:]):\n",
    "        print(f\"Score: {cosine_scores[i][i+j+1]:.4f}:\\t{s1}, {s2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2288a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(embeddings[2].reshape(-1,384).cpu(),cmap=\"Greys\",center=0,square=False)\n",
    "plt.gcf().set_size_inches(10,1)\n",
    "plt.axis('off')\n",
    "plt.title(sentences[2])\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(embeddings[5].reshape(-1,384).cpu(),cmap=\"Greys\",center=0,square=False)\n",
    "plt.gcf().set_size_inches(10,1)\n",
    "plt.axis('off')\n",
    "plt.title(sentences[5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
