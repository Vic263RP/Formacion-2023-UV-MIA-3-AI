{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de la librería `datasets`\n",
    "Usamos esta librería para cargar de forma dinámica un dataset para entrenar/hacer inferencia en modelos de DL.  \n",
    "Se instala con:  \n",
    "\n",
    "`conda install -c huggingface -c conda-forge datasets` \n",
    "\n",
    "Ref: https://huggingface.co/docs/datasets/index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de tomar el tiempo para descargar un conjunto de datos, suele ser útil obtener rápidamente información general sobre él. La información de un conjunto de datos se almacena dentro de DatasetInfo y puede incluir detalles como la descripción del conjunto de datos, las características y el tamaño del conjunto de datos.\n",
    "\n",
    "Utiliza la función `load_dataset_builder()` para cargar un constructor de conjunto de datos y examinar los atributos de un conjunto de datos sin comprometerte a descargarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "ds_builder = load_dataset_builder(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_builder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cuáles son los atributos y métodos del objeto descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in dir(ds_builder) if not e.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in dir(ds_builder.info) if not e.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#descripción del dataset\n",
    "print(ds_builder.info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#características del dataset\n",
    "ds_builder.info.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una división (split) es un subconjunto específico de un conjunto de datos, como entrenamiento y prueba. Enumera los nombres de las divisiones de un conjunto de datos con la función `get_dataset_split_names()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "get_dataset_split_names(\"glue\", \"mrpc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego puedes cargar una división específica utilizando el parámetro \"split\". Cargar una división de un conjunto de datos devuelve un objeto Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no especificas una división, 🤗 Datasets devuelve un objeto DatasetDict en su lugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando un dataset\n",
    "Hay dos tipos de objetos de conjunto de datos, un Dataset regular y luego un **IterableDataset**. Un Dataset proporciona acceso aleatorio rápido a las filas y permite la asignación de memoria para que la carga de conjuntos de datos grandes utilice solo una cantidad relativamente pequeña de memoria del dispositivo. Pero para conjuntos de datos realmente, realmente grandes que ni siquiera caben en el disco o en la memoria, un IterableDataset te permite acceder y utilizar el conjunto de datos sin tener que esperar a que se descargue por completo.\n",
    "\n",
    "### Objeto `dataset`\n",
    "Cuando cargas una división de un conjunto de datos, obtienes un objeto Dataset. Puedes hacer muchas cosas con un objeto Dataset, por lo que es importante aprender cómo manipular e interactuar con los datos almacenados en su interior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in dir(dataset) if not e.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Indexación\n",
    "\n",
    "Un Dataset contiene columnas de datos, y cada columna puede ser un tipo de dato diferente. El índice, o etiqueta de eje, se utiliza para acceder a ejemplos del conjunto de datos. Por ejemplo, la indexación por fila devuelve un diccionario con un ejemplo del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primera fila del dataset\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#última fila del dataset\n",
    "dataset[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La indexación por el nombre de la columna devuelve una lista de todos los valores en esa columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentence1']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes combinar la indexación por fila y por nombre de columna para obtener un valor específico en una posición determinada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"sentence1\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al revés también funciona pero es más lento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentence1\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing\n",
    "El uso de la técnica de \"slicing\" devuelve una rebanada o subconjunto del conjunto de datos, lo cual es útil para ver varias filas a la vez. Para hacer un corte (slice) de un conjunto de datos, utiliza el operador \":\" para especificar un rango de posiciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filas entre la 3 y la 6\n",
    "dataset[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objeto `IterableDataset`\n",
    "\n",
    "Un IterableDataset se carga cuando estableces el parámetro \"streaming\" en True al cargar un conjunto de datos utilizando la función `load_dataset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
    "for example in iterable_dataset:\n",
    "     print(example)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset.dataset_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un IterableDataset itera progresivamente sobre un conjunto de datos de un ejemplo a la vez, por lo que no es necesario esperar a que todo el conjunto de datos se descargue antes de poder usarlo. Como puedes imaginar, esto es muy útil para conjuntos de datos grandes que deseas utilizar de inmediato.\n",
    "\n",
    "Sin embargo, esto significa que el comportamiento de un IterableDataset es diferente al de un Dataset regular. No obtienes acceso aleatorio a los ejemplos en un IterableDataset. En su lugar, debes iterar sobre sus elementos, por ejemplo, llamando a `next(iter())` o utilizando un bucle for para obtener el siguiente elemento del IterableDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(iterable_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes obtener un subconjunto del conjunto de datos con un número específico de ejemplos utilizando la función IterableDataset.take():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(iterable_dataset.take(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Además de cargar conjuntos de datos, el principal objetivo de 🤗 Datasets es ofrecer una amplia variedad de funciones de preprocesamiento para llevar un conjunto de datos a un formato adecuado para el entrenamiento con tu marco de aprendizaje automático.\n",
    "\n",
    "Hay muchas formas posibles de preprocesar un conjunto de datos, y todo depende de tu conjunto de datos específico. A veces es posible que necesites cambiar el nombre de una columna, y otras veces puede que necesites desenrollar campos anidados. 🤗 Datasets ofrece una forma de hacer la mayoría de estas cosas. Pero en casi todos los casos de preprocesamiento necesitarás tokenizar un conjunto de datos de texto.\n",
    "\n",
    "El último paso de preprocesamiento generalmente implica establecer el formato del conjunto de datos para que sea compatible con el formato de entrada esperado por tu marco de aprendizaje automático.\n",
    "### Tokenizado\n",
    "Cargamos un modelo de tokenizado de la librería 🤗 Transformers y ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(dataset[0][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más rápida de tokenizar todo tu conjunto de datos es utilizar la función `map()`. Esta función acelera la tokenización al aplicar el tokenizador a lotes de ejemplos en lugar de ejemplos individuales. Establece el parámetro batched en True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establece el formato de tu conjunto de datos para que sea compatible con tu framework de aprendizaje automático:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza la función `set_format()` para convertir el formato del dataset a torch y especificar las columnas a formatear. Esta función aplica el formato on-the-fly. Después de convertir a tensores PyTorch, se convierte en un objeto `torch.utils.data.DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza la función `to_tf_dataset()` para establecer el formato del conjunto de datos para que sea compatible con TensorFlow. También necesitarás importar un objeto de la clase `DataCollator` de 🤗 Transformers para combinar las longitudes de secuencia variables en un solo lote con longitudes iguales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_dataset = dataset.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset creado se puede usar en Tensorflow directamente para entrenar/hacer inferencia en un modelo:\n",
    "```python\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar las predicciones\n",
    "🤗 Datasets proporciona varias métricas comunes y específicas de procesamiento del lenguaje natural (NLP) para medir el rendimiento de tus modelos. vamos a ver cómo cargar una métrica y la utilizarla para evaluar las predicciones de tu modelo.\n",
    "\n",
    "Puedes ver qué métricas están disponibles con la función `list_metrics()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy fácil cargar una métrica con 🤗 Datasets. De hecho, te darás cuenta de que es muy similar a cargar un conjunto de datos. Puedes cargar una métrica desde el Hub con la función load_metric():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objeto `Metric`\n",
    "Antes de comenzar a utilizar un objeto Metric, es importante conocerlo un poco mejor. Al igual que con un conjunto de datos, puedes obtener información básica sobre una métrica. Por ejemplo, accedemos al parámetro `inputs_description` en `datasets.MetricInfo` para obtener más información sobre el formato de entrada esperado de una métrica y algunos ejemplos de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric.inputs_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular las métricas\n",
    "Una vez que hayas cargado una métrica, estás listo para usarla para evaluar las predicciones de un modelo. Proporciona las predicciones del modelo y las referencias a la función `compute()`:\n",
    "```python\n",
    "model_predictions = model(model_inputs)\n",
    "final_score = metric.compute(predictions=model_predictions, references=gold_references)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un dataset\n",
    "A veces, es posible que necesites crear un conjunto de datos si estás trabajando con tus propios datos. Crear un conjunto de datos con 🤗 Datasets te brinda todas las ventajas de la biblioteca: carga y procesamiento rápido, capacidad de trabajar con conjuntos de datos enormes, asignación de memoria y más. Puedes crear fácil y rápidamente un conjunto de datos con enfoques de bajo código de 🤗 Datasets, lo que reduce el tiempo necesario para comenzar a entrenar un modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación a partir de archivos locales\n",
    "Puedes crear un conjunto de datos a partir de archivos locales especificando la ruta a los archivos de datos. Hay dos formas de crear un conjunto de datos utilizando los métodos from_:\n",
    "\n",
    "El método `from_generator()` es la forma más eficiente en términos de memoria para crear un conjunto de datos a partir de un generador debido al comportamiento iterativo de los generadores. Esto es especialmente útil cuando trabajas con un conjunto de datos realmente grande que puede no caber en memoria, ya que el conjunto de datos se genera progresivamente en disco y luego se asigna a memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def gen():\n",
    "    yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n",
    "    yield {\"pokemon\": \"squirtle\", \"type\": \"water\"}\n",
    "ds = Dataset.from_generator(gen)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un `IterableDataset` basado en generador necesita ser iterado con un bucle `for`, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset\n",
    "ds = IterableDataset.from_generator(gen)\n",
    "for example in ds:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `from_dict()` es una forma directa de crear un dataset a partir de un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict({\"pokemon\": [\"bulbasaur\", \"squirtle\"], \"type\": [\"grass\", \"water\"]})\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo completo del uso de un dataset en una tarea de clasificación se puede encontrar en https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "Creamos un dataset a partir de un objeto generador que devuelve los documentos de un archivo de texto línea a línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_texts(fname):\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield ({'texto': line, 'long': len(line)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = build_texts(\"lee_background.cor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in textos:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_generator(build_texts, gen_kwargs={\"fname\": 'lee_background.cor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset en realidad también se podría haber creado con el método `from_file` de la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('text', data_files = {'train': 'lee_background.cor'}, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9427b6968756065d1bd05eea5b80a14e2325e56f8688553e9d79bdc9b1540118"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
