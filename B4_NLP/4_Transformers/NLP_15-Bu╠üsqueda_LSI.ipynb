{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda de texto (*information retrieval*)\n",
    "Vamos a usar el algoritmo LSI para realizar una búsqueda indexada de textos similares. Versión modificada para usar matriz TFIDF\n",
    "### Cargamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spacy para lematizar\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos un generador para obtener los documentos del Corpus línea a línea desde el archivo del conjunto de ejemplo y convertirlos en un listado de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "stop_words = [word.text for word in nlp.vocab if word.is_stop] #listado de stop-words\n",
    "\n",
    "def lemmatize_doc(text, allowed_postags=['NOUN', 'PROPN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Función que devuelve el lema de una string,\n",
    "    excluyendo las palabras cuyo POS_TAG no está en la lista\"\"\"\n",
    "    text_out = [token.lemma_.lower() for token in nlp(text) if token.pos_ in allowed_postags and len(token.lemma_)>3]\n",
    "    return text_out\n",
    "            \n",
    "def build_texts(fname):\n",
    "    \"\"\"\n",
    "    Generador que devuelve el texto tokenizado a partir de un archivo\n",
    "    línea a línea\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield lemmatize_doc(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lee_data_file = 'lee_background.cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto=build_texts(lee_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(texto))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el diccionario y el corpus para Topic Modeling\n",
    "Las dos entradas para el modelo LDA son un diccionario (id2word) y un corpus de `gensim`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_Corpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: en cada iteración devuelve el vector TF-IDF\n",
    "    del siguiente documento en el corpus.\n",
    "    El corpus es el listado de críticas alojadas en el directorio\n",
    "    pasado como argumento al instanciar la clase.\n",
    "    \n",
    "    Procesa un documento cada vez, así\n",
    "    nunca carga el corpus entero en RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        #creamos bigramas y trigramas\n",
    "        self.bigram = gensim.models.Phrases(build_texts(self.filename), min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "        #optimizamos una vez entreando\n",
    "        self.bigram_mod = gensim.models.phrases.Phraser(self.bigram)\n",
    "\n",
    "        self.trigram = gensim.models.Phrases(self.bigram_mod[build_texts(self.filename)], min_count=5, threshold=50)  \n",
    "        self.trigram_mod = gensim.models.phrases.Phraser(self.trigram)\n",
    "        #crea el diccionario = mapeo de documentos a sparse vectors\n",
    "        self.diccionario = gensim.corpora.Dictionary(\n",
    "            self.trigram_mod[map(lambda x: self.bigram_mod[x], build_texts(self.filename))])\n",
    "        #calculamos el modelo TFIDF\n",
    "        self.corpus_bow = (self.diccionario.doc2bow(text) for text in\n",
    "                           self.trigram_mod[map(lambda x: self.bigram_mod[x], build_texts(self.filename))])\n",
    "        self.tfidf = gensim.models.TfidfModel(self.corpus_bow)\n",
    "        \n",
    "    def __len__(self):\n",
    "        #necesitamos saber la longitud del corpus para visualizar con pyLDAvis\n",
    "        return self.diccionario.num_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        __iter__ es un iterable => TFIDF_Corpus es un streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in build_texts(self.filename):\n",
    "            # transforma cada doc (lista de tokens) en un vector sparse uno a uno\n",
    "            yield self.tfidf[self.diccionario.doc2bow(self.trigram_mod[self.bigram_mod[tokens]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea diccionario\n",
    "corpus_tfidf = TFIDF_Corpus(lee_data_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos como ejemplo el primer doc\n",
    "for c in corpus_tfidf:\n",
    "    print(c)\n",
    "    print(len(c))\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que en el modelo BoW de `gensim` el primer elemento de cada tupla es el ID del término en el diccionario, y el segundo su frecuencia en el doc.  \n",
    "`diccionario[ID]` devuelve el término con índice ID en el vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_tfidf.diccionario.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf.tfidf.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "### Modelo LSI\n",
    "Este modelo ordena los temas y saca un listado ordenado. Hay que especificar el número de topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus_tfidf, num_topics=100, id2word=corpus_tfidf.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus_tfidf:\n",
    "    print(lsimodel[c])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Búsqueda de documentos por temática (*information retrieval*)\n",
    "Para buscar los documentos más similares a un documento dado, hay que trabajar con el modelo *space vector* generado por el algoritmo LSI. Primero, generamos una matriz LSI para todos los documentos del corpus. Para buscar el documento más parecido a un nuevo texto, calculamos su vector LSI y buscamos cuál es el más cercano dentro de la matriz LSI del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos un índice de similitud entre los documentos del corpus\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "#creamos corpus transformado\n",
    "lsi_corpus = lsimodel[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primer documento, como BOW_corpus no es indexable no podemos indexar tampoco lsi_corpus\n",
    "for i in lsi_corpus:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos recuperar los documentos que queramos en una lista con las herramientas de iteración\n",
    "from itertools import islice\n",
    "\n",
    "primer_doc = islice(lsi_corpus, 1) #devuelve un objeto de tipo generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(primer_doc)) #alternativamente list(primer_doc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_10_15 = islice(lsi_corpus, 10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in docs_10_15:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos índice\n",
    "index = MatrixSimilarity(lsi_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El índice guarda los vectores LSI de cada texto del corpus en su atributo `index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.index.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la similitud de cualquier documento del corpus al resto de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[next(islice(lsi_corpus, 1))]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nos quedamos con los 10 primeros\n",
    "sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims_sorted[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos el documento original para evaluar su parecido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "\n",
    "linecache.getline(lee_data_file, 1) #noticia  nº0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_sorted[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linecache.getline(lee_data_file, sims_sorted[1][0]+1) #noticia más parecida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linecache.getline(lee_data_file, sims_sorted[2][0]+1) #segunda noticia más parecida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos calcular el documento más similar dentro del corpus a un nuevo documento calculando primero su matriz TF-IDF/BoW y luego transformando a matriz LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"the new Pakistan government falled in the terrorist attack by the islamic group Hamas\"\n",
    "texto_lemmatizado = lemmatize_doc(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_lemmatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_new = corpus_tfidf.trigram_mod[corpus_tfidf.bigram_mod[texto_lemmatizado]]\n",
    "texto_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow_new = corpus_tfidf.diccionario.doc2bow(texto_new)\n",
    "corpus_tfidf_new = corpus_tfidf.tfidf[corpus_bow_new]\n",
    "lsi_corpus_new = lsimodel[corpus_tfidf_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_corpus_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora buscamos en el índice cuáles son los documentos más parecidos dentro del corpus al nuevo documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[lsi_corpus_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims_sorted[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto del documento más cercano es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linecache.getline(lee_data_file, sims_sorted[0][0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
