{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de texto con modelos *transformers*\n",
    "Implementemos un clasificador usando un modelo BERT haciendo *fine-tuning* sobre un conjunto de análisis de sentimiento en Twitter.  \n",
    "\n",
    "Usamos la librería `transformers` en su implementación para `Tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo a utilizar\n",
    "nombre_modelo = 'bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los datos\n",
    "df = pd.read_csv('tweets_max.csv')\n",
    "\n",
    "#seleccionamos columnas de interés\n",
    "df = df[['content', 'polarity']]\n",
    "\n",
    "#dejamos polaridades definidas\n",
    "df = df[(df['polarity']=='P') | (df['polarity']=='N')]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de texto\n",
    "Usamos Spacy para separar el texto en tokens y mantener sólo las palabras importantes, dejando su lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "pattern1 = re.compile(r'@[\\w_]+') #elimina menciones\n",
    "pattern2 = re.compile(r'https?://[\\w_./]+') #elimina URL\n",
    "#pattern3 = re.compile(r'#[\\w_]+') #elimina hashtags\n",
    "pattern4 = re.compile('[{}]+'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpiamos las menciones, URL y hashtags del texto. Luego \n",
    "    quitamos signos de puntuación\"\"\"\n",
    "    text = pattern1.sub('mención', text)\n",
    "    text = pattern2.sub('URL', text)\n",
    "    #text = pattern3.sub('hashtag', text)\n",
    "    text = pattern4.sub(' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(df['content'].iloc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparemos el conjunto de datos\n",
    "Fijamos un tamaño máximo de vocabulario.  \n",
    "Separamos los tweets en tokens dentro de este vocabulario y creamos las secuencias de longitud fija.  \n",
    "La longitud de la secuencia viene dada por la longitud en tokens del tweet más largo. Sólo se conservan los tokens de las palabras en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpiamos texto y quitamos tweets que se han quedado vacíos\n",
    "df.content=df.content.apply(clean_text)\n",
    "df = df[df['content']!='']\n",
    "#el conjunto de salida es la polaridad, hay que convertir a binario\n",
    "#codificamos 'P' como 1 y 'N' se queda como 0\n",
    "Y=(df.polarity=='P').values*1\n",
    "\n",
    "#Separamos entrenamiento y test\n",
    "#realmente habría que sacar los tokens sólo del conjunto de entrenamiento...\n",
    "X_train_tweets, X_test_tweets, Y_train, Y_test = train_test_split(\n",
    "    df.content,\n",
    "    Y, \n",
    "    test_size = 0.3,\n",
    "    random_state = 0)\n",
    "print(X_train_tweets.shape,Y_train.shape)\n",
    "print(X_test_tweets.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimamos la longitud máxima de documento\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=np.max([len(l.split()) for l in X_train_tweets])\n",
    "print('longitud máxima: {}'.format(MAX_SEQUENCE_LENGTH))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos la entrada para el modelo  \n",
    "Los modelos de `transformers` utilizan 3 vectores para cada entrada:  \n",
    " - 'input_ids': ID del vocabulario para hacer un embedding de los tokens  \n",
    " - 'token_type_ids': ID de la frase (en aplicaciones con 2 frases de entrada)\n",
    " - 'attention_mask': máscara de atención de los tokens  \n",
    "Usamos una función de tokenizado específica del modelo para obtener estos vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizamos y codificamos como Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(nombre_modelo)\n",
    "train_encodings = tokenizer(X_train_tweets.to_list(), truncation=True, padding=True, return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings['input_ids'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La longitud máxima es mayor que el número de palabras (por el tokenizado *WordPiece*)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los tokens que ha generado y decodificar de nuevo cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos como ejemplo el primer tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tweets.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(train_encodings['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "Tokeniza el conjunto de TEST utilizando el mismo modelo y las mismas características (longitud máxima) del conjunto de TRAIN.  \n",
    "Define en la variable `MAX_SEQUENCE_LENGTH` la máxima longitud de entrada a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## completar\n",
    "MAX_SEQUENCE_LENGTH=train_encodings['input_ids'].shape[1]\n",
    "test_encodings = tokenizer(\n",
    "    X_test_tweets.to_list(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings['input_ids'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para pasar estos datos como entrada al modelo los convertimos en un `Dataset` de Tensorflow que es un objeto generador que devuelve un diccionario con las muestras de cada iteración (o batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    Y_train\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    Y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de BERT\n",
    "Ajustamos el modelo de BERT a nuestro problema de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos modelo de clasificación\n",
    "id2label = {0: \"Neg\", 1: \"Pos\"}\n",
    "label2id = {val: key for key, val in id2label.items()}\n",
    "config = AutoConfig.from_pretrained(\n",
    "    nombre_modelo, hidden_dropout_prob=0.1, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    nombre_modelo, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros de la última capa vienen dados por el nº de etiquetas (una neurona por etiqueta) con una entrada de 768 dimensiones (embedding del token `[CLS]' de cada muestra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos\n",
    "batch_size = 8\n",
    "n_epochs = 10\n",
    "history=model.fit(train_dataset.batch(batch_size),\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Fine-tuning BERT')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(test_dataset.batch(batch_size), verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos las predicciones del modelo\n",
    "predict=model.predict(test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predict.logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El modelo de Sentence Classification de Transormers siempre devuelve en la última capa los logits de cada clase\n",
    "#el núm. de clases se especifica en la configuración del modelo\n",
    "\n",
    "Y_test_label = list(map(lambda l: model.config.id2label[l], Y_test))\n",
    "predict_label = list(map(lambda l: model.config.id2label[l], np.argmax(predict.logits, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test_label, predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Necesitamos la probabilidad sólo para calcular el AUC o ajustar el umbral\n",
    "predict_proba = tf.nn.softmax(predict.logits)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(Y_test, predict_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    Y_test,\n",
    "    predict_proba[:,1],\n",
    "    name=\"Positive class\",\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning en Keras\n",
    "Usamos la primera capa del modelo BERT de Keras y sobre la salida entrenamos un clasificador binario en Keras. De esta manera podríamos añadir más capas ocultas entre la salida del encoder BERT y la capa de salida del clasificador.  \n",
    "Si usamos la primera salida del modelo BERT tenemos los `last_hidden_state` de todos los tokens de entrada. Por tanto tenemos un tensor de `MAX_SEQUENCE_LENGTH` x 768 valores.   \n",
    "Si usamos la segunda salida del modelo BERT tenemos el `pooler_output` del hidden_state del primer token (CLS). Por tanto tenemos un vector de 768 valores.  \n",
    "Replicamos el modelo que utiliza TFBertForSequenceClassification (https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/bert/modeling_tf_bert.py#L1601):  \n",
    " - Después del pooler_output pasamos por un dropout para generar los logits.  \n",
    " - Los logits entran a una capa densa con activación sigmoide o softmax según el clasificador.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la salida `pooler_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "#######################################\n",
    "### --------- Setup BERT ---------- ###\n",
    "\n",
    "\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = AutoConfig.from_pretrained(nombre_modelo)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load the Transformers BERT model\n",
    "transformer_model = TFAutoModel.from_pretrained(nombre_modelo, config = config)\n",
    "\n",
    "#######################################\n",
    "### ------- Build the model ------- ###\n",
    "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), name='attention_mask', dtype='int32') \n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "pooled_output = transformer_model(input_ids, attention_mask)[1]  # (bs, dim)\n",
    "logits = Dropout(0.1)(pooled_output)  # (bs, dim)\n",
    "# Then build your model output\n",
    "output = Dense(units=1,\n",
    "               kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "               activation=\"sigmoid\",\n",
    "               name='clases')(logits)\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output, name='BERT_BinaryClass')\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history=model.fit(train_dataset.batch(batch_size), epochs=n_epochs, batch_size=batch_size, validation_data=test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Fine-tuning Keras')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset.batch(batch_size))\n",
    "predict_label = list(map(lambda l: id2label[l], predict.ravel()>0.5))\n",
    "\n",
    "print(classification_report(Y_test_label, predict_label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando el embedding de `[CLS]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "#######################################\n",
    "### --------- Setup BERT ---------- ###\n",
    "\n",
    "\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = AutoConfig.from_pretrained(nombre_modelo)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load the Transformers BERT model\n",
    "transformer_model = TFAutoModel.from_pretrained(nombre_modelo, config = config)\n",
    "\n",
    "#######################################\n",
    "### ------- Build the model ------- ###\n",
    "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), name='attention_mask', dtype='int32') \n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "last_hidden_state = transformer_model(input_ids, attention_mask)[0]  # (bs, seq, dim)\n",
    "logits = Dropout(0.1)(last_hidden_state[:,0,:])  # (bs, dim)\n",
    "# Then build your model output\n",
    "output = Dense(units=1,\n",
    "               kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "               activation=\"sigmoid\",\n",
    "               name='clases')(logits)\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output, name='BERT_BinaryClass')\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history=model.fit(train_dataset.batch(batch_size), epochs=n_epochs, batch_size=batch_size, validation_data=test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Fine-tuning Keras')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset.batch(batch_size))\n",
    "predict_label = list(map(lambda l: id2label[l], predict.ravel()>0.5))\n",
    "\n",
    "print(classification_report(Y_test_label, predict_label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del modelo BERT pre-entrenado para generar doc embeddings\n",
    "Usamos el modelo BERT pre-entrenado (sin hacer fine-tuning) para generar los embeddings de los documentos. Luego entrenamos un clasificador binario sobre nuestro corpus.  \n",
    "Probamos con la salida `pooler_output` y el `last_hidden_state` del primer token (CLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si quisiéramos obtener los tokens de las entradas tendríamos que hacer\n",
    "tokenized = X_train_tweets.apply(\n",
    "    (lambda x: tokenizer.encode(x, \n",
    "                                add_special_tokens=True,\n",
    "                                truncation=True,\n",
    "                                padding='max_length',\n",
    "                                max_length=MAX_SEQUENCE_LENGTH)))\n",
    "entradas = tf.convert_to_tensor(tokenized.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como ya los hemos calculado antes los extraemos de la variable de encodings\n",
    "entradas_train = train_encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = AutoConfig.from_pretrained(nombre_modelo)\n",
    "config.output_hidden_states = False\n",
    "#volvemos a cargar el modelo con la configuración anterior\n",
    "transformer_model = TFAutoModel.from_pretrained(nombre_modelo, config = config)\n",
    "\n",
    "#calculamos los doc embeddings sobre las entradas (inferencia)\n",
    "output_train = transformer_model(entradas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La salida tiene el valor de la última capa oculta y el 'pooled_output' de toda la secuencia\n",
    "output_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cogemos la salida del pooler_output como embedding de documento\n",
    "salidas_train = output_train.pooler_output\n",
    "salidas_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculamos los vectores del conjunto de entrenamiento\n",
    "#Como ya los hemos calculado antes los extraemos de la variable de encodings\n",
    "entradas_test = test_encodings['input_ids']\n",
    "output_test = transformer_model(entradas_test)\n",
    "salidas_test = output_test.pooler_output\n",
    "salidas_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos y validamos con scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "#Entrenamos el modelo con el conjunto de train y validamos\n",
    "modelLR.fit(salidas_train, Y_train)\n",
    "prediccion = modelLR.predict(salidas_test)\n",
    "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "Repite el modelo utilizando el embedding del token `[CLS]` en lugar de la salida `pooled_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraemos vectores\n",
    "#Cogemos la salida del pooler_output como embedding de documento\n",
    "salidas_train = output_train.last_hidden_state[:,0,:]\n",
    "print(salidas_train.shape)\n",
    "salidas_test = output_test.last_hidden_state[:,0,:]\n",
    "print(salidas_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el modelo con el conjunto de train y validamos\n",
    "modelLR.fit(salidas_train, Y_train)\n",
    "prediccion = modelLR.predict(salidas_test)\n",
    "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de la máscara de atención\n",
    "Repetimos usando las máscaras de atención (tokens válidos) para ver si mejora el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = train_encodings['attention_mask']\n",
    "mask_test = test_encodings['attention_mask']\n",
    "\n",
    "#calculamos los doc embeddings sobre las entradas\n",
    "last_hidden_states = transformer_model.predict({'input_ids':entradas_train, 'attention_mask':mask_train})\n",
    "salidas_train = last_hidden_states.last_hidden_state[:,0,:]\n",
    "last_hidden_states = transformer_model.predict({'input_ids':entradas_test, 'attention_mask':mask_test})\n",
    "salidas_test = last_hidden_states.last_hidden_state[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el modelo con el conjunto de train y validamos\n",
    "modelLR.fit(salidas_train, Y_train)\n",
    "prediccion = modelLR.predict(salidas_test)\n",
    "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la 'pooled_output' y entrenamos con este tensor\n",
    "salidas_train = transformer_model.predict({'input_ids':entradas_train, 'attention_mask':mask_train})[1]\n",
    "salidas_test = transformer_model.predict({'input_ids':entradas_test, 'attention_mask':mask_test})[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el modelo con el conjunto de train y validamos\n",
    "modelLR.fit(salidas_train, Y_train)\n",
    "prediccion = modelLR.predict(salidas_test)\n",
    "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos y validamos con una capa densa\n",
    "Entrenamos con un modelo DL básico equivalente al que usa `TFBertForSequenceClassification`, pero aquí no se ajustan los pesos del modelo BERT.  \n",
    "Definimos un modelo con la API funcional de Keras usando los vectores de salida del BERT como sentence embedding a la entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your model input\n",
    "doc_embeddings = Input(shape=(config.hidden_size,), name='doc_embeddings', dtype='float32')\n",
    "\n",
    "# Then build your model output\n",
    "pooled_output = Dropout(0.1)(doc_embeddings)  # (bs, dim)\n",
    "output = Dense(units=1,\n",
    "               kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "               activation=\"sigmoid\",\n",
    "               name='clases')(pooled_output)\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=doc_embeddings, outputs=output, name='Binary_BertPretrained')\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-08) #el entrenamiento es muy sensible a estos valores\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(salidas_train, np.asarray(Y_train), epochs=n_epochs, batch_size=batch_size, validation_data=(salidas_test, np.asarray(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('embeddings BERT + capa densa')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(salidas_test)\n",
    "predict_clases = predict>0.5\n",
    "\n",
    "print(classification_report(Y_test, predict_clases, target_names=['N','P']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es equivalente a utilizar la capa del TFBertModel sin re-entrenar en el modelo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Transformers BERT model\n",
    "transformer_model = TFAutoModel.from_pretrained(nombre_modelo, config = config, name=\"Bert_model\")\n",
    "transformer_model.bert.trainable = False #congelamos la actualización de las capas del BERT\n",
    "\n",
    "#######################################\n",
    "### ------- Build the model ------- ###\n",
    "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), name='attention_mask', dtype='int32') \n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "cls_output = transformer_model(input_ids, attention_mask)[0]  # (bs, seq, dim)\n",
    "logits = Dropout(0.1)(cls_output[:,0,:])  # (bs, dim)\n",
    "# Then build your model output\n",
    "output = Dense(units=1,\n",
    "               kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "               activation=\"sigmoid\",\n",
    "               name='clases')(logits)\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output, name='BERT_BinaryClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history=model.fit(train_dataset.batch(batch_size), epochs=n_epochs, batch_size=batch_size, validation_data=test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('BERT pre-entrenado con máscara atención')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset.batch(batch_size))\n",
    "predict_clases = predict>0.5\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, predict_clases, target_names=['N','P'], zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de las capas internas del modelo BERT\n",
    "Por último probamos a usar las 4 últimas capas ocultas de `[CLS]` concatenadas para la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Transformers BERT model\n",
    "transformer_model = TFAutoModel.from_pretrained(nombre_modelo, output_hidden_states=True, name=\"Bert_model\")\n",
    "transformer_model.bert.trainable = False #congelamos la actualización de las capas del BERT\n",
    "\n",
    "#######################################\n",
    "### ------- Build the model ------- ###\n",
    "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), name='attention_mask', dtype='int32') \n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "hidden_states = transformer_model(input_ids, attention_mask)[2]  # (layer, bs, seq, dim)\n",
    "\n",
    "hidden_states_size = 4 # count of the last states \n",
    "hiddes_states_ind = list(range(-hidden_states_size, 0, 1))\n",
    "\n",
    "selected_hiddes_states = tf.keras.layers.concatenate(tuple([hidden_states[i][:,0,:] for i in hiddes_states_ind])) #first token of each layer\n",
    "\n",
    "pooled_output = Dropout(0.1)(selected_hiddes_states)  # (bs, dim)\n",
    "# Then build your model output\n",
    "output = Dense(units=1,\n",
    "               kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "               activation=\"sigmoid\",\n",
    "               name='clases')(pooled_output)\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output, name='BERT_BinaryClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de parámetros en este caso viene dado por el número de capas a contatenar x dimensiones embedding + BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 768 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history=model.fit(train_dataset.batch(batch_size), epochs=n_epochs, batch_size=batch_size, validation_data=test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('BERT pre-entrenado últimas 4 capas + densa')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset.batch(batch_size))\n",
    "predict_clases = predict>0.5\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, predict_clases, target_names=['N','P']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos este modelo entrenando toda la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Transformers BERT model\n",
    "transformer_model = TFAutoModel.from_pretrained(nombre_modelo, output_hidden_states=True, name=\"Bert_model\")\n",
    "\n",
    "#######################################\n",
    "### ------- Build the model ------- ###\n",
    "# TF Keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), name='attention_mask', dtype='int32') \n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "hidden_states = transformer_model(input_ids, attention_mask)[2]  # (layer, bs, seq, dim)\n",
    "\n",
    "hidden_states_size = 4 # count of the last states \n",
    "hiddes_states_ind = list(range(-hidden_states_size, 0, 1))\n",
    "\n",
    "selected_hiddes_states = tf.keras.layers.concatenate(tuple([hidden_states[i][:,0,:] for i in hiddes_states_ind])) #first token of each layer\n",
    "\n",
    "pooled_output = Dropout(0.1)(selected_hiddes_states)  # (bs, dim)\n",
    "# Then build your model output\n",
    "output = Dense(units=1,\n",
    "               kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "               activation=\"sigmoid\",\n",
    "               name='clases')(pooled_output)\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output, name='BERT_BinaryClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history=model.fit(train_dataset.batch(batch_size), epochs=n_epochs, batch_size=batch_size, validation_data=test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('BERT últimas 4 capas + densa')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset.batch(batch_size))\n",
    "predict_clases = predict>0.5\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, predict_clases, target_names=['N','P']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "845820aa507da265ae36614cc34fbe135a81ea548fdd838f22fa22ab25d20da4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
