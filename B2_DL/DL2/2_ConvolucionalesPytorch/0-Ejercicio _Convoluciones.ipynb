{"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["\n","![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \n","\n","#**Máster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\n","---"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YeuAheYyhdZw"},"outputs":[],"source":["# import libraries\n","import numpy as np\n","import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"0HOkOefftqyg"},"source":["# Problemas sobre convoluciones: tamaños, padding y stride\n","\n","Muy buena visualización sobre convoluciones, padding, stride y dilation en esta [dirección](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n"]},{"cell_type":"markdown","metadata":{"id":"EiLI77Lh9yms"},"source":["### Convoluciona una imagen de tamaño  1x256x256 para obtener un resultado de 1x252x84"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1704785951155,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"VhIKo0_iaGz2","outputId":"ff81c4e7-8d3e-4552-96bf-e7aa374fc65a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected size: [  1 252  84]\n","Empirical size: [252, 84]\n"]}],"source":["# parameters\n","inChans  = 1 # B/N or RGB\n","imsize   = [256,256]\n","outChans = 1\n","krnSize  = 7 # should be an odd number\n","stride   = (1,3)\n","padding  = 1\n","\n","# create the instance\n","c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n","\n","# create an image\n","img = torch.rand(1,inChans,imsize[0],imsize[1])\n","\n","# run convolution and compute its shape\n","resimg = c(img)\n","empSize = torch.squeeze(resimg).shape\n","\n","# compute the size of the result according to the formula\n","expectSize = np.array([outChans,0,0],dtype=int)\n","expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n","expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n","\n","# check the size of the output\n","print(f'Expected size: {expectSize}')\n","print(f'Empirical size: {list(empSize)}')"]},{"cell_type":"markdown","metadata":{"id":"fYebEMtDK55U"},"source":["Ten en cuenta los siguientes detalles:\n","+ El tamaño de la imagen (imsize) se sumnistra con una lista: [alto,ancho].\n","+ El tamaño de filtro/kernel debe ser un valor impar, y es el mismo de ancho que de alto.\n","+ El padding tambien se aplica igual al ancho que al alto.\n","+ El desplazamiento (stride) puede ser distinto en vertical (para la altura) y en horizontal (para la anchura). Se suministra como una tupla: (stride1, stride2).\n","+ El tamaño final se redondea hacia abajo.\n","+ NO aplicamos capas de pooling, solo UNA convolución."]},{"cell_type":"markdown","metadata":{"id":"sCWyuNySDagy"},"source":["# A resolver"]},{"cell_type":"markdown","metadata":{"id":"bteB_P10DSCM"},"source":["### 1) Convolucionar una imagen de tamaño 3x64x64 para obtener un resultado de 10x28x28"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"_8CM5aVADSCN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Required size: [10, 28, 28]\n","Expected size: [10 28 28]\n","Empirical size: [10, 28, 28]\n"]}],"source":["# parameters\n","inChans  = 3\n","imsize   = [64, 64]\n","outChans = 10\n","krnSize  = 9\n","stride   = (2, 2)\n","padding  = 0\n","\n","# create the instance\n","c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n","\n","# create an image\n","img = torch.rand(1,inChans,imsize[0],imsize[1])\n","\n","# run convolution and compute its shape\n","resimg = c(img)\n","empSize = torch.squeeze(resimg).shape\n","\n","# compute the size of the result according to the formula\n","expectSize = np.array([outChans,0,0],dtype=int)\n","expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n","expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n","\n","# check the size of the output\n","print(\"Required size: [10, 28, 28]\")\n","print(f'Expected size: {expectSize}')\n","print(f'Empirical size: {list(empSize)}')"]},{"cell_type":"markdown","metadata":{"id":"2jfWAkiWDWU7"},"source":["### 2) Convolucionar una imagen de tamaño 3x196x96 para obtener un resultado de 5x66x49\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"XieXWJ9gDWU7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Required size: [5, 66, 49]\n","Expected size: [ 5 66 49]\n","Empirical size: [5, 66, 49]\n"]}],"source":["# parameters\n","inChans  = 3\n","imsize   = [196, 96]\n","outChans = 5\n","krnSize  = 1\n","stride   = (3, 2)\n","padding  = 1\n","\n","# create the instance\n","c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n","\n","# create an image\n","img = torch.rand(1,inChans,imsize[0],imsize[1])\n","\n","# run convolution and compute its shape\n","resimg = c(img)\n","empSize = torch.squeeze(resimg).shape\n","\n","# compute the size of the result according to the formula\n","expectSize = np.array([outChans,0,0],dtype=int)\n","expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n","expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n","\n","# check the size of the output\n","print(\"Required size: [5, 66, 49]\")\n","print(f'Expected size: {expectSize}')\n","print(f'Empirical size: {list(empSize)}')"]},{"cell_type":"markdown","metadata":{"id":"XdRhRVE7FfN2"},"source":["### 3) Convolucionar una imagen de tamaño 1x32x32 para obtener un resultado de 6x28x28"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"t5f9x7HjFfN2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Required size: [6, 28, 28]\n","Expected size: [ 6 28 28]\n","Empirical size: [6, 28, 28]\n"]}],"source":["# note: these dimensions are the input -> first hidden layer of the famous LeNet-5\n","\n","# parameters\n","inChans  = 1 \n","imsize   = [32, 32]\n","outChans = 6\n","krnSize  = 5\n","stride   = (1, 1)\n","padding  = 0\n","\n","# create the instance\n","c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n","\n","# create an image\n","img = torch.rand(1,inChans,imsize[0],imsize[1])\n","\n","# run convolution and compute its shape\n","resimg = c(img)\n","empSize = torch.squeeze(resimg).shape\n","\n","# compute the size of the result according to the formula\n","expectSize = np.array([outChans,0,0],dtype=int)\n","expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n","expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n","\n","# check the size of the output\n","print(\"Required size: [6, 28, 28]\")\n","print(f'Expected size: {expectSize}')\n","print(f'Empirical size: {list(empSize)}')"]},{"cell_type":"markdown","metadata":{"id":"vrHM60CkF2pl"},"source":["### 4) Convolucionar una imagen de tamaño 3x227x227 para obtener un resultado de 96x55x55"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"d0R0DITNF2pl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Required size: [96, 55, 55]\n","Expected size: [96 55 55]\n","Empirical size: [96, 55, 55]\n"]}],"source":["# note: these dimensions are the input -> first hidden layer of the famous AlexNet\n","\n","# parameters\n","inChans  = 3\n","imsize   = [227, 227]\n","outChans = 96\n","krnSize  = 9\n","stride   = (4, 4)\n","padding  = 0\n","\n","# create the instance\n","c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n","\n","# create an image\n","img = torch.rand(1,inChans,imsize[0],imsize[1])\n","\n","# run convolution and compute its shape\n","resimg = c(img)\n","empSize = torch.squeeze(resimg).shape\n","\n","# compute the size of the result according to the formula\n","expectSize = np.array([outChans,0,0],dtype=int)\n","expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n","expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n","\n","# check the size of the output\n","print(\"Required size: [96, 55, 55]\")\n","print(f'Expected size: {expectSize}')\n","print(f'Empirical size: {list(empSize)}')"]},{"cell_type":"markdown","metadata":{"id":"cvoJ9X5IHtl5"},"source":["### 5) Convolucionar una imagen de tamaño 3x224x224 para obtener un resultado de 64x224x224"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"PG4InNjQHtl6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Required size: [64, 224, 224]\n","Expected size: [ 64 224 224]\n","Empirical size: [64, 224, 224]\n"]}],"source":["# note: these dimensions are the input -> first hidden layer of the famous VGG-16\n","\n","# parameters\n","inChans  = 3\n","imsize   = [224, 224]\n","outChans = 64\n","krnSize  = 3\n","stride   = (1,1)\n","padding  = 1\n","\n","# create the instance\n","c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n","\n","# create an image\n","img = torch.rand(1,inChans,imsize[0],imsize[1])\n","\n","# run convolution and compute its shape\n","resimg = c(img)\n","empSize = torch.squeeze(resimg).shape\n","\n","# compute the size of the result according to the formula\n","expectSize = np.array([outChans,0,0],dtype=int)\n","expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n","expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n","\n","# check the size of the output\n","print(\"Required size: [64, 224, 224]\")\n","print(f'Expected size: {expectSize}')\n","print(f'Empirical size: {list(empSize)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8zLORJxEpqY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-P_8vOjIVeP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDmDF1pPIVgn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXW4RaOAIVi4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLsIIb--IVkv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzMISeuyIVmx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTTygSkjIVov"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbnr43FxIVq_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yn6a-vl4IVs6"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mb0fBtyfEpsj"},"source":["# Soluciones (No hacer trampas!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwR_5NyFErcf"},"outputs":[],"source":["# 1)\n","inChans  = 3\n","imsize   = [64,64]\n","outChans = 10\n","krnSize  = 9\n","stride   = (2,2)\n","padding  = 0\n","\n","# 2)\n","inChans  = 3\n","imsize   = [196,96]\n","outChans = 5\n","krnSize  = 5\n","stride   = (3,2)\n","padding  = 3\n","\n","# 3)\n","inChans  = 1\n","imsize   = [32,32]\n","outChans = 6\n","krnSize  = 5\n","stride   = (1,1)\n","padding  = 0\n","\n","# 4)\n","inChans  = 3\n","imsize   = [227,227]\n","outChans = 96\n","krnSize  = 11\n","stride   = (4,4)\n","padding  = 1\n","\n","# 5)\n","inChans  = 3\n","imsize   = [224,224]\n","outChans = 64\n","krnSize  = 3\n","stride   = (1,1)\n","padding  = 1"]},{"cell_type":"markdown","metadata":{"id":"H3NgDJWWvrPT"},"source":["# Referencias\n"]},{"cell_type":"markdown","metadata":{"id":"DDxYpo0uwjaI"},"source":["+ Doc oficial : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n","+ Curso **A deep understanding of deep learning**: https://www.udemy.com/course/deeplearning_x/\n","+ Padding y stride: https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529\n","+ Padding y stride; https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html"]}],"metadata":{"colab":{"collapsed_sections":["H3NgDJWWvrPT"],"provenance":[{"file_id":"19imE5cZVTySOpFOe4Uvw97qMPMgs0MJ2","timestamp":1619520513379},{"file_id":"1GRajDS-VF5z8IslzZuMqbis3X6HDD-Uo","timestamp":1619468278654},{"file_id":"1m0n2-UmB2tJiIDadlFkE6L5A4iZSqeBf","timestamp":1619459134813},{"file_id":"19G9gTeBlYPQ-s3VS_3K2bVFtKTP344j6","timestamp":1619444797767},{"file_id":"1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM","timestamp":1619155961717},{"file_id":"1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW","timestamp":1617803880910},{"file_id":"15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4","timestamp":1617737766196},{"file_id":"1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ","timestamp":1617734878578},{"file_id":"1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j","timestamp":1617196833019},{"file_id":"1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H","timestamp":1617124341706},{"file_id":"1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn","timestamp":1616697516760},{"file_id":"1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg","timestamp":1616615469755},{"file_id":"1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK","timestamp":1616608248670}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
