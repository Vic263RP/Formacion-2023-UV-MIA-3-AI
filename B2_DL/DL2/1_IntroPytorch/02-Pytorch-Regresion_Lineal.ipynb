{"cells":[{"cell_type":"markdown","metadata":{"id":"ghPSdPkCrLyk"},"source":["![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \n","\n","#**Máster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"9WOIeW_unJDd"},"source":["#<center>**Regresion Lineal y descenso de gradiente con PyTorch**</center>"]},{"cell_type":"markdown","metadata":{"id":"xXt0ghevf9Q_"},"source":["# Regresión lineal con operaciones"]},{"cell_type":"markdown","metadata":{"id":"uriFNBrirLyw"},"source":["Este notebook cubre los siguientes aspectos:\n","\n","- Regresion lineal y ajuste empleando descenso de gradiente\n","- Implementación de modelos de este tipo empleando tensores Pytorch\n","- Entrenamiento del modelo de regresion lineal usando el descenso de gradiente\n","- Implementación de ambos empleando las clases y métodos específicamente preparados en Pytorch"]},{"cell_type":"markdown","metadata":{"id":"8RQ6rbisrLy0"},"source":["## Regresión lineal\n","\n","En este notebook vamos a repasar una de las técnicas básicas y fundacionales del aprendizaje máquina y de las redes neuronales: la *regresión lineal*.\n","Vamos a crear un modelo que prediga la cosecha a recoger de manzanas y naranjas (*target variables*) a partir de las observaciones de temperatura, lluvia y humedad (*input variables o features*) en una región. Estos son los datos de entrenamiento:\n","\n","![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n","\n","En un modelo de regresión lineal cada variable dependiente o *target* es estimada como la suma ponderada de las variables de entrada, más un valor constante de ajuste, conocido como *bias* :\n","\n","```\n","yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n","yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n","```\n","\n","Visualmente esto significa que la cosecha de manzanas es una función lineal *o planar*  de la temperatura, la lluvía y la humedad:\n","\n","![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)"]},{"cell_type":"markdown","metadata":{"id":"qu9QdXVjrLy1"},"source":["La parte de aprendizaje en una función de regresión lineal consiste en obtener un conjunto de pesos o coeficientes `w11, w12,... w23, b1 y b2` empleando los datos de entrenamiento, con la finalidad de poder hacer predicciones para nuevos datos.\n","Los pesos *aprendidos* serán empleados para predecir los valores de cosecha de manzanas y naranjas en una región empleando los datos de temperatura, lluvia y humedad de esa región. .\n","\n","El entrenamiento que vamos a realizar consiste en ir ajustando los pesos ligeramente muchas veces para ir obteniendo mejores predicciones a partir de los valores resultantes conocidos y empleando una técnica de optimización ampliamente usada y conocida llamada *descenso de gradiente*.\n","\n","Empezamos por importar Numpy y Pytorch."]},{"cell_type":"code","execution_count":473,"metadata":{"id":"sOgTUnE1rLy1"},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"CP0-TRTLrLy2"},"source":["## Datos de entrenamiento\n","\n","Vamos a emplear para representar los datos de entrenamiento dos matrices: `inputs` y `targets`, cada fila será una observación y cada coluumna una variable\n","."]},{"cell_type":"code","execution_count":474,"metadata":{"id":"3ZL_bN93rLy2"},"outputs":[],"source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43],\n","                   [91, 88, 64],\n","                   [87, 134, 58],\n","                   [102, 43, 37],\n","                   [69, 96, 70]], dtype='float32')"]},{"cell_type":"code","execution_count":475,"metadata":{"id":"MWPWDvZwrLy3"},"outputs":[],"source":["# Targets (apples, oranges)\n","targets = np.array([[56, 70],\n","                    [81, 101],\n","                    [119, 133],\n","                    [22, 37],\n","                    [103, 119]], dtype='float32')"]},{"cell_type":"markdown","metadata":{"id":"lH_7f7IlrLy4"},"source":["Separamos las entradas y los targets porque hemos de trabajar separadamente con cada una de ellas. Por otro lado, se han creado como arrays Numpy porque es la forma habitual en que los vamos a encontrar: importación de los datos CSV como arrays, prepararlos y finalmente convertirlos a tensores de Pytorch.\n","\n","Los convertimos a tensores PyTorch."]},{"cell_type":"code","execution_count":476,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1673631801613,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"Hi62YPcXrLy5","outputId":"f65c4c1f-9972-4721-9d64-87b33f5972c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]])\n","tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.]])\n"]}],"source":["inputs = torch.from_numpy(inputs)\n","targets = torch.from_numpy(targets)\n","print(inputs)\n","print(targets)"]},{"cell_type":"markdown","metadata":{"id":"SwGf-LYnrLy8"},"source":["## Modelo de regresion lineal desde cero\n","\n","Los pesos y ajustes (`w11, w12,... w23, b1 y b2`) son representados como matrices, que en un primer momento contienen valores iniciales aleatorios.\n","can also be represented as matrices, initialized as random values.\n","La primera fila de `w` y el primer elemento de `b` son los coeficientes necesarios para calcular la primera variable, i.e., la cosecha de manzanas, y de forma similar la segunda para las naranjas."]},{"cell_type":"code","execution_count":477,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1673631845543,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"z0-el_zmrLy9","outputId":"5ee0f44c-b0a3-4e9c-dddd-db0cdfd1c2ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.5287, -0.5115, -0.3799],\n","        [ 0.7919,  0.8849, -0.5109]], requires_grad=True)\n","tensor([0.1604, 0.4857], requires_grad=True)\n"]}],"source":["# Weights and biases\n","w = torch.randn(2, 3, requires_grad=True)\n","b = torch.randn(2, requires_grad=True)\n","print(w)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"9CDG649GrLy-"},"source":["`torch.randn` crea un tensor con las dimensiones dadas con elementos tomados de forma aleatoria de una [distribución normal](https://en.wikipedia.org/wiki/Normal_distribution) con media 0 y desviación estandard 1.\n","\n","Por tanto, nuestro *modelo* es real y simplemente una función que realiza una multiplicación de matrices entre las entradas `inputs` y los pesos `w` (transpuestos) y añade el factor bias `b` (replicado para cada observación, diferente para cada target).\n","\n","![matrix-mult](https://i.imgur.com/WGXLFvA.png)\n","\n","Empelando las operaciones disponibles en Pytorch, podemos definir el modelo como sigue:"]},{"cell_type":"code","execution_count":478,"metadata":{"id":"6ZnQB-BBrLy-"},"outputs":[],"source":["def model(x):\n","    return x @ w.t() + b"]},{"cell_type":"markdown","metadata":{"id":"s7Ym5lihrLy-"},"source":["`@` representa la multuiplicación de matrices en PyTorch, y el método `.t` devuelve un tensor transpuesto.\n","\n","La matriz obtenida al emplear los datos de entrada con los coeficientes del modelo es un conjunto de predicciones para las variables objetivo *targets*."]},{"cell_type":"code","execution_count":479,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1673632328686,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"pg7Ymw0PrLy_","outputId":"1407fc04-6a6c-4086-c913-a97cdbfd2efe"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ -89.0440,   95.6169],\n","        [-117.2814,  117.7256],\n","        [-136.4178,  158.3290],\n","        [ -89.8192,  100.4109],\n","        [-112.0223,  104.3167]], grad_fn=<AddBackward0>)\n"]}],"source":["# Genero predicciones (iniciales)\n","preds = model(inputs)\n","print(preds)"]},{"cell_type":"code","execution_count":480,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1673632331072,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"FkH2oYmmrLzA","outputId":"eb86314e-4188-4af4-91da-50d33d1928b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.]])\n"]}],"source":["# Comparamos con los targets reales\n","print(targets)"]},{"cell_type":"markdown","metadata":{"id":"OZGtGJcjrLzA"},"source":["Podemos ver una gran diferencia entre las predicciones obtenidas inicialmente y los valores reales. Esto ocurre porque los pesos y bias empleados han sido inicializados aleatoriamente y no podemos esperar que esos valores se correspondan y funcionen bien directamente. Un modelo inicializado aleatoriamente no está preparado para funcionar. Se trata simplemente de un inicio.\n"]},{"cell_type":"markdown","metadata":{"id":"0KmMqA8IrLzA"},"source":["## Función de pérdida o de coste\n","\n","Antes de mejorar el modelo, necesitamos evaluar como de bien está funcionando. Para ello comparamos los resultados obtenidos con los resultados que se deberían obtener. En este caso vamos a emplear como función evladora el error cuadrático medio o **mean squared error** (MSE).\n","Desglosando el pseeudo código para calcularlo sería algo así:\n","* Calculamos la diferencia entre las dos matrices(`preds` y `targets`).\n","* Elevamos al cuadrado todos los elementos y de esa forma evitamos valores negativos.\n","* Calculamos la media de los elementos en una matriz resultante.\n","\n","El resultado de este cálculo es un único número (MSE)."]},{"cell_type":"code","execution_count":481,"metadata":{"id":"F0fTNoWxrLzB"},"outputs":[],"source":["# MSE\n","def mse(t1, t2):\n","    diff = t1 - t2\n","    return torch.sum(diff * diff) / diff.numel()"]},{"cell_type":"markdown","metadata":{"id":"D95gt0_CrLzB"},"source":["`torch.sum` retorna la suma de todos los elementos de un tensor. El método `.numel` retorna el número de elementos de un tensor.\n","\n","Calculamos el MSE para las predicciones que hemos obtenidos con nuestro modelo."]},{"cell_type":"code","execution_count":482,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194,"status":"ok","timestamp":1673632456382,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"63YeUzDTrLzC","outputId":"cdda35cd-7ced-4f75-89f7-4f0539a59201"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(19014.3711, grad_fn=<DivBackward0>)\n"]}],"source":["loss = mse(preds, targets)\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"cprvF2KhrLzC"},"source":["Así es como podemos interpretar el resultado: *de media, cada elemento en la predicción difiere de su valor correcto la raíz cuadrada del error obtenido*.\n","Y eso es bastante malo, dado que los resultados que pretendemos predecir están en un rango de entre 50-200. El resultado se suele llamar *loss* porque implica la pérdida que introduce el modelo entre lo que obtenemos y lo que deberíamos obtener. Cuanto **menor es la pérdida** o error, **mejor es el modelo**."]},{"cell_type":"code","execution_count":483,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1673632500287,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"cCI45hdUkrpC","outputId":"6eccdc37-d048-4892-9ca1-1523cc938c13"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(137.8926, grad_fn=<SqrtBackward0>)\n"]}],"source":["#RMSE\n","print(torch.sqrt(loss))"]},{"cell_type":"markdown","metadata":{"id":"EI0qiKAFrLzD"},"source":["## Cálculo de gradientes\n","\n","Con Pytorch podemos calcular automáticamente los gradientes o derivadas del error con respecto de los pesos y del bias, porque hemos definido el parámetro `requires_grad` a `True`. Vamos a ver lo útil que resulta esta funcionalidad."]},{"cell_type":"code","execution_count":484,"metadata":{"id":"beq8p1gTrLzD"},"outputs":[],"source":["# Calculamos gradientes\n","loss.backward()"]},{"cell_type":"markdown","metadata":{"id":"G7BcKtcUrLzD"},"source":["Los gradientes están ahora guardados en la propiedad `.grad` de cada tensor. Nótese que la derivada del error w.r.t. a una matriz de pesos es una matriz también de las mismas dimensiones."]},{"cell_type":"code","execution_count":485,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1673632553861,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"-0ZMorGtrLzE","outputId":"212f3234-b1e8-45aa-ee48-409a72267e2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.5287, -0.5115, -0.3799],\n","        [ 0.7919,  0.8849, -0.5109]], requires_grad=True)\n","tensor([[-15419.0537, -17368.6113, -10586.0000],\n","        [  2210.0896,   1579.8677,    991.8834]])\n"]}],"source":["# Gradients for weights\n","print(w)\n","print(w.grad)"]},{"cell_type":"markdown","metadata":{"id":"3K1ZnLVvrLzE"},"source":["## Ajuste de pesos y bias para reducir el error\n","\n","La pérdida es una [función cuadrática](https://en.wikipedia.org/wiki/Quadratic_function) de nuestros pesos y biases,  y nuestro objetivo es encontrar el conjunto de pesos donde la pérdida sea la más baja. Si dibujamos un gráfico de la pérdida con respecto a cualquiera de los pesos y bias, tendría el aspecto de la figura que se muestra abajo. Un importante detalle sobre cálculo es que el gradiente indica precisamente el ratio de cambio de la pérdida, es decir, la [pendiente](https://en.wikipedia.org/wiki/Slope) con respecto a los pesos y biases.\n","\n","Si el gradiente es **positivo**\n","\n","* **incrementar** el peso de esa variable ligeramente **incrementará** el error (*loss*)\n","* **reducir** el peso ligeramente **reducirá** el error.\n","\n","![postive-gradient](https://i.imgur.com/WLzJ4xP.png)\n","\n","Si el gradiente es **negativo**:\n","\n","* **incrementar** el peso de esa variable ligeramente **reducirá** el error (*loss*)\n","* **reducir** el peso ligeramente **aumentará** el error.\n","\n","![negative=gradient](https://i.imgur.com/dvG2fxU.png)\n","\n","El incremento o reducción del error cambiando el peso de un elemento es proporcional al gradiente del error con respecto a dicho elemento (*variable*).\n","Esta observación supone la base del algoritmo de optimización por *descenso de gradiente* que será el que emplearemos para mejorar nuestro modelo (por  _descenso_ a lo largo del _gradiente_).\n","\n","Dadas las relaciones descritas entre incremento/reducción del peso e incremento reducción del error, la forma en que podemos reducirlo consistirá en **restar** al peso de cada variable una pequeña cantidad proporcional al gradiente con respecto a dicha variable."]},{"cell_type":"code","execution_count":486,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1673632953679,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"afl_XeJBrLzF","outputId":"94f7cb52-cc02-4138-cdd6-6315437f95df"},"outputs":[{"data":{"text/plain":["tensor([[-0.5287, -0.5115, -0.3799],\n","        [ 0.7919,  0.8849, -0.5109]], requires_grad=True)"]},"execution_count":486,"metadata":{},"output_type":"execute_result"}],"source":["w\n","#w.grad"]},{"cell_type":"code","execution_count":487,"metadata":{"id":"zN-9rgzXSdZ6"},"outputs":[],"source":["#w = w - w.grad*1e-5"]},{"cell_type":"code","execution_count":488,"metadata":{"id":"rBNco83TrLzF"},"outputs":[],"source":["# Actualizamos los pesos y bias\n","with torch.no_grad():\n","    # w -= w.grad * 1e-5  # w = w - w.grad*1e-5\n","    # b -= b.grad * 1e-5\n","    w.sub_(w.grad * 1e-5)\n","    b.sub_(b.grad * 1e-5)"]},{"cell_type":"code","execution_count":489,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1673632962248,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"LJACRg2a7Iv7","outputId":"d62a406d-e70c-47a7-d6e2-1d8f35919b1e"},"outputs":[{"data":{"text/plain":["tensor([[-0.3745, -0.3379, -0.2741],\n","        [ 0.7698,  0.8691, -0.5208]], requires_grad=True)"]},"execution_count":489,"metadata":{},"output_type":"execute_result"}],"source":["w"]},{"cell_type":"markdown","metadata":{"id":"REY7fBs3rLzG"},"source":["Como se puede observar, hemos multiplicado los gradientes obteenidos por un número muy pequeño (`10^-5` en este caso). Esto es para asegurar que no modificamos los pesos en cantidades muy grandes. Queremos ir tomando pequeños pasos hacia la dirección de descenso siempre, no un gran salto que nos pueda desviar. Este número es lo que llamamos el ratio de aprendizaje *learning rate* del algoritmo.\n","\n","Podemos usar `torch.no_grad` para indicar a PyTorch que no queremos que internamente vaya calculando gradientescuando actualizamos los pesos, ya que no es necesario y únicamente provocamos más carga computacional."]},{"cell_type":"code","execution_count":490,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":196,"status":"ok","timestamp":1673632989908,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"-C_24VbFrLzG","outputId":"166f816e-56ff-4f11-a060-b0deb2e7a10b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(13005.9395, grad_fn=<DivBackward0>)\n"]}],"source":["# Verificamos que la pérdida se ha reducido (probablemente muy poco)\n","preds = model(inputs)\n","loss = mse(preds, targets)\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"cVnMi-FrrLzG"},"source":["A continuación, será necesario resetear los gradientes a cero empleando el   método `.zero_()`. Necesitamos hacer esto porque PyTorch acumula los gradientes. Si no lo hacemos, la próxima vez que invocamos el método `.backward` en la función de pérdida, los nuevos valores de gradientes se sumarían a los existentes, lo que llevaría a resultados incorrectos e inesperados."]},{"cell_type":"code","execution_count":491,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190,"status":"ok","timestamp":1673633058319,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"_oXTtLdlrLzH","outputId":"8e88cc0d-2836-4ac0-9910-228fc8c7e434"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","tensor([0., 0.])\n"]}],"source":["w.grad.zero_()\n","b.grad.zero_()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"markdown","metadata":{"id":"vBXlTCArrLzH"},"source":["## Entrenamiento del modelo usando descenso de gradiente\n","\n","Como hemos visto, podemos reducir la pérdida y mejorar nuestro modelo empleando la técnica de descenso de gradiente. Así pues, podemos _entrenar_ el modelo siguiendo los siguientes pasos:\n","\n","1. Generamos predicciones\n","\n","2. Calculamos el error/pérdida\n","\n","3. Calculamos los gradientes c.r.a. los pesos y biases\n","\n","4. Ajustamos los pesos restando una pequeña cantidad proporcianal a los gradientes obtenidos\n","\n","5. Reseteamos los gradientes a cero para repetir la operación\n","\n","Vamos a implementar esto paso a paso."]},{"cell_type":"code","execution_count":492,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1673633085690,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"betZe18grLzI","outputId":"f0bdeadc-154d-4063-fe78-b12e2c291c0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-61.5973,  92.5182],\n","        [-81.1888, 113.6891],\n","        [-93.5876, 153.7137],\n","        [-62.7046,  97.1100],\n","        [-77.2972, 100.5805]], grad_fn=<AddBackward0>)\n"]}],"source":["# Generamos predicciones\n","preds = model(inputs)\n","print(preds)"]},{"cell_type":"code","execution_count":493,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1673633088061,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"uCScDDI9rLzI","outputId":"cc02b6fa-ddfe-4f2e-a415-647bbe3a670a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(13005.9395, grad_fn=<DivBackward0>)\n"]}],"source":["# Calculamos el error/pérdida\n","loss = mse(preds, targets)\n","print(loss)"]},{"cell_type":"code","execution_count":494,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1673633094247,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"O5rA2H86rLzI","outputId":"6b47cf69-c802-474e-f730-236bdd5b802a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-12583.8555, -14317.8398,  -8704.3438],\n","        [  1892.1821,   1243.4912,    783.2975]])\n","tensor([-151.4751,   19.5223])\n"]}],"source":["# Calculamos los gradientes\n","loss.backward()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"code","execution_count":495,"metadata":{"id":"ekrB7z46rLzJ"},"outputs":[],"source":["# Ajustamos los pesos\n","with torch.no_grad():\n","    w -= w.grad * 1e-5\n","    b -= b.grad * 1e-5\n","    w.grad.zero_()\n","    b.grad.zero_()"]},{"cell_type":"markdown","metadata":{"id":"RlsdvI9brLzK"},"source":["Veamos los pesos y biases."]},{"cell_type":"code","execution_count":496,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173,"status":"ok","timestamp":1673633103373,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"KR2m6382rLzK","outputId":"5f8a41e1-6583-4ad8-e467-5e4f2c4e74a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.2487, -0.1947, -0.1870],\n","        [ 0.7509,  0.8567, -0.5287]], requires_grad=True)\n","tensor([0.1638, 0.4853], requires_grad=True)\n"]}],"source":["print(w)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"xNfv2tnnrLzK"},"source":["Con los nuevos pesos y biases, el modelo tiene un error menor."]},{"cell_type":"code","execution_count":497,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194,"status":"ok","timestamp":1673633108212,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"W_9o6GIrrLzL","outputId":"a694a773-60be-49a6-9151-510fb148be37"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(8954.9434, grad_fn=<DivBackward0>)\n"]}],"source":["# Calculamos la pérdida\n","preds = model(inputs)\n","loss = mse(preds, targets)\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"O2ZUarC0rLzL"},"source":["Podemos observar que tenemos una mejora significativa, ajustando los pesos y biases segun esta técnica."]},{"cell_type":"markdown","metadata":{"id":"1KWQ_1_rrLzL"},"source":["## Entrenar durante múltiples ciclos/épocas (epochs)\n","\n","Para reducir el error aún más, podemos repetir el proceso de ajustar pesos y biases usando los gradientes múltiples veces. Cada iteración se llama ciclo o época (_epoch_). Vamos a entrenar el modelo 100 epochs."]},{"cell_type":"code","execution_count":498,"metadata":{"id":"XEFGWWRBrLzM"},"outputs":[],"source":["for i in range(100):\n","    preds = model(inputs)\n","    loss = mse(preds, targets)\n","    loss.backward()\n","    with torch.no_grad():\n","        # w -= w.grad * 1e-5\n","        # b -= b.grad * 1e-5\n","        w.sub_(w.grad * 1e-5)\n","        b.sub_(b.grad * 1e-5)\n","        w.grad.zero_()\n","        b.grad.zero_()"]},{"cell_type":"code","execution_count":499,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1673633218058,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"WEhIdxZZrLzM","outputId":"af6377df-b382-45c5-d037-305e3d8e869e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(235.2014, grad_fn=<DivBackward0>)\n"]}],"source":["# Calculamos el nuevo error\n","preds = model(inputs)\n","loss = mse(preds, targets)\n","print(loss)"]},{"cell_type":"code","execution_count":500,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190,"status":"ok","timestamp":1673633220998,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"00dH8UTYmmqj","outputId":"9085b3b3-0889-481d-9799-5377de72f0d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(15.3363, grad_fn=<SqrtBackward0>)\n"]}],"source":["print(torch.sqrt(loss))"]},{"cell_type":"markdown","metadata":{"id":"4P8SPCQ2rLzN"},"source":["\n","El error ahora es bastante menor que en el momento inicial. Vamos a ver las nuevas predicciones y compremoslas con los valores target."]},{"cell_type":"code","execution_count":501,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1673633225492,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"LMg3ZeRbrLzN","outputId":"0c670c4c-98dd-4bef-937e-f617485047d2"},"outputs":[{"data":{"text/plain":["tensor([[ 61.1334,  75.1556],\n","        [ 82.1592,  93.7397],\n","        [112.4226, 140.9305],\n","        [ 43.7066,  63.6865],\n","        [ 88.6675,  91.6120]], grad_fn=<AddBackward0>)"]},"execution_count":501,"metadata":{},"output_type":"execute_result"}],"source":["# Prediciones\n","preds"]},{"cell_type":"code","execution_count":502,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1673633227180,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"6zULhmNirLzO","outputId":"d2707d8b-2097-42f8-d38c-a428a09f9f0e"},"outputs":[{"data":{"text/plain":["tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.]])"]},"execution_count":502,"metadata":{},"output_type":"execute_result"}],"source":["# Targets\n","targets"]},{"cell_type":"markdown","metadata":{"id":"Q4C2tOm4rLzO"},"source":["Observamos que las predicciones son ahora bastante cercanas a los objetivos. Podemos mejorar estos resultados entrenando más ciclos."]},{"cell_type":"markdown","metadata":{"id":"4nSexto8rLzR"},"source":["# Regresión lineal empleando las funciones propias de Pytorch\n","\n","Hasta ahora hemos implementado la regresión lineal y el descenso de gradiente empleando operaciones básicas sobre tensores. Sin embargo, dado que estas operaciones son un patrón común en deep learning, Pytorch provee una serie de **funciones y clases propias** específicamente preparadas para facilitar la creación y entrenamiento con tan solo unas cuantas líneas de código.\n","\n","Empezaremos importando el paqete `torch.nn` de PyTorch, el cual contiene las clases de utilidad para construir redes neuronales (_neural networks_)."]},{"cell_type":"code","execution_count":503,"metadata":{"id":"qqm585tErLzR"},"outputs":[],"source":["import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"HZgYP8ZsrLzR"},"source":["Igual que antes, vamos a representar las entradas y salidas como matrices."]},{"cell_type":"code","execution_count":504,"metadata":{"id":"9Agf0fskrLzR"},"outputs":[],"source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43],\n","                   [91, 88, 64],\n","                   [87, 134, 58],\n","                   [102, 43, 37],\n","                   [69, 96, 70],\n","                   [74, 66, 43],\n","                   [91, 87, 65],\n","                   [88, 134, 59],\n","                   [101, 44, 37],\n","                   [68, 96, 71],\n","                   [73, 66, 44],\n","                   [92, 87, 64],\n","                   [87, 135, 57],\n","                   [103, 43, 36],\n","                   [68, 97, 70]],\n","                  dtype='float32')\n","\n","# Targets (apples, oranges)\n","targets = np.array([[56, 70],\n","                    [81, 101],\n","                    [119, 133],\n","                    [22, 37],\n","                    [103, 119],\n","                    [57, 69],\n","                    [80, 102],\n","                    [118, 132],\n","                    [21, 38],\n","                    [104, 118],\n","                    [57, 69],\n","                    [82, 100],\n","                    [118, 134],\n","                    [20, 38],\n","                    [102, 120]],\n","                   dtype='float32')\n","\n","inputs = torch.from_numpy(inputs)\n","targets = torch.from_numpy(targets)"]},{"cell_type":"code","execution_count":505,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1673633314292,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"7YSJX-qQrLzS","outputId":"fd8306eb-1032-4045-e271-a187ba58ed2b"},"outputs":[{"data":{"text/plain":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.],\n","        [ 74.,  66.,  43.],\n","        [ 91.,  87.,  65.],\n","        [ 88., 134.,  59.],\n","        [101.,  44.,  37.],\n","        [ 68.,  96.,  71.],\n","        [ 73.,  66.,  44.],\n","        [ 92.,  87.,  64.],\n","        [ 87., 135.,  57.],\n","        [103.,  43.,  36.],\n","        [ 68.,  97.,  70.]])"]},"execution_count":505,"metadata":{},"output_type":"execute_result"}],"source":["inputs"]},{"cell_type":"markdown","metadata":{"id":"XxL_xNkbrLzS"},"source":["Vamos a usar 15 observaciones de entrenamiento para ilustrar como trabajar con conjuntos más grandes en pequeños lotes (_batches_)."]},{"cell_type":"markdown","metadata":{"id":"14AZDfjFrLzT"},"source":["## Dataset y DataLoader\n","\n","Vamos a crear un `TensorDataset`, el cual va a permitir acceder a las filas de `inputs` y sus respectivos `targets` como tuplas, además de proveer APIs estandard para trabajar con muchos diferentes datasets en PyTorch."]},{"cell_type":"code","execution_count":506,"metadata":{"id":"v0Mxl6D6rLzT"},"outputs":[],"source":["from torch.utils.data import TensorDataset"]},{"cell_type":"code","execution_count":507,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1673633405426,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"BE4xRZFarLzT","outputId":"ad39ca73-795a-428a-8b9c-5dd6c57db44e"},"outputs":[{"data":{"text/plain":["(tensor([[ 73.,  67.,  43.],\n","         [ 91.,  88.,  64.],\n","         [ 87., 134.,  58.]]),\n"," tensor([[ 56.,  70.],\n","         [ 81., 101.],\n","         [119., 133.]]))"]},"execution_count":507,"metadata":{},"output_type":"execute_result"}],"source":["# Define dataset\n","train_ds = TensorDataset(inputs, targets)\n","train_ds[0:3]"]},{"cell_type":"markdown","metadata":{"id":"UGTUR0WKrLzU"},"source":["`TensorDataset` nos permite acceder a una pequeña sección de los datos de entrenamiento usando la notación de índices de array (`[0:3]` en el código anterior). Devuelve una tupla con dos elementos. El primer elemento contiene las variables de entrada de las filas seleccionadas y el segundo contiene los objetivos (`target`)."]},{"cell_type":"markdown","metadata":{"id":"7myFSQiIrLzU"},"source":["Vamos a crear también un  `DataLoader`, el cual irá dividiendo los datos en lotes (_batches_) de un tamaño predefinido mientras hace el entrenamiento. tambien aporta otras utilidades como el barajeo (_shuffling_) y el muestreo aleatorio de los datos."]},{"cell_type":"code","execution_count":508,"metadata":{"id":"RIahdUBxrLzU"},"outputs":[],"source":["from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":509,"metadata":{"id":"ZvIVfKUirLzU"},"outputs":[],"source":["# Definimos el data loader\n","batch_size = 5\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"5GSjfNphrLzV"},"source":["Ahora podemos usar el data loader en un bucle `for`. Veamos un ejemplo:"]},{"cell_type":"code","execution_count":510,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1673633594884,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"IZVM1j_CrLzV","outputId":"c8125702-88e9-4401-928e-46992cadb6fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 91.,  87.,  65.],\n","        [ 73.,  67.,  43.],\n","        [ 92.,  87.,  64.],\n","        [ 87., 135.,  57.],\n","        [ 74.,  66.,  43.]])\n","tensor([[ 80., 102.],\n","        [ 56.,  70.],\n","        [ 82., 100.],\n","        [118., 134.],\n","        [ 57.,  69.]])\n"]}],"source":["for xb, yb in train_dl:\n","    print(xb)\n","    print(yb)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"PNBYpzThrLzV"},"source":["En cada iteración, el data loader devuelve un lote de datos con el tamaño indicado. Si `shuffle` es `True`, \"barajará\" los datos antes de crear los lotes. Esto ayuda a alatorizar las entradas al algoritmo de optimización, lo cual redunda en una reducción del error más rápida."]},{"cell_type":"markdown","metadata":{"id":"plceojdqrLzW"},"source":["## nn.Linear\n","\n","En lugar de inicializar los pesos y biases manualmente, podemos definir el modelo usando la clase `nn.Linear` de PyTorch, la cual lo hace automáticamente.\n","\n","(Más sobre `nn.linear` [aquí](https://pytorch.org/docs/stable/nn.html#linear-layers))\n"]},{"cell_type":"code","execution_count":511,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1673633732200,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"KmOcrDJArLzW","outputId":"0c6ff603-1bf4-4b0d-85e6-fbc71aab231b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameter containing:\n","tensor([[ 0.4319, -0.1339,  0.3695],\n","        [ 0.2331, -0.2949,  0.5113]], requires_grad=True)\n","Parameter containing:\n","tensor([0.0759, 0.4411], requires_grad=True)\n"]}],"source":["# Define modelo\n","model = nn.Linear(3, 2)\n","print(model.weight)\n","print(model.bias)"]},{"cell_type":"markdown","metadata":{"id":"eFyWBxK9rLzW"},"source":["Los modelos en PyTorch tienen un método muy útil llamado `.parameters`, el cual retorna una lista conteniendo todas las matrices de pesos y biases presentes en ese modelo. Para nuestro modelo de regresion lineal, tenemos una matriz de pesos y otra de biases."]},{"cell_type":"code","execution_count":512,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1673633864343,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"PXgzbBafrLzX","outputId":"2d3675eb-100a-4f6c-ddad-5819e02257bd"},"outputs":[{"data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.4319, -0.1339,  0.3695],\n","         [ 0.2331, -0.2949,  0.5113]], requires_grad=True),\n"," Parameter containing:\n"," tensor([0.0759, 0.4411], requires_grad=True)]"]},"execution_count":512,"metadata":{},"output_type":"execute_result"}],"source":["# Parametros\n","list(model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"pgMML2MZrLzX"},"source":["Ahora podemos usar el modelo generado para realizar nuestras predicciones."]},{"cell_type":"code","execution_count":513,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1673633912275,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"nGHn-WjwrLzY","outputId":"8fa8783b-c5a0-467e-b711-9d04c90dd657"},"outputs":[{"data":{"text/plain":["tensor([[38.5196, 19.6838],\n","        [51.2409, 28.4236],\n","        [41.1346, 10.8594],\n","        [52.0424, 30.4520],\n","        [42.8846, 24.0049],\n","        [39.0855, 20.2117],\n","        [51.7444, 29.2298],\n","        [41.9361, 11.6038],\n","        [51.4766, 29.9240],\n","        [42.8222, 24.2831],\n","        [39.0231, 20.4899],\n","        [51.8068, 28.9516],\n","        [40.6312, 10.0533],\n","        [52.1048, 30.1737],\n","        [42.3187, 23.4769]], grad_fn=<AddmmBackward0>)"]},"execution_count":513,"metadata":{},"output_type":"execute_result"}],"source":["# Genera prediciones\n","preds = model(inputs)\n","preds"]},{"cell_type":"markdown","metadata":{"id":"AOIc9oSQrLzY"},"source":["## Funcion de error o pérdida\n","\n","En lugar de definir una función de error manualmente, podemos usar la función propia de Pytorch `mse_loss`.\n"]},{"cell_type":"code","execution_count":514,"metadata":{"id":"bSJcJY6orLzY"},"outputs":[],"source":["# Importa nn.functional\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"pWYGEHSarLzZ"},"source":["El paquete `nn.functional` contiene muchas otras y útiles funciones de cálculo de error entre otras utilidades.\n","\n","(Más sobre `nn.functional`[aquí](https://pytorch.org/docs/stable/nn.functional.html))"]},{"cell_type":"code","execution_count":515,"metadata":{"id":"ZljPEP0srLzZ"},"outputs":[],"source":["# Definimos la función de loss\n","loss_fn = F.mse_loss"]},{"cell_type":"markdown","metadata":{"id":"XneC34O9rLzZ"},"source":["Calculamos el error para las predicciones de nuestro modelo de la siguiente forma:"]},{"cell_type":"code","execution_count":516,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1673633989412,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"crqM8TDhrLzZ","outputId":"e885cf45-7137-4cb9-dacb-6fddeb35fabb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(4336.8589, grad_fn=<MseLossBackward0>)\n"]}],"source":["loss = loss_fn(model(inputs), targets)\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"29J0-Tn6rLza"},"source":["## Optimización\n","\n","En lugar de manualmente manipular los pesos y biases del modelo a través de los gradientes, podemos usar el optimizador propio `optim.SGD`. SGD es la abreviatura de \"stochastic gradient descent\". El término estocástico indica que las muestras son seleccionadas en lotes aleatorios en lugar de individualmente."]},{"cell_type":"code","execution_count":517,"metadata":{"id":"EgOLXduIrLza"},"outputs":[],"source":["# Define optimizador\n","opt = torch.optim.SGD(model.parameters(), lr=1e-5)"]},{"cell_type":"markdown","metadata":{"id":"wAnliLuxrLza"},"source":["Obsérvese que los parámetros del modelo son pasados como argumento a `optim.SGD` de forma que el optimizador sepa qué matrices de pesos y biases osn las que tiene que ir modificando durante el proceso de actualización. También podemos especificar el ratio de aprendizaje \"lr\" que controla la cantidad con que los parametros son modificados."]},{"cell_type":"markdown","metadata":{"id":"DAYgy6btrLzb"},"source":["## Entrenamiento del modelo\n","\n","Vamos ahora a realizar el entrenamiento del modelo. Seguiremos el mismo proceso que ya hemos visto para implementar el descenso de gradiente:\n","\n","1. Generamos predicciones\n","\n","2. Calculamos el error/pérdida\n","\n","3. Calculamos los gradientes c.r.a. los pesos y biases\n","\n","4. Ajustamos los pesos restando una pequeña cantidad proporcianal a los gradientes obtenidos\n","\n","5. Reseteamos los gradientes a cero para repetir la operación\n","\n","El único cambio es que ahora vamos a trabajar con lotes de datos en lugar de emplear el dataset de entrenamiento completo en cada iteracion.\n","\n","Vamos a definir una función `fit` que realizará el entrenamiento de esa forma para un número dado de ciclos (_epochs_)."]},{"cell_type":"code","execution_count":518,"metadata":{"id":"y5rMu6BPrLzb"},"outputs":[],"source":["def fit(num_epochs, model, loss_fn, opt, train_dl):\n","\n","    # Repetir para el número especificado de epochs\n","    for epoch in range(num_epochs):\n","        # Entrena por lotes de datos\n","        for xb,yb in train_dl:\n","            # 1. Generamos prediccione\n","            pred = model(xb)\n","            # 2. Calculamos el error/pérdida\n","            loss = loss_fn(pred, yb)\n","            # 3. Calculamos los gradientes\n","            loss.backward()\n","            # 4. Actualizamos los parámetros\n","            opt.step()\n","            # 5. Reseteamos los gradientes a cero\n","            opt.zero_grad()\n","        # Imprimimos el progreso\n","        if (epoch+1) % 10 == 0:\n","            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss))"]},{"cell_type":"markdown","metadata":{"id":"uKYTBgY1rLzb"},"source":["Algunso detalles sobre la función definida:\n","\n","* Usamos el data loader definido previamente para obtener lotes de datos para cada iteración.\n","\n","* En lugar de actualizar los parametros (pesos y biases) manualmente, usamos `opt.step` para realiarlo y `opt.zero_grad` para resetear a cer los gradientes.\n","\n","* Se ha añadido unas líneas de log que imprimen el error del último lote entrenado cada 10 iteraciones, de forma que podemos seguir en progreso de entrenamiento. `loss.item` retorna el valor actual almacenado en el tensor de error _loss_.\n","\n","Vamos a entrenar 100 veces:"]},{"cell_type":"code","execution_count":519,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1673634580825,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"8r-JEYySrLzc","outputId":"be66134e-450a-444f-c0f1-5333122364e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [10/300], Loss: 1077.6354\n","Epoch [20/300], Loss: 145.7458\n","Epoch [30/300], Loss: 134.2375\n","Epoch [40/300], Loss: 38.8716\n","Epoch [50/300], Loss: 127.1328\n","Epoch [60/300], Loss: 84.8114\n","Epoch [70/300], Loss: 25.2052\n","Epoch [80/300], Loss: 46.4073\n","Epoch [90/300], Loss: 16.6616\n","Epoch [100/300], Loss: 31.3531\n","Epoch [110/300], Loss: 16.3092\n","Epoch [120/300], Loss: 11.3158\n","Epoch [130/300], Loss: 6.1468\n","Epoch [140/300], Loss: 4.6520\n","Epoch [150/300], Loss: 2.7056\n","Epoch [160/300], Loss: 3.0739\n","Epoch [170/300], Loss: 2.7682\n","Epoch [180/300], Loss: 3.1925\n","Epoch [190/300], Loss: 1.9596\n","Epoch [200/300], Loss: 1.4957\n","Epoch [210/300], Loss: 2.0867\n","Epoch [220/300], Loss: 0.7162\n","Epoch [230/300], Loss: 1.6094\n","Epoch [240/300], Loss: 1.9583\n","Epoch [250/300], Loss: 1.3432\n","Epoch [260/300], Loss: 1.6719\n","Epoch [270/300], Loss: 1.2612\n","Epoch [280/300], Loss: 1.4335\n","Epoch [290/300], Loss: 1.3023\n","Epoch [300/300], Loss: 1.4030\n"]}],"source":["fit(300, model, loss_fn, opt, train_dl)"]},{"cell_type":"markdown","metadata":{"id":"2JExdgXXrLzc"},"source":["Generamos predicciones y observemos si se acercan a los objetivos"]},{"cell_type":"code","execution_count":520,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170,"status":"ok","timestamp":1673634360797,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"45dkmCS8rLzc","outputId":"e5264773-c0fc-4fdb-a267-56944b736596"},"outputs":[{"data":{"text/plain":["tensor([[ 57.0666,  70.4886],\n","        [ 81.7504, 100.3154],\n","        [118.8660, 133.0013],\n","        [ 21.2135,  37.9629],\n","        [101.1374, 118.0886],\n","        [ 55.8174,  69.4080],\n","        [ 81.5452, 100.3801],\n","        [119.1275, 133.5827],\n","        [ 22.4627,  39.0436],\n","        [102.1814, 119.2340],\n","        [ 56.8614,  70.5533],\n","        [ 80.5012,  99.2347],\n","        [119.0712, 132.9366],\n","        [ 20.1695,  36.8176],\n","        [102.3866, 119.1693]], grad_fn=<AddmmBackward0>)"]},"execution_count":520,"metadata":{},"output_type":"execute_result"}],"source":["\n","preds = model(inputs)\n","preds"]},{"cell_type":"code","execution_count":521,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1673634363100,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"2yKivQFPrLzd","outputId":"4ddfd369-ed9e-4e86-ebca-322956bdb504"},"outputs":[{"data":{"text/plain":["tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.],\n","        [ 57.,  69.],\n","        [ 80., 102.],\n","        [118., 132.],\n","        [ 21.,  38.],\n","        [104., 118.],\n","        [ 57.,  69.],\n","        [ 82., 100.],\n","        [118., 134.],\n","        [ 20.,  38.],\n","        [102., 120.]])"]},"execution_count":521,"metadata":{},"output_type":"execute_result"}],"source":["\n","targets"]},{"cell_type":"markdown","metadata":{"id":"nydotCibrLzd"},"source":["Ahora las predicciones son bastante cercanas a nuestros valores objetivos. Hemos entrenado un modelo razonablemente bueno que nos permite predecir la cosecha a partir de tres variables temperatura, lluvias y humedad en una región. Podemos emplear el modelo para realizar predicciones para otra región pasandole un lote con una sola fila como entrada:"]},{"cell_type":"code","execution_count":522,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1673634391554,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"3ONCHxZ3rLze","outputId":"785bcbc2-4cd2-4a13-ceee-2c7d1e11a01a"},"outputs":[{"data":{"text/plain":["tensor([[53.5050, 67.5934]], grad_fn=<AddmmBackward0>)"]},"execution_count":522,"metadata":{},"output_type":"execute_result"}],"source":["model(torch.tensor([[75, 63, 44.]]))"]},{"cell_type":"markdown","metadata":{"id":"7YO3RYSarLze"},"source":["El modelo predice una cosecha de aprox. 54 tons/hect. de manzanas, y 68 tons/hect. de naranjas."]},{"cell_type":"markdown","metadata":{"id":"XuqpMZKlXyeC"},"source":["# Que hemos visto\n","\n","- Desarrollo \"a mano\" de la regresión y su optimización con descenso de gradiente.\n","- Modulo nn.Linear: tiene definida la funcion lineal y la inicialización de parámetros.\n","- Activación de Autograd de los tensores.\n","- Funcionde otimización de pytorch.\n","- Dataset y Dataloader para preparar lotes de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5O48ulfjXyMs"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"n2QOf-HEltUP"},"source":["## Fin del Notebook"]},{"cell_type":"markdown","metadata":{"id":"BZDr1zYmBkRV"},"source":["Referencias y modelos empleados para el Notebook:\n","\n","*   Documentación de [Pytorch](https://pytorch.org/docs/stable/index.html)\n","*   [PyTorch Tutorial for Deep Learning Researchers](https://github.com/yunjey/pytorch-tutorial) by Yunjey Choi\n","*   [FastAI](https://www.fast.ai/) development notebooks by Jeremy Howard.\n","*   Documentación y cursos en [Pierian Data](https://www.pieriandata.com/)\n","*   Tutoriales y notebooks del curso \"Deep Learning with Pytorch: Zero to GANs\" de [Aakash N S](https://jovian.ai/aakashns)\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1SQN5h_jcS8-B_SDrrD7y0Cawd7bfnwwi","timestamp":1609695028653}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
