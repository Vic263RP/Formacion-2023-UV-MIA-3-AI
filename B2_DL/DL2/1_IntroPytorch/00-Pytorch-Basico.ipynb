{"cells":[{"cell_type":"markdown","metadata":{"id":"AVBv0eq8108q"},"source":["![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \n","\n","#**Máster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"VZZRnhKHlmxp"},"source":["#<strong><center>Tensores en Pytorch</center></strong>\n","\n","# Tensores y operaciones básicas\n","Esta sección cubre: \n","* Convertir arrays NumPy a tensores PyTorch \n","* Crear tensores desde cero\n","\n","## Realizamos la importación de modulos habitual"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3914,"status":"ok","timestamp":1673464450557,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"9lcxzjoFlmxr"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"_CJ-Bsrxlmxs"},"source":["Comprobamos version de PyTorch"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1673464450558,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"f5l1VMiwlmxt","outputId":"aa985faa-db37-49f6-c945-659e84eea973"},"outputs":[{"data":{"text/plain":["'2.2.2+cu121'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"bKisj_jKlmxw"},"source":["## Conversión de arrays NumPy a tensores PyTorch\n","Un <a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.Tensor</tt></strong></a> es una matriz multi-dimensional que contiene elementos de un mismo tipo de datos.<br>\n","Los cálculos entre tensores solo se pueden dar si los tensores son del mismo dtype.<br>\n","En pytorch, los tensores son usados también como sustitución de Numpy para usar la potencia y posibilidades de GPUs."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1641572440327,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"lLAllnXLlmxx","outputId":"2f6ad801-97d8-4e80-a816-884315e2a38d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 2 3 4 5]\n","int64\n","<class 'numpy.ndarray'>\n"]}],"source":["arr = np.array([1,2,3,4,5])\n","print(arr)\n","print(arr.dtype)\n","print(type(arr))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1641572465294,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"oz1OWfvElmxy","outputId":"e235b089-6ed8-43ce-8fda-4e3f62eab5d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4, 5])\n"]}],"source":["x = torch.from_numpy(arr)\n","# Equivalente a x = torch.as_tensor(arr)\n","\n","print(x)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1641572480746,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Sv-ceY6olmx0","outputId":"5c666bc3-088e-4d17-fdc3-b6ac8204bdef"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.int64\n"]}],"source":["# Imprimimos el tipo de datos que contiene el tensor\n","print(x.dtype)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1641572485959,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Z9q-4ggclmx1","outputId":"f38c8743-6f42-43ee-c52f-6b80f4937a23"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'>\n","torch.LongTensor\n"]}],"source":["# Imprimimos en tipo de objeto que es el tensor\n","print(type(x))\n","print(x.type()) # más especifico!"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1641572718813,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"HEuEvx7slmx2","outputId":"c8a53f95-9945-48df-e701-8fd70243ad9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.  1.  2.]\n"," [ 3.  4.  5.]\n"," [ 6.  7.  8.]\n"," [ 9. 10. 11.]]\n"]}],"source":["arr2 = np.arange(0.,12.).reshape(4,3)\n","print(arr2)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1641572720321,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"bqL1ZSgZlmx3","outputId":"9b0e49af-968b-4131-f454-3207b0ff4081"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.]], dtype=torch.float64)\n","torch.DoubleTensor\n"]}],"source":["x2 = torch.from_numpy(arr2)\n","print(x2)\n","print(x2.type())"]},{"cell_type":"markdown","metadata":{"id":"jKN8-qCRlmx4"},"source":["Aquí <tt>torch.DoubleTensor</tt> se refiere a datos \"64-bit floating point\"."]},{"cell_type":"markdown","metadata":{"id":"GTvOGNcMlmx4"},"source":["## Tipos de datos en Tensores\n","Puedes consultar aquí en detalle los <a href='https://pytorch.org/docs/stable/tensors.html'>tipos de datos en Tensores</a>\n","<table style=\"display: inline-block\">\n","<tr><th>TYPE</th><th>NAME</th><th>EQUIVALENT</th><th>TENSOR TYPE</th></tr>\n","<tr><td>32-bit integer (signed)</td><td>torch.int32</td><td>torch.int</td><td>IntTensor</td></tr>\n","<tr><td>64-bit integer (signed)</td><td>torch.int64</td><td>torch.long</td><td>LongTensor</td></tr>\n","<tr><td>16-bit integer (signed)</td><td>torch.int16</td><td>torch.short</td><td>ShortTensor</td></tr>\n","<tr><td>32-bit floating point</td><td>torch.float32</td><td>torch.float</td><td>FloatTensor</td></tr>\n","<tr><td>64-bit floating point</td><td>torch.float64</td><td>torch.double</td><td>DoubleTensor</td></tr>\n","<tr><td>16-bit floating point</td><td>torch.float16</td><td>torch.half</td><td>HalfTensor</td></tr>\n","<tr><td>8-bit integer (signed)</td><td>torch.int8</td><td></td><td>CharTensor</td></tr>\n","<tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td></td><td>ByteTensor</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"rNdPMJrYlmx5"},"source":["## Copiar vs. compartir"]},{"cell_type":"markdown","metadata":{"id":"TDwABI40lmx6"},"source":["<a href='https://pytorch.org/docs/stable/torch.html#torch.from_numpy'><strong><tt>torch.from_numpy()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.as_tensor'><strong><tt>torch.as_tensor()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.tensor'><strong><tt>torch.tensor()</tt></strong></a><br>\n","\n","Hay diferentes funciones disponibles para <a href='https://pytorch.org/docs/stable/torch.html#creation-ops'>crear tensores</a>. Cuando usamos <a href='https://pytorch.org/docs/stable/torch.html#torch.from_numpy'><strong><tt>torch.from_numpy()</tt></strong></a> y <a href='https://pytorch.org/docs/stable/torch.html#torch.as_tensor'><strong><tt>torch.as_tensor()</tt></strong></a>, el tensor PyTorch y el array origen NumPy comparten la misma memoria. Esto significa que el cambio en uno afecta al otro. Sin embargo, la función <a href='https://pytorch.org/docs/stable/torch.html#torch.tensor'><strong><tt>torch.tensor()</tt></strong></a> siempre hace una copia nueva del tensor. "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1641572770889,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"hBsc-04elmx6","outputId":"faf004bb-d5d6-4313-e4fb-77512188582e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4])\n"]}],"source":["# torch.from_numpy()\n","arr = np.arange(0,5)\n","t = torch.from_numpy(arr)\n","print(t)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1641572780799,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"k2jedzPSlmx7","outputId":"f3a62989-1382-4761-9bb6-517dd2d51bea"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0,  1, 77,  3,  4])\n"]}],"source":["arr[2]=77\n","print(t)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1641572814478,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"V-kMxUIelmx8","outputId":"87b85688-20ff-420b-8ce1-d21de5c2cb17"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4])\n"]}],"source":["# torch.tensor()\n","arr = np.arange(0,5)\n","t = torch.tensor(arr)\n","print(t)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1641572830618,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"gHQv4cSFlmx9","outputId":"b9bc2b5b-a9a3-498d-b937-8076eb7a3a4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4])\n"]}],"source":["arr[2]=77\n","print(t)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1641572834867,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"KsIjF8Yylmx-","outputId":"f8276140-733e-4582-ca30-5d223b6fb293"},"outputs":[{"data":{"text/plain":["array([ 0,  1, 77,  3,  4])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["arr"]},{"cell_type":"markdown","metadata":{"id":"OdlBo5y8lmx-"},"source":["## Clases constructoras\n","<a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.Tensor()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.FloatTensor()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.LongTensor()</tt></strong></a>, etc.<br>\n","\n","Hay una diferencia entre emplear la función predeterminada <font color=black><tt>torch.tensor(data)</tt></font> y la clase constructora <font color=black><tt>torch.Tensor(data)</tt></font>.<br>\n","La función predeterminada adjudica el tipo de datos de los datos suministrados o del argumento \"dtype\" que se le pase. <br>\n","La clase constructora <tt>torch.Tensor()</tt> es simplemente un alias para <tt>torch.FloatTensor(data)</tt>. Consideremos lo siguiente:"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"yCpyPohZlmx_"},"outputs":[],"source":["data = np.array([1,2,3])"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1641572975816,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"5I2-20qAlmx_","outputId":"8f5bf5d4-b770-41b3-cf92-cb33259f8eb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1., 2., 3.]) torch.FloatTensor\n"]}],"source":["a = torch.Tensor(data)  # Equivalente a cc = torch.FloatTensor(data)\n","print(a, a.type())"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1641573046058,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Ntt_2H8almyA","outputId":"445ac5b7-e8a1-4472-ba50-16f7acb52f19"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3]) torch.LongTensor\n"]}],"source":["b = torch.tensor(data)\n","print(b, b.type())"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1641573047304,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"hC_-JDn6lmyB","outputId":"4c9bb560-5227-49d7-fc8f-60ebc0e80aaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3]) torch.LongTensor\n"]}],"source":["c = torch.LongTensor(data)\n","print(c, c.type())\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1641573048213,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"RNLmqWw8lmyC","outputId":"dee5f529-7608-4289-cee0-274dbb1d95b2"},"outputs":[{"data":{"text/plain":["torch.float32"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["a.dtype"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1641573049172,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"maM0xiUmsMc1","outputId":"1fe4d46c-8163-40da-e4d1-ba9753b99792"},"outputs":[{"data":{"text/plain":["torch.int64"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["b.dtype"]},{"cell_type":"markdown","metadata":{"id":"uW-pM_L0lmyE"},"source":["## Creando tensores desde cero\n","### Inicializando tensores con <tt>.empty()</tt>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.empty'>\n","# <strong><tt>torch.empty()</tt></strong></a> devuelve un tensor <em>no inicializado</em>. Esencialmente un bloque de memoria es reservado de acerdo al tamaño del tensor, y ningún valor ya dispuesto en dicho bloque es retornado. Es similar al comportamiento de <tt>numpy.empty()</tt>."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1673464463030,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"8sbJnyT4lmyE","outputId":"0d52961e-2e0c-4367-d174-a08cfb46f0e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 8.7522e-36,  0.0000e+00,  4.8766e-36],\n","        [ 0.0000e+00, -1.1689e+07,  4.5713e-41],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"]}],"source":["x = torch.empty(4, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"DpuN0YUalmyF"},"source":["### Inicialización de tensores con <tt>.zeros()</tt> and <tt>.ones()</tt>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.zeros'><strong><tt>torch.zeros(size)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.ones'><strong><tt>torch.ones(size)</tt></strong></a><br>\n","Es aconsejable pasar como argumento el dtype que queremos. "]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1673464465698,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"pfOEQgcxlmyF","outputId":"781c2037-ecd7-4bc1-c520-0b16d00eda71"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]])\n"]}],"source":["x = torch.zeros(4, 3, dtype=torch.int64)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"uoKASDHSlmyG"},"source":["### Tensores desde rangos\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.arange'><strong><tt>torch.arange(start,end,step)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.linspace'><strong><tt>torch.linspace(start,end,steps)</tt></strong></a><br>\n","\n","Nota: con <tt>.arange()</tt>, <tt>end</tt> es exclusivo, mientras que con <tt>linspace()</tt>, <tt>end</tt> es inclusivo."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1673464473592,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"NbZllptQlmyH","outputId":"1bab66b9-58ed-4311-ee82-f6ff7001ce60"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0,  2,  4],\n","        [ 6,  8, 10],\n","        [12, 14, 16]])\n"]}],"source":["x = torch.arange(0,18,2).reshape(3,3)\n","print(x)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1641573193210,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"1f9iYa6AlmyH","outputId":"e11bdcee-a529-4e36-965b-35fe569a6304"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.0000,  1.6364,  3.2727,  4.9091],\n","        [ 6.5455,  8.1818,  9.8182, 11.4545],\n","        [13.0909, 14.7273, 16.3636, 18.0000]])\n"]}],"source":["x = torch.linspace(0,18,12).reshape(3,4)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"gD6yLbIKQ76e"},"source":["Nota2: ¿Que pasa si las dimensiones no cuadran?"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1673464599669,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"SuT9aG2AQDcc"},"outputs":[],"source":["x = torch.linspace(0,18,12).reshape(3,4)"]},{"cell_type":"markdown","metadata":{"id":"vaifWyUplmyI"},"source":["### Tensores desde datos\n","<tt>torch.tensor()</tt> adjudica el tipo de dato (dtype) basado en los datos suministrados:"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1641573278958,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"6h2i7SsilmyJ","outputId":"05c9dd24-c5a0-4dcb-fcb4-cdfcb5cf9e2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4])\n","torch.int64\n","torch.LongTensor\n"]}],"source":["x = torch.tensor([1, 2, 3, 4])\n","print(x)\n","print(x.dtype)\n","print(x.type())"]},{"cell_type":"markdown","metadata":{"id":"1U0eMQPRlmyJ"},"source":["Alternativamente puedes establecer el tipo de dato segun el método de tensor empleado. Para listado de los tipos de tensores ver https://pytorch.org/docs/stable/tensors.html"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1641573291123,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"LY21d8B8lmyK","outputId":"ce09ddfa-c6ad-4e1f-f089-46d779aafb7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 6., 7.])\n","torch.float32\n","torch.FloatTensor\n"]}],"source":["x = torch.FloatTensor([5,6,7])\n","print(x)\n","print(x.dtype)\n","print(x.type())"]},{"cell_type":"markdown","metadata":{"id":"GyoTA243lmyL"},"source":["También puedes pasar el tipo de dato como un argumento. Para una lista de tipos de datos dtypes visitar https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype<br>"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1641573306265,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"wOqa7_V9lmyL","outputId":"ebb080d5-be29-4dd5-ccae-549afb1fa71f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 8,  9, -3], dtype=torch.int32)\n","torch.int32\n","torch.IntTensor\n"]}],"source":["x = torch.tensor([8,9,-3], dtype=torch.int)\n","print(x)\n","print(x.dtype)\n","print(x.type())"]},{"cell_type":"markdown","metadata":{"id":"vH2Fm0oKlmyM"},"source":["### Cambiando el dtype de tensores existentes\n","\n","Para cambiar el dtype no debes usar <tt>x = torch.tensor(x, dtype=torch.type)</tt> ya que dará un error de intento inapropiado de clonado de tensor.<br>\n","En su lugar se debe emplear el metodo <tt>.type()</tt>."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1641573316894,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"W1GoayN_lmyM","outputId":"a1b3c7fc-d97c-4458-afd2-6a9f912e5fb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Old: torch.IntTensor\n","New: torch.LongTensor\n"]}],"source":["print('Old:', x.type())\n","\n","x = x.type(torch.int64)\n","\n","print('New:', x.type())"]},{"cell_type":"markdown","metadata":{"id":"afFlesoolmyN"},"source":["### Tensores de números aleatorios\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.rand'><strong><tt>torch.rand(size)</tt></strong></a> devuelve muestras aleaorias con una distribución uniforme entre [0, 1]<br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randn'><strong><tt>torch.randn(size)</tt></strong></a> devuelve muestras con una distribución \"standard normal\" con [σ = 1]<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;A diferencia de <tt>rand</tt> que es uniforme, valores cercanos a cero son más probables a aparecer en este tipo de generación.<br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randint'><strong><tt>torch.randint(low,high,size)</tt></strong></a> devuelve enteros aleatorios desde low (incluido) hasta high (excluido)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1641469217661,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"iDqxqLsNlmyO","outputId":"9ffc83b4-3b5e-4e3c-e4dc-86e9a7721b81"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.6454, 0.4048, 0.1499],\n","        [0.0414, 0.1212, 0.6056],\n","        [0.2797, 0.1823, 0.2042],\n","        [0.5711, 0.0407, 0.2904]])\n"]}],"source":["x = torch.rand(4, 3)\n","print(x)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1641469219033,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"1VNc2vQylmyP","outputId":"c4e8ba75-4f30-40b4-d370-1f89b87c0ad6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.9585,  0.6328,  0.0367],\n","        [ 0.7564,  0.7016, -1.4272],\n","        [-0.5774, -0.3773, -1.3684],\n","        [ 0.5437, -0.5501,  0.4665]])\n"]}],"source":["x = torch.randn(4, 3)\n","print(x)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1641573424141,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"pfYy0uJplmyQ","outputId":"5c8ed12d-e5a9-4c3b-c829-bb1173bce774"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[3, 0, 3],\n","        [1, 2, 3],\n","        [0, 4, 4],\n","        [3, 2, 1]])\n"]}],"source":["x = torch.randint(0, 5, (4, 3))\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"t2_Ck2TClmyR"},"source":["### Tensores de números aleatorios que toman un tamaño de entrada (input size)\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.rand_like'><strong><tt>torch.rand_like(input)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randn_like'><strong><tt>torch.randn_like(input)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randint_like'><strong><tt>torch.randint_like(input,low,high)</tt></strong></a><br> Estos métodos retornan tensores con números aleatorios con las mismas dimensiones que <tt>input</tt>"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1641573446515,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"xWpiBHhhlmyR","outputId":"fb1c2c7d-b01e-45aa-d929-c058bd4a82d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.]])\n"]}],"source":["x = torch.zeros(2,5)\n","print(x)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1641469240079,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Q125f1nplmyS","outputId":"947c1529-2aec-4842-e8b1-02452a4a9440"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.4551,  0.0221, -0.1409, -0.1821,  0.1592],\n","        [-0.3996,  0.1088,  0.0290, -0.9534,  0.8647]])\n"]}],"source":["x2 = torch.randn_like(x)\n","print(x2)"]},{"cell_type":"markdown","metadata":{"id":"nuwZ6BqPlmyT"},"source":["La misma sintáxis puede ser empleada con<br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.zeros_like'><strong><tt>torch.zeros_like(input)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.ones_like'><strong><tt>torch.ones_like(input)</tt></strong></a>"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1641469244340,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"6XP_qivLlmyU","outputId":"52ea224e-c078-48c8-dfc6-44494f7fa730"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])\n"]}],"source":["x3 = torch.ones_like(x2)\n","print(x3)"]},{"cell_type":"markdown","metadata":{"id":"okgqGPiTlmyU"},"source":["### Estableciendo una semilla (seed) aleatoria\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.manual_seed'><strong><tt>torch.manual_seed(int)</tt></strong></a> es empleado para obtener resultados reproducibles"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1641573581410,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"n3yP1eiKlmyV","outputId":"9f507be7-ac8d-46e6-b6f5-f64255072844"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009]])\n"]}],"source":["torch.manual_seed(42)\n","x = torch.rand(2, 3)\n","print(x)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1641573585547,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"KpyaOXurlmyW","outputId":"abde080d-de72-4c5e-8fdd-b6c975bd3823"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.5596, 0.5591, 0.0915],\n","        [0.2100, 0.0072, 0.0390]])\n"]}],"source":["torch.manual_seed(4)\n","x = torch.rand(2, 3)\n","print(x)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1641573602536,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"1if2_GfdlmyX","outputId":"6b713a2f-9c11-4c46-b0e6-df63ec3c8d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.9929, 0.9131, 0.6186],\n","        [0.9744, 0.3189, 0.2148]])\n"]}],"source":["x = torch.rand(2, 3)\n","print(x)\n"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1641573620051,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"scEZVvw8lmyX","outputId":"ac95253a-a047-4a83-d7e9-47757e47ec91"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009]])\n"]}],"source":["torch.manual_seed(42)\n","x = torch.rand(2, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"yUHjaEv2lmyZ"},"source":["## Atributos de los tensores\n","Además <tt>dtype</tt>, podemos obtener otros <a href='https://pytorch.org/docs/stable/tensor_attributes.html'>atributos de tensores</a> como <tt>shape</tt>, <tt>device</tt> and <tt>layout</tt>"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1641573643408,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"xTkUoFvqlmya","outputId":"b426ae9a-585d-4513-a4f7-94b4286c073c"},"outputs":[{"data":{"text/plain":["torch.Size([2, 3])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1641573645561,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"UmLtn144lmya","outputId":"6473047a-4690-4789-bbcd-cf63170160ab"},"outputs":[{"data":{"text/plain":["torch.Size([2, 3])"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["x.size()  # equivalente a x.shape"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1641573661282,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"exi9Abq2lmyc","outputId":"01d6aa9b-2476-4b7c-a485-994578f11f45"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["x.device"]},{"cell_type":"markdown","metadata":{"id":"aNmte7Islmyc"},"source":["PyTorch soporta el uso de múltiples <a href='https://pytorch.org/docs/stable/tensor_attributes.html#torch-device'>devices</a>, beneficiandose de la potencia de una o más GPUs en lugar de una CPU.<br>\n","No vamos a explorar eso en este notebook, pero es importante saber que esas operaciones entre tensores solo pueden tener lugar si los tensores estan alojados en la memoria del mismo dispositivo (device)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1641573687920,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"T6oGAXmClmyc","outputId":"4589251b-efc0-4e3f-eca7-b4a298f49640","scrolled":true},"outputs":[{"data":{"text/plain":["torch.strided"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["x.layout"]},{"cell_type":"markdown","metadata":{"id":"AtFAVgUAlmyd"},"source":["PyTorch tiene una clase para contener las opciones de disposicion de memoria o  <a href='https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.layout'>memory layout</a>. La configuración por defecto es <a href='https://en.wikipedia.org/wiki/Stride_of_an_array'>strided</a> y en principio es la que vamos a emplear en estas sesiones. "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"pWx_dGzIltTx"},"source":["# Operaciones con tensores \n","* Indexado y subselección\n","* Redimensionado de tensores ( vistas de tensores)\n","* Aritmética y operaciones matemáticas con tensores\n","* Productos\n","* Multiplicacion de matrices \n","* Y más operaciones...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xEQxZ4qLltTz"},"source":["## Indexado y subselección (slicing)\n","Extraer valores específicos de un tensor funciona igual que con los arrays de Numpy<br>\n","\n","\n","![Indexing](https://imgur.com/DDsVVeE.png)\n","\n","Fuente de imagen: http://www.scipy-lectures.org/_images/numpy_indexing.png"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1641573868236,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Woo2I20AltTz","outputId":"6c04bde0-833e-4d13-f52d-531d6e60c987","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n"]}],"source":["x = torch.arange(6).reshape(3,2)\n","print(x)"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1641573870981,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Azd6gdZnltT0","outputId":"d824ee4d-1c36-4f20-ff30-da7eafe8ee40"},"outputs":[{"data":{"text/plain":["tensor([1, 3, 5])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# Accediendo a la columna de la derecha\n","x[:,1]"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1641469397730,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"C7HckuO0ltT1","outputId":"611d5b28-7df8-4109-aad6-fcdeb727d894"},"outputs":[{"data":{"text/plain":["tensor([[1],\n","        [3],\n","        [5]])"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# Accediendo a la columna de la derecha, como una \"rebanada\" (3,1), i.e. manteniendo el formato original del array.\n","x[:,1:]"]},{"cell_type":"markdown","metadata":{"id":"mccQ3x4AltT2"},"source":["## Redimensionando tensores con <tt>.view()</tt>\n","<a href='https://pytorch.org/docs/master/tensors.html#torch.Tensor.view'><strong><tt>view()</tt></strong></a> y <a href='https://pytorch.org/docs/master/torch.html#torch.reshape'><strong><tt>reshape()</tt></strong></a> hacen esencialmente lo mismo, retornan un tensor redimensionado sin cambiar el tensor original en cuestion. <br>\n","Las diferencias se pueden revisar con más detalle <a href='https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch'>aquí</a>."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1641573957712,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"OcOq8EfoltT2","outputId":"84bc25bc-6b1c-42bf-ff44-5d43a1457af7"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}],"source":["x = torch.arange(10)\n","print(x)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1641573959491,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"WxHH-eEiltT3","outputId":"8164fe3e-65b5-49ed-e188-9e7470cc04f6"},"outputs":[{"data":{"text/plain":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["x.view(2,5)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1641573961728,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"WTkUgUwLltT4","outputId":"663f9e69-8008-4484-b229-6dcf8f9c1152"},"outputs":[{"data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["x.view(5,2)"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1641573964076,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"-PXzNtIlltT4","outputId":"54c0802f-02c5-4780-c2cf-ead322ea7448"},"outputs":[{"data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["# x no ha cambiado su forma\n","x"]},{"cell_type":"markdown","metadata":{"id":"LIwfhJw_ltT5"},"source":["### View refleja los datos actualizados"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1641573972780,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"_Il7069UltT5","outputId":"05c5bf5e-0082-4bce-a98f-84f0a2c38c33"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[234,   1,   2,   3,   4],\n","        [  5,   6,   7,   8,   9]])\n"]}],"source":["z = x.view(2,5)\n","x[0]=234\n","print(z)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1641573986657,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"ecCw8R6wxTrW","outputId":"94c97d36-7b7c-4899-e155-b6144dfdad84"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[12,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9]])\n"]}],"source":["z[0,0] = 12\n","print(z)"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1641573997653,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"Akhc99YCxvcJ","outputId":"ae6f55a8-13fc-4a1f-b56b-f8d5cd332adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([12,  1,  2,  3,  4,  5,  6,  7,  8,  9])\n"]}],"source":["print(x) # Los cambios en z afectan a x (es solo una \"vista\" de x)"]},{"cell_type":"markdown","metadata":{"id":"P-F6rBQ7ltT6"},"source":["### Las vistas pueden inferir el tamaño \n","Pasando el valor <tt>-1</tt> PyTorch inferirá el valor correcto para el tensor dado<br> \n","(Inferir es obtener el número de la dimensión automáticamente)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1641574068142,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"4Ex7VtVJltT6","outputId":"7c1f1148-0e9c-4829-b3b0-105a063e0aa9"},"outputs":[{"data":{"text/plain":["tensor([[12,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9]])"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["x.view(2,-1)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1641574068741,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"8rPT-879ltT7","outputId":"5734346a-f115-4193-8e5e-3a746592f4ce"},"outputs":[{"data":{"text/plain":["tensor([[12,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9]])"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["x.view(-1,5)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([12,  1,  2,  3,  4,  5,  6,  7,  8,  9])"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["# Este el que fa es aplanar l'array (convertir-lo en un vector unidimensional)\n","x.view(-1)"]},{"cell_type":"markdown","metadata":{"id":"vfgLliVWltT7"},"source":["### Adoptar la forma de otro tensor con <tt>.view_as()</tt>\n","<a href='https://pytorch.org/docs/master/tensors.html#torch.Tensor.view_as'><strong><tt>view_as(input)</tt></strong></a> solo funcionará con tensores que tienen el mismo número de elementos."]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1641574070875,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"4tU8Qp8TltT8","outputId":"ad4e87f8-d071-4d5e-c5c1-6e5e956dcd7e"},"outputs":[{"data":{"text/plain":["tensor([[12,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9]])"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["x.view_as(z)"]},{"cell_type":"markdown","metadata":{"id":"ZuTIHhDyltT8"},"source":["## Aritmética de Tensores\n","La suma de tensores se puede realizar de diferentes formas según el resultado que queramos.<br>\n","Como una simple expresión:"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1641574086494,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"fgz3qsyDltT9","outputId":"88ddb158-bfec-4235-87e5-449e8360d461"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["a = torch.tensor([1.,2.,3.])\n","b = torch.tensor([4.,5.,6.])\n","print(a + b)"]},{"cell_type":"markdown","metadata":{"id":"PePC58TSltT9"},"source":["Como argumentos que pasamos en una función de torch:"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1641574097641,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"EjERhGNVltT-","outputId":"0c32b6ea-f41e-44bc-9237-3234a926fdd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["print(torch.add(a, b))"]},{"cell_type":"markdown","metadata":{"id":"XVp5Qo4vltT-"},"source":["A un tensor de salida que indicamos como argumento:"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1641574107056,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"rNzoSb97ltT-","outputId":"88c81f83-b7df-4534-fdda-f9d6ae915155"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["result = torch.empty(3)\n","torch.add(a, b, out=result)  # equivale a result=torch.add(a,b)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"kIxu1ljcltT_"},"source":["Operando in situ sobre el tensor"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1641574143574,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"OEzHF40IltT_","outputId":"72371aea-c767-40a1-9ef7-ad68fd7106af"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["a.add_(b)  # equivale a a=torch.add(a,b)\n","print(a)"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1641574150312,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"vhPcrE0qcaGq","outputId":"163bcce2-0c7a-40c2-87ba-98b8b0183a31"},"outputs":[{"data":{"text/plain":["tensor([5., 7., 9.])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["a"]},{"cell_type":"markdown","metadata":{"id":"oKpDI4K0ltUA"},"source":["<div class=\"alert alert-info\"><strong>NOTA:</strong> Cualquier operación que modifica a un tensor in situ debe incluir como sufijo un guión bajo _.\n","    <br>En el ejemplo: <tt>a.add_(b)</tt> modifica <tt>a</tt>.</div>"]},{"cell_type":"markdown","metadata":{"id":"uCLQCvBWltUA"},"source":["### Operaciones Básicas para Tensores\n","<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Arithmetic</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>a + b</td><td>a.add(b)</td><td>element wise addition</td></tr>\n","<tr><td>a - b</td><td>a.sub(b)</td><td>subtraction</td></tr>\n","<tr><td>a * b</td><td>a.mul(b)</td><td>multiplication</td></tr>\n","<tr><td>a / b</td><td>a.div(b)</td><td>division</td></tr>\n","<tr><td>a % b</td><td>a.fmod(b)</td><td>modulo (remainder after division)</td></tr>\n","<tr><td>a<sup>b</sup></td><td>a.pow(b)</td><td>power</td></tr>\n","<tr><td>&nbsp;</td><td></td><td></td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Jl29DawBltUB"},"source":["<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Monomial Operations</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>|a|</td><td>torch.abs(a)</td><td>absolute value</td></tr>\n","<tr><td>1/a</td><td>torch.reciprocal(a)</td><td>reciprocal</td></tr>\n","<tr><td>$\\sqrt{a}$</td><td>torch.sqrt(a)</td><td>square root</td></tr>\n","<tr><td>log(a)</td><td>torch.log(a)</td><td>natural log</td></tr>\n","<tr><td>e<sup>a</sup></td><td>torch.exp(a)</td><td>exponential</td></tr>\n","<tr><td>12.34  ==>  12.</td><td>torch.trunc(a)</td><td>truncated integer</td></tr>\n","<tr><td>12.34  ==>  0.34</td><td>torch.frac(a)</td><td>fractional component</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"uNAzlJXVltUB"},"source":["<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Trigonometry</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>sin(a)</td><td>torch.sin(a)</td><td>sine</td></tr>\n","<tr><td>cos(a)</td><td>torch.sin(a)</td><td>cosine</td></tr>\n","<tr><td>tan(a)</td><td>torch.sin(a)</td><td>tangent</td></tr>\n","<tr><td>arcsin(a)</td><td>torch.asin(a)</td><td>arc sine</td></tr>\n","<tr><td>arccos(a)</td><td>torch.acos(a)</td><td>arc cosine</td></tr>\n","<tr><td>arctan(a)</td><td>torch.atan(a)</td><td>arc tangent</td></tr>\n","<tr><td>sinh(a)</td><td>torch.sinh(a)</td><td>hyperbolic sine</td></tr>\n","<tr><td>cosh(a)</td><td>torch.cosh(a)</td><td>hyperbolic cosine</td></tr>\n","<tr><td>tanh(a)</td><td>torch.tanh(a)</td><td>hyperbolic tangent</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"vbKgb_R3ltUC"},"source":["<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Summary Statistics</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>$\\sum a$</td><td>torch.sum(a)</td><td>sum</td></tr>\n","<tr><td>$\\bar a$</td><td>torch.mean(a)</td><td>mean</td></tr>\n","<tr><td>a<sub>max</sub></td><td>torch.max(a)</td><td>maximum</td></tr>\n","<tr><td>a<sub>min</sub></td><td>torch.min(a)</td><td>minimum</td></tr>\n","<tr><td colspan=\"3\">torch.max(a,b) returns a tensor of size a<br>containing the element wise max between a and b</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Ynqzm67dltUD"},"source":["<div class=\"alert alert-info\"><strong>NOTA:</strong> Muchas operaciones aritméticas requieren valores flotantes. Aquellas que trabajan con enteros retornarán tensores de enteros.<br>\n","Por ejemplo, <tt>torch.div(a,b)</tt> realizará una dicisión con redondeo (truncando el decimal) si empleamos tipo entero, y una división clásica si empleamos floats.</div>"]},{"cell_type":"markdown","metadata":{"id":"S9bj4403ltUD"},"source":["#### Observemos diferentes formas de operaciones: "]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1641574205116,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"4fbCT2PpltUE","outputId":"bddc5a29-6283-4fff-bc74-efdd3665c196"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(21.)\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(torch.add(a,b).sum())"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1641574206709,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"ct4KsMgEltUF","outputId":"87d2e18a-35d3-4348-d3e3-828cca47ee9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(21.)\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(sum(a + b))"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1641574209575,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"oFbgMs4LltUF","outputId":"ff25cfee-0b27-44be-82cc-75f79dc4ce9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(21.)\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(sum(a.add_(b)))"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1610126189944,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"7LvVVgH6ltUG","outputId":"9e9790f6-2e58-4938-ccd4-9456994b0392"},"outputs":[{"data":{"text/plain":["tensor([5., 7., 9.])"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["a"]},{"cell_type":"markdown","metadata":{"id":"ToaRDphMltUG"},"source":["## Producto escalar (dot product)\n","El producto escalar <a href='https://en.wikipedia.org/wiki/Dot_product'>dot product</a> es la suma de los productos de los correspondientes elementos de dos tensores 1D. Si los tensores son dos vectores, el producto escalar será:<br>\n","\n","$\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf$\n","\n","Si los tensores incluyen a un tensor columna, entonces el producto escalar será igual a la suma de del resultado de las matrices multiplicadas. Por eljemplo:<br>\n","$\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf$<br><br>\n","El producto escalar (Dot products) puede ser expresado como <a href='https://pytorch.org/docs/stable/torch.html#torch.dot'><strong><tt>torch.dot(a,b)</tt></strong></a> o `a.dot(b)` o `b.dot(a)`"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1641574231992,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"ve9_OrXAltUH","outputId":"b67bd07e-a384-41d9-c115-88b9291db1ce","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 4., 10., 18.])\n","\n","tensor(32.)\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(a.mul(b)) # para referencia\n","print()\n","print(a.dot(b))"]},{"cell_type":"markdown","metadata":{"id":"GB1_DSw7ltUI"},"source":["<div class=\"alert alert-info\"><strong>NOTA:</strong> Hay una ligera diferencia entre <tt>torch.dot()</tt> y <tt>numpy.dot()</tt>. Mientras <tt>torch.dot()</tt> solo acepta argumentos de 1D y devuelve el producto escalar, <tt>numpy.dot()</tt> también acepta argumentos 2D y realiza una multiplicación de matrices.<br>Vemos la multiplicacin de matrices en Pytorch a continuación"]},{"cell_type":"markdown","metadata":{"id":"i721jrEqltUI"},"source":["## Multiplicación de matrices\n","La multiplicación de matrices 2D <a href='https://en.wikipedia.org/wiki/Matrix_multiplication'>(Matrix multiplication)</a> es posible cuando el número de columnas en el tensor <strong><tt>A</tt></strong> es igual el número de filas del tensor <strong><tt>B</tt></strong>. En este caso el producto del tensor  <strong><tt>A</tt></strong> de dimensiones $(x,y)$ y el tensor <strong><tt>B</tt></strong> con dimensiones $(y,z)$ resulta en un tensor de tamaño $(x,z)$\n","\n","![MatrixMultiplication](https://i.imgur.com/2xdyiul.jpg)\n","\n","\n","$\\begin{bmatrix} a & b & c \\\\\n","d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\\n","(dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$</div></div>\n","\n","<div style=\"clear:both\">Image source: <a href='https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg'>https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg</a></div>\n","\n","La multiplicación de matrices en Pytorch puede ser calculada empleando <a href='https://pytorch.org/docs/stable/torch.html#torch.mm'><strong><tt>torch.mm(a,b)</tt></strong></a> o `a.mm(b)` *o* `a @ b`"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1641574303333,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"poRZgY8jltUJ","outputId":"7a247d79-0ee7-476c-f9e6-2d0200defd57"},"outputs":[{"name":"stdout","output_type":"stream","text":["a:  torch.Size([2, 3])\n","b:  torch.Size([3, 2])\n","a x b:  torch.Size([2, 2])\n"]}],"source":["a = torch.tensor([[0,2,4],[1,3,5]], dtype=torch.float)\n","b = torch.tensor([[6,7],[8,9],[10,11]], dtype=torch.float)\n","\n","print('a: ',a.size())\n","print('b: ',b.size())\n","print('a x b: ',torch.mm(a,b).size())"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1641574348121,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"tWspQWHEltUJ","outputId":"e9f8ac24-b9d3-47a4-9eff-449a4f5278e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56., 62.],\n","        [80., 89.]])\n"]}],"source":["print(torch.mm(a,b))"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1641574351092,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"4igAkxOvltUJ","outputId":"7497e263-093d-4a9c-8083-2863b244a644"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56., 62.],\n","        [80., 89.]])\n"]}],"source":["print(a.mm(b))"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":250,"status":"ok","timestamp":1641574352705,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"OlS2_i4cltUK","outputId":"7a2c9794-170b-45bf-a5cc-e0e0b20e54eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56., 62.],\n","        [80., 89.]])\n"]}],"source":["print(a @ b)"]},{"cell_type":"markdown","metadata":{"id":"DJIJEJqCltUK"},"source":["### Multiplicación de matrices con broadcasting\n","Multiplicación de matrices que implica <a href='https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics'>broadcasting</a> se puede realizar empleando <a href='https://pytorch.org/docs/stable/torch.html#torch.matmul'><strong><tt>torch.matmul(a,b)</tt></strong></a> o `a.matmul(b)` o `a @ b`"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202,"status":"ok","timestamp":1673465076619,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"gXkhC4_NltUL","outputId":"abd7514c-fdf6-4dcb-8846-64c2247e5d7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 3, 5])\n"]}],"source":["t1 = torch.randn(2, 3, 4)\n","t2 = torch.randn(4, 5)\n","\n","print(torch.matmul(t1, t2).size())"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1673465087242,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"ATEccNAySrDZ","outputId":"e8237e36-d6ab-4bab-ba82-d38ca776cdd3"},"outputs":[{"data":{"text/plain":["tensor([[[-0.0431, -1.6047,  1.7878, -0.4780],\n","         [-0.2429, -0.9342, -0.2483, -1.2082],\n","         [-2.3169, -0.2168, -1.3847, -0.8712]],\n","\n","        [[-0.2234,  1.7174,  0.3189, -0.4245],\n","         [-0.8286,  0.3309, -1.5576,  0.9956],\n","         [-0.8798, -0.6011, -1.2742,  2.1228]]])"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["t1"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1673465114312,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"WE9im7dZSxo_","outputId":"9da73bf9-7790-45ad-9a80-50fcfe2ef0aa"},"outputs":[{"data":{"text/plain":["tensor([[-1.0892, -0.3553, -0.9138, -0.6581,  2.2181],\n","        [ 0.5232,  0.3466, -0.1973, -1.0546,  1.2780],\n","        [ 0.1453,  0.2311,  0.0566,  0.4263,  0.5750],\n","        [-0.6417, -2.2064, -0.7508,  2.8140,  0.3598]])"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["t2"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":184,"status":"ok","timestamp":1673465233083,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"gK9v9pALTFcw","outputId":"bad46ead-99a9-4203-ac70-436dfbd0a44d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[-0.2260,  0.9269,  0.8162,  1.1375, -1.2903],\n","         [ 0.5151,  2.3709,  1.2993, -2.3608, -2.3100],\n","         [ 2.7680,  2.3504,  2.7358, -1.2885, -6.5258]],\n","\n","        [[ 1.4605,  1.6850,  0.2020, -2.7228,  1.7300],\n","         [ 0.2103, -2.1476, -0.1439,  2.3341, -1.9523],\n","         [-0.9037, -4.8739, -0.7433,  6.6434, -2.6886]]])\n"]}],"source":["print(torch.matmul(t1,t2))"]},{"cell_type":"markdown","metadata":{"id":"6UsLLBJ-ltUL"},"source":["Sin embargo, la misma operatión da <tt><strong>RuntimeError</strong></tt> con <tt>torch.mm()</tt>:"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"elapsed":190,"status":"error","timestamp":1673465320447,"user":{"displayName":"Juanjo Garces","userId":"15357993443425315544"},"user_tz":-60},"id":"2UKLo8FrltUL","outputId":"5ddfafe8-ad4d-4534-eb66-d8b253483f7d"},"outputs":[{"ename":"RuntimeError","evalue":"self must be a matrix","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mRuntimeError\u001b[0m: self must be a matrix"]}],"source":["print(torch.mm(t1, t2))"]},{"cell_type":"markdown","metadata":{"id":"JSlSQJlbltUM"},"source":["___\n","# Operaciones avanzadas"]},{"cell_type":"markdown","metadata":{"id":"lXzB-rhEltUN"},"source":["## L2 o norma Euclídea (distancia euclídea)\n","Ver <a href='https://pytorch.org/docs/stable/torch.html#torch.norm'><strong><tt>torch.norm()</tt></strong></a>\n","\n","La <a href='https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm'>norma Euclídea</a> para el vector $x$ donde $x=(x_1,x_2,...,x_n)$ es calculada como <br>\n","\n","${\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}$\n","\n","\n","Cuando se aplica a una matriz, <tt>torch.norm()</tt> retorna la norma matricial o <a href='https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm'>Frobenius norm</a> por defecto."]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1641574422658,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"To__Yz5jltUN","outputId":"2f995db9-cdd8-47dc-f919-1483d111f01d"},"outputs":[{"data":{"text/plain":["tensor(17.)"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.tensor([2.,5.,8.,14.])\n","x.norm()"]},{"cell_type":"markdown","metadata":{"id":"vvD50Nf_ltUN"},"source":["## Número de elementos\n","Ver <a href='https://pytorch.org/docs/stable/torch.html#torch.numel'><strong><tt>torch.numel()</tt></strong></a>\n","\n","Retorna el numero de elementos en un tensor."]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1641574438815,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"hls-47JfltUO","outputId":"0db5f963-7bda-457d-c9e3-275f2823bac4"},"outputs":[{"data":{"text/plain":["21"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.ones(3,7)\n","x.numel()"]},{"cell_type":"markdown","metadata":{"id":"Y-Wbp_VAltUO"},"source":["Esto es espcialmente útil en ciertos cálculos como el error cuadrático medio ($Mean Squared Error$):<br>\n","<tt>\n","def mse(t1, t2):<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;diff = t1 - t2<br>\n","    &nbsp;&nbsp;&nbsp;&nbsp;return torch.sum(diff * diff) / diff<strong>.numel()</strong></tt>"]},{"cell_type":"markdown","metadata":{"id":"PJnSCwuj_GE6"},"source":["## Gradientes con tensores\n","\n","Podemos combinar tensores en las operaciones artiméticas habituales. Veamos un ejemplo:"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1641574525942,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"w1ip8DUi_GE6","outputId":"24d8a37f-e2c4-44d3-f1ba-adcc08b56832"},"outputs":[{"data":{"text/plain":["(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["# Creamos tensores. Atención a los argumentos...\n","x = torch.tensor(3.)\n","w = torch.tensor(4., requires_grad=True)\n","b = torch.tensor(5., requires_grad=True)\n","x, w, b"]},{"cell_type":"markdown","metadata":{"id":"sY8ds-wN_GE7"},"source":["Hemos creado tres tensores: `x`, `w`, and `b`, todos ellos números. `w` y `b` tienen un parámetro adicional `requires_grad` puesto a `True`. Veremos lo que hace en las siguientes celdas\n","\n","Vamos a crear un tensor `y` que combina dichos tensores."]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1641574578982,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"yCCamx4k_GE7","outputId":"cab37828-dbf3-45eb-beca-37a6b8bfc426"},"outputs":[{"data":{"text/plain":["tensor(17., grad_fn=<AddBackward0>)"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["y = w * x + b\n","y"]},{"cell_type":"markdown","metadata":{"id":"AD2qOIi-_GE7"},"source":["Como es de esperar, `y` es un tensor con el valor `3 * 4 + 5 = 17`. \n","Lo que hace a Pytorch único es que **podemos calcular automáticamente** la derivada de `y` w.r.t. (con respecto a) los tensores que tienen configurado `requires_grad` a `True` en este caso w and b. Esta funcionalidad de PyTorch se llama _autograd_ (automatic gradients).\n","\n","Para calcular las derivadas, tenemos que invocar el método `.backward` en el resultado `y`."]},{"cell_type":"code","execution_count":92,"metadata":{"id":"zZYQstan_GE8"},"outputs":[{"ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calcular derivadas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/work/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/work/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}],"source":["# Calcular derivadas\n","y.backward()"]},{"cell_type":"markdown","metadata":{"id":"i4v7zusU_GE8"},"source":["Las derivadas de `y` con respecto a los tensores de entrada estan almacenadas en el atributo`.grad` de los respectivos tensores."]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1641574654261,"user":{"displayName":"Juanjo Garces","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Girb9Xks6WIv16JGqJpHaAp8qGqkbBASBUbBOFusg=s64","userId":"15357993443425315544"},"user_tz":-60},"id":"xe4VBmk4_GE9","outputId":"37f44c0d-6d53-4798-a001-4b0d052caaf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["dy/dx: None\n","dy/dw: tensor(3.)\n","dy/db: tensor(2.)\n"]}],"source":["# Mostrar gradientes\n","print('dy/dx:', x.grad)\n","print('dy/dw:', w.grad)\n","print('dy/db:', b.grad)"]},{"cell_type":"markdown","metadata":{"id":"bsxsE7JP_GE9"},"source":["Como era de esperar, `dy/dw` tiene el mismo valor que `x`, i.e., `3`, y `dy/db` tiene el valor `1`. Observa que `x.grad` es `None` porque `x` no tiene `requires_grad` puesto a `True`. <br> \n","El nombre  \"grad\" en `w.grad` es una abreviatura para _gradient_, que es otra denominación de la derivada. El término _gradient_ es principalmente usado cuando se trabaja con vectores y matrices y es especialmente útil en la **computación de redes neuronales**. "]},{"cell_type":"markdown","metadata":{"id":"n2QOf-HEltUP"},"source":["## Fin del Notebook"]},{"cell_type":"markdown","metadata":{"id":"BZDr1zYmBkRV"},"source":["Referencias y modelos empleados para el Notebook: \n","\n","*   Documentación de [Pytorch](https://pytorch.org/docs/stable/index.html) \n","*   [PyTorch Tutorial for Deep Learning Researchers](https://github.com/yunjey/pytorch-tutorial) by Yunjey Choi\n","*   [FastAI](https://www.fast.ai/) development notebooks by Jeremy Howard.\n","*   Documentación y cursos en [Pierian Data](https://www.pieriandata.com/)\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"12kt5EoBNTZuZMTqjarRF28gX5nFMr3V7","timestamp":1609616137829}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
