{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"provenance":[{"file_id":"1JSTUoVgg74IMjfsMOBubajfTIvBWj7VI","timestamp":1609881644021}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"9nqPW8adYjVJ"},"source":["![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \n","\n","#<strong>**Máster en Inteligencia Artificial Avanzada y Aplicada  IA^3**</strong>\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"MqTSfSSlcCeJ"},"source":["##<center>**Deep learning aplicado a clasificación**<center>"]},{"cell_type":"markdown","metadata":{"id":"vfnAssBm2cZv"},"source":["##Ejercicio Propuesto\n","\n","Empleando como base el Notebook comentado de regresión, realiza ahora la adaptación necesaria para convertirlo a un problema de clasificación. \n","\n","En concreto, el objetivo ahora es predecir el tipo de tarifa 0 o 1 que se cobrará por cada viaje, teniendo en cuenta las mismas variables e información que en el caso de regresión. El código es prácicamente igual en muchos aspectos, pero hay que tener en cuenta los cambios necesarios en varios puntos como \n","* la variable target pasa a ser otra. Revisa el dataframe para encontrarla\n","* la arquitectura de la red ahora debe proporcionar dos salidas\n","* la función de coste a emplear debe ser apropiada para clasificación\n","* la forma de evaluar los resultados debe cambiar\n","\n","El Notebook mantiene las mismas secciones y en el mismo orden como guía para el proceso"]},{"cell_type":"markdown","metadata":{"id":"ZLjAp0-BGBxz"},"source":["## Red neuronal completa, problema de clasificación\n","El objetivo es estimar el **tipo de tarifa** de un viaje en taxi de la ciudad de Nueva York a partir de varios datos. La inspiración detrás de este código es una reciente <a href='https://www.kaggle.com/c/new-york-city-taxi-fare-prediction'>competición de kaggle</a>.\n","\n","En el notebook nos vamos a centrar en varios aspectos: \n","\n","*   Importación de los datos desde ficheros csv\n","*   Abordaje de un problema real, con diferentes tipos de variables.\n","*   Procesado de las variables y generación de nuevas\n","*   Tratamiento de variables categóricas\n","*   Preparación de un modelo con varias capas ocultas (_deep_)\n","*   Entrenamiento, validación y testeo del modelo\n","* Guardado del modelo\n","* Realización de nuevas predicciones.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dPzSwyOtGBx0"},"source":["## Importaciones standard\n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"LJRIkmKxGBx0"},"source":["import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IOBCsYrYGBx1"},"source":["## Carga del dataset NYC Taxi Fares \n","\n","La  <a href='https://www.kaggle.com/c/new-york-city-taxi-fare-prediction'>competición de Kaggle</a> provee un conjunto de datos de cerca 55 millones de registros. Los datos solo contienen las fecha, hora, latitud, longitud de la recogida y del destino, el número de pasajeros y el coste del viaje, que es el objetivo a predecir. Queda a elección del participante obtener y emplear cualquier información adicional. Por ejemplo, ¿influye la hora del día? ¿el día de la semana? ¿como determinamos la distancia recorrida?\n","Para este ejercicio vamos a limitar el dataset a (solo) 120000 registros, desde el 11 al 24 de Abril de 2010. Los registros se han ordenado aleatoriamente. Vamos a ver como podemos calcular distancias desde coordenadas GPS y como preparar un dataframe de pandas con los datos que consideremos necesarios, como aprovechar la información de fecha y hora, generar nuevas variables de interés , etc.\n"]},{"cell_type":"markdown","metadata":{"id":"W0r0gcKMckP6"},"source":["Para cargar los datos del dataset que vamos a emplear tenemos diversas opciones. \n","* Los datos originales podemos cargarlos del repositorio de Kaggle de la competición. Sin embargo son muchos datos y en el ejercicio únicamente vamos a emplear una parte de ellos. \n","* Podemos cargarlos desde nuestra unidad Drive ejecutando los siguientes scripts y siguiendo las instrucciones. Para montar la unidad nos genera una clave particular que hemos de introducir. A oartir de ese momento, la estructura de carpetas de nuestro disco Drive es accesible para el Notebook dentro de /content/drive. Más detalles en la [doc de Google](https://colab.research.google.com/notebooks/io.ipynb)"]},{"cell_type":"code","metadata":{"id":"1N0wH5ELaqGr"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kn7DjM3GbcP1"},"source":["#Lee el fichero \"NYCTaxiFares.csv\"  desde tu Drive o localmente y examinalo con head\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9IAScb3GBx2"},"source":["#Examina la variable target `fare_class`:\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQJmiFtZti6T"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQIR_HbSGBx2"},"source":["Observamos que 2/3 estan por debajo de 10\\$ y 1/3 han tenido una tarifa de 10\\$ o más.\n","\n","La correspondencia entre tarifas y la clase asignada es:\n","<table style=\"display: inline-block\">\n","<tr><th>Class</th><th>Values</th></tr>\n","<tr><td>0</td><td>< \\$10.00</td></tr>\n","<tr><td>1</td><td>>= \\$10.00</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"obTVf4IwGBx3"},"source":["## Calculando la disancia recorrida\n","\n","La <a href='https://en.wikipedia.org/wiki/Haversine_formula'>fórmula haversine </a> calcula la distancia en una esfera, dados dos puntos en coordenadas GPS.\n","\n","Vamos a llamar a la latitud $\\varphi$ (phi) y a la longitud $\\lambda$ (lambda).\n","\n","La formula de esta distancia es:\n","\n","${\\displaystyle d=2r\\arcsin \\left({\\sqrt {\\sin ^{2}\\left({\\frac {\\varphi _{2}-\\varphi _{1}}{2}}\\right)+\\cos(\\varphi _{1})\\:\\cos(\\varphi _{2})\\:\\sin ^{2}\\left({\\frac {\\lambda _{2}-\\lambda _{1}}{2}}\\right)}}\\right)}$\n","\n","donde\n","\n","$\\begin{split} r&: \\textrm {radio de la esfera (el radio de la Tierra es de promedio 6371 km)}\\\\\n","\\varphi_1, \\varphi_2&: \\textrm {latitudes de punto 1 a punto 2}\\\\\n","\\lambda_1, \\lambda_2&: \\textrm {longitudes de punto 1 a punto 2}\\end{split}$"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"hT1_cI9HGBx3"},"source":["def haversine_distance(df, lat1, long1, lat2, long2):\n","    \"\"\"\n","    Calculo de la distancia haversine entre 2 puntos GPS\n","    \"\"\"\n","    r = 6371  # radio de la Tierra en km\n","       \n","    phi1 = np.radians(df[lat1])\n","    phi2 = np.radians(df[lat2])\n","    \n","    delta_phi = np.radians(df[lat2]-df[lat1])\n","    delta_lambda = np.radians(df[long2]-df[long1])\n","     \n","    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n","    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n","    d = (r * c) # en km\n","\n","    return d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2SrCJP8K7CP"},"source":["Creamos una nueva columna con las distancias calculadas"]},{"cell_type":"code","metadata":{"id":"-9R3_gtnGBx4"},"source":["## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is6MNfUiGBx4"},"source":["## Columna FechaHora y valores derivados que pueden ser útiles\n","\n","La Fecha y hora nos vienen en un formato importado de texto. Pasar eso a un objete de fechahora nos permitirá extraer información como el día de la semana, momento del día am o pm, etc.\n","\n","_**Nota**: Los datos están grabados en formato UTC. Teniendo en cuenta la fecha y el horario que manejan en Nueva York en ese momento, la hora correcta EDT necesita un ajuste de 4 horas menos (UTC-4)_\n","\n","Generamos las nuevas columnas"]},{"cell_type":"code","metadata":{"id":"m4sTH9GtGBx4"},"source":["# Genera EDTdate, Hour, AMorPM y Weekday\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RypBuYZqGBx5"},"source":["# Revisa valor mínimo de EDTdate: \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-sDhZ5DGBx8"},"source":["# Revisa valor máximo de EDTdate: \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQNvuunTGBx8"},"source":["## Separamos variables categóricas y continuas\n","\n"]},{"cell_type":"markdown","metadata":{"id":"djx9TU6sGBx-"},"source":["Especifica las columnas que vamos a usar de cada tipo. Observemos que **no** vamos a emplear todas. Dejamos fuera pickup_datetime y EDTdate ya que en su lugar vamos a emplear las nuevas columnas categóricas que hemos prparado</div>\n"]},{"cell_type":"code","metadata":{"id":"92_5cSgrGBx9"},"source":["# Muestra los nombres de todas las columnas: \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"_6sG0hpnGBx9"},"source":["# Especifica las que corresponden a categóricas, continuas y la variable objetivo\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcdMZyYWGBx-"},"source":["## Categorizar\n","\n","Panda nos permite emplear un tipo de dato <a href='https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html'><strong>category dtype</strong></a> para convertir valores categóricos a códigos numéricos. Así, un dataset con meses del año tendrá asignados 12 códigos, uno por mes (0-11). Lo que hace pandas es sustituir las columnas por códigos y retiene una lista índice de las categorías. En los siguientes pasos llamaremos a las categorías`categories` y a su codificación `codes`"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ZJ3YlZgjGBx_"},"source":["# Categoriza las variables categóricas\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqTnMb6AGBx_"},"source":["# Comprueba los tipos \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOe-9vErGBx_"},"source":["Vamos a comprobar como `df['Hour']`es ahora una variable categórica codificada:"]},{"cell_type":"code","metadata":{"id":"tl8w71w2GByA"},"source":["# Comprueba la variable Hour: \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6HirlgkGByB"},"source":["# Comprueba la variable AMorPM: \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zlmIHF6Ke4kF"},"source":["# Comprueba la variable Weekday: \n","\n","## TU CÓDIGO AQUÍ\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FaLV1FmRGByD"},"source":["**NOTA:** Observese que los días de la semana están codificados sin el orden \"normal\".\n","\n","**NOTA2:** Los valores NaN se codifican como -1. No tenemos ninguno en ese conjunto de datos en particular. "]},{"cell_type":"markdown","metadata":{"id":"wdLuKk-dGByE"},"source":["Ahora podemos combinar los códigos de las tres columnas categóricas en un array de entrada con la función de Numpy <a href='https://docs.scipy.org/doc/numpy/reference/generated/numpy.stack.html'><tt>numpy.stack</tt></a>. No necesitamos los índices de las categorías, solo los valores. "]},{"cell_type":"code","metadata":{"id":"uESkYD0RGByE"},"source":["# Prepara los valores de cada variable y combinalos:\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-G9erjQGByF"},"source":["**NOTA:** Esto se puede hacer en una sola línea empleando _list comprehension_:\n","\n","`cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1)`\n","\n","De momento no nos preocuparemos por el tipo de datos `dtype`. Lo podremos convertir a int64 cuando pasemos el array a tensor.\n"]},{"cell_type":"markdown","metadata":{"id":"lUGJCA25GByF"},"source":["## Conversión de numpy arrays a tensores"]},{"cell_type":"code","metadata":{"id":"d2OfxZBCGByF"},"source":["# Pasa a tensores las Categoricas\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IH5z05pNGByG"},"source":["También pasaremos las continuas a tensor para el modelo. No las vamos a normalizar aquí, dejamos ese paso para realizarlo dentro del modelo. \n","\n","**NOTA:** Por cuestiones de la normalización que realizaremos posteriormente, vamos a dejar las variables continuas como `Float (float32)` en lugar de `Double (float64)`"]},{"cell_type":"code","metadata":{"id":"ItjJiJoQGByG"},"source":["# Pasa a tensores las Continuas \n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lA_01zDwGByH"},"source":["# verifica el tipo de las continuas\n","\n","## TU CÓDIGO AQUÍ  **"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KdQ3PERnvZ0F"},"source":["\n","**Nota**: La función CrossEntropyLoss que se puede usar para evaluar el coste en un problema de clasificación espera un tensor 1D. Es por ello que se debe reemplazar <tt>.reshape(-1,1)</tt> (genera formato columna) por <tt>.flatten()</tt> (genera formato fila/vector)."]},{"cell_type":"code","metadata":{"id":"1-vv4_ahGByH"},"source":["# Convertimos etiquetas a tensor\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2VyFQBGhBmi"},"source":["Comprobamos dimensiones: "]},{"cell_type":"code","metadata":{"id":"6m90vYswGByI"},"source":["# de cats\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lf5kQNpOGByI"},"source":["# de conts\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"koCJEEAQGByJ"},"source":["# del vector de salida y\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JIfFNr6RGByJ"},"source":["## Embedding para las variables categóricas\n","\n","Las variables categóricas proporcionan un mejor resultado si en lugar de tratarlas como un código numérico, realizamos un proceso llamado Embedding (se traduciría como Integración/incrustación). En el embedding cada código asignado se mapea a unas variables nuevas. Esto es así porque el embedding resuelve ciertos problemas que se dan al tratar variables categóricas, que representan categorías a menudo no ordinales, pero sí relacionadas en otros aspectos.\n","\n","Existen diversas formas de afrontar ese recodificado. Uno de los más simples y conocidos consiste en el OHE _One Hot Encoding_. Sin embargo, existen formas algo más evolucionadas como la que vamos a tratar en este ejercicio. \n","El Embedding es una parte fundamental en el procesado de lenguaje natural, aunque su uso no se limita a dicha area. \n","Encontramos [aquí](https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9) y [aquí](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526) buenas explicaciones de los que es y porqué es conveniente. \n","\n","La regla del pulgar para determinar un tamaño de embedding es dividir el numero total de categorías únicas de la variable en cuestion entre 2, pero sin pasar de 50. "]},{"cell_type":"code","metadata":{"id":"aWEPWZTNGByK"},"source":["# Determina los tamaños para embedding para Hours, AMvsPM y Weekdays\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsEMIR-UGByK"},"source":["## Definimos un modelo tabular (_TabularModel_)\n","\n","Este tipo de modelo está inspirado en los procedimientos de la librería <a href='https://docs.fast.ai/tabular.models.html'>fast.ai library</a>  El hecho de llamarlo modelo tabular viene de que los datos provienen de tablas, con tipos diferentes y que en mayor o menor medida se han procesado previamente. En ese preprocesado se generan o descartan variables si se considera y se hace una distinción entre variables continuas y categóricas. \n","El objetivo final es definir un modelo basado en el número de variables continuas (dado por <tt>conts.shape[1]</tt>) más el número de variables categóricas y sus embeddings (dados por <tt>len(emb_szs)</tt> y <tt>emb_szs</tt> respectivamente).  La salida será una regresión (un valor único, tipo flotante), o una clasificación (un grupo de intervalos y sus valores softmax).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WTQyts24GByK"},"source":["**Vamos a estudiar en detalle los pasos que vamos a realizar a continuación**\n","\n","1. Extendemos la clase base Module indicandle los siguientes parámetros:\n","   * <tt>emb_szs: </tt>lista de tuplas: el tamaño de cada variable categórica (número de categorías únicas) junto con el tamaño elegido para su _embedding_\n","   * <tt>n_cont:  </tt>int: número de variables continuas\n","   * <tt>out_sz:  </tt>int: tamaño de salida\n","   * <tt>layers:  </tt>list of ints: tamaño de las capas intermedias que deseemos poner\n","   * <tt>p:       </tt>float: factor de dropout para cada capa (por simplicidad usaremos el mismo en todas las capas)\n","\n","<tt><font color=black>class TabularModel(nn.Module):<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5):<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().\\_\\_init\\_\\_()</font></tt><br>\n","\n","2. Generamos las capas embedded con  <a href='https://pytorch.org/docs/stable/nn.html#modulelist'><tt><strong>torch.nn.ModuleList()</strong></tt></a> y con  <a href='https://pytorch.org/docs/stable/nn.html#embedding'><tt><strong>torch.nn.Embedding()</strong></tt></a>\n","Los datos categóricos serán filtrados a través del Embeddings en la sección forward de la clase con el siguiente código:<br>\n","<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])</font></tt><br><br>\n","3. Incluidmos un factor dropout `p` en el embeddings con <a href='https://pytorch.org/docs/stable/nn.html#dropout'><tt><strong>torch.nn.Dropout()</strong></tt></a> Por defecto pondremos `p=0.5`<br>\n","<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.emb_drop = nn.Dropout(emb_drop)</font></tt><br><br>\n","4. Realizaremos una normalización sobre las variables continuas con <a href='https://pytorch.org/docs/stable/nn.html#batchnorm1d'><tt><strong>torch.nn.BatchNorm1d()</strong></tt></a><br>\n","<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.bn_cont = nn.BatchNorm1d(n_cont)</font></tt><br><br>\n","5. Definimos una secuencia de capas de red neuronal donde cada nivel incluirá una función lineal, una función de activación (en nuestro caso <a href='https://pytorch.org/docs/stable/nn.html#relu'><strong>ReLU</strong></a>),\n","un paso de normalización y un factor de dropout. Para combinar la secuencia de capas usaremos <a href='https://pytorch.org/docs/stable/nn.html#sequential'><tt><strong>torch.nn.Sequential()</strong></tt></a>. Obsérvese que este código permite especificar **tantas capas ocultas** como especifiquemos en el argumento `layers`<br>\n","<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.bn_cont = nn.BatchNorm1d(n_cont)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;layerlist = []<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;n_emb = sum((nf for ni,nf in emb_szs))<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;n_in = n_emb + n_cont<br>\n","<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;for i in layers:<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.Linear(n_in,i)) <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.ReLU(inplace=True))<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.BatchNorm1d(i))<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.Dropout(p))<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_in = i<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.Linear(layers[-1],out_sz))<br>\n","<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;self.layers = nn.Sequential(*layerlist)</font></tt><br><br>\n","6. Definimos el método `forward`. En este método se realiza el preprocesado de  embeddings sobre las categóricas y la normalización sobre las continuas antes de unirlas todas y pasarlas a través de las capas. <br>Usaremos <a href='https://pytorch.org/docs/stable/torch.html#torch.cat'><tt><strong>torch.cat()</strong></tt></a> para combinar los tensores en uno.<br>\n","<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, x_cat, x_cont):<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;embeddings = []<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;for i,e in enumerate(self.embeds):<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings.append(e(x_cat[:,i]))<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;x = torch.cat(embeddings, 1)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;x = self.emb_drop(x)<br>\n","<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;x_cont = self.bn_cont(x_cont)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;x = torch.cat([x, x_cont], 1)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;x = self.layers(x)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;return x</font></tt>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"qZvpP7O5GByO"},"source":["Teniendo en cuenta todo esto, define la clase `TabularModel`: "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"NjftU1GIGByQ"},"source":["class TabularModel(nn.Module):\n","\n","    ## TU CÓDIGO AQUÍ\n","\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojuXHTDA79UU"},"source":["Establece una semilla de 33 para reproducibilidad de resultados e instancia el modelo. \n","**NOTA** Configura dos capas ocultas de [200,100] y ten en cuenta que el nuevo modelo requiere 2 salidas. Pon un factor de dropout de 0.4"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"LmVK4ChNGByQ"},"source":["## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ivCSRU76bYg"},"source":["Comprobemos que la arquitectura es la correcta: "]},{"cell_type":"code","metadata":{"id":"y_kjWxznGByR"},"source":["## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6RuEF59GByR"},"source":["## Definimos función de coste y optimizador\n","En ejercicios anteriores hemos usado la función de coste MSE y RMSE. Para nuestro problema de clasificación vamos a usa la fcnión de coste de Entropía Cruzada <a href='https://pytorch.org/docs/stable/nn.html#crossentropyloss'><strong><tt>torch.nn.CrossEntropyLoss()</tt></strong></a><br>\n","\n","Para el optimizador continuamos <a href='https://pytorch.org/docs/stable/optim.html#torch.optim.Adam'><strong><tt>torch.optim.Adam()</tt></strong></a> "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"2_QL4CF0GByR"},"source":["# define criterio y optimizador\n","\n","## TU CÓDIGO AQUÍ  **"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"xPq-EJ8JUhX-"}},{"cell_type":"markdown","metadata":{"id":"SN81m27pGByS"},"source":["## Preparamos conjuntos de train/test\n","En este punto, nuestro conjunt es un único lote de 120000 registros.  Esto llevará un tiempo de entrenamiento, por lo que podemos plantearnos reducirlo. Vamos a emplear 60000. Recordemos que los tensores ya estaban ordenados aleatoriamente."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ORraM46BGByS"},"source":["# Toma un lote de 60000 y reserva el 20% para test\n","batch_size = 60000\n","test_size = int(batch_size * .2)\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-f0hIUHpGByS"},"source":["# comprueba cantidad para train\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKY3pW2zGByT"},"source":["# comprueba cantidad para test\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYZUfkcuyRFf"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YDbGD4JOGByT"},"source":["## Entrenando el modelo \n","\n","Genera un bucle de entrenamiento con todos los pasos que conlleva. Empieza con 300 epochs, aunque esto es a elección. \n","Recuerda mostrar cada ciertos epochs información sobre el proceso. Incluye instrucciones e infor para controlar el tiempo empleado en el entrenamiento.\n"]},{"cell_type":"code","metadata":{"id":"XHuDDrRxGByT"},"source":["import time\n","start_time = time.time()\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PW6mAuK1GByU"},"source":["## Visualizamos la función de error "]},{"cell_type":"code","metadata":{"id":"1dAZvynkGByU"},"source":["# Plotea la pérdida/error frente a los epochs\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MgMLMPTPGByU"},"source":["## Validamos el modelo\n","Ahora vamos a lanzar el modelo con el conjunto de test y a comparar los resultados con las etiquetas conocidas. \n","\n","Dado que en este paso no es necesario actualizar pesos ni biases, no es necesario emplear la función autograd, por lo que ponemos <tt>torch.no_grad()</tt> y evitamos cálculos (y tiempo ) innecesario."]},{"cell_type":"code","metadata":{"id":"oiWCWvvkGByV"},"source":["# (método validation_step en el ejemplo anterior)\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wa9wsFiZGByV"},"source":["El valor del CE por sí solo no es un buen indicador en este caso. \n","\n","Vamos a obtener la precision del modelo calculando la tasa de aciertos para todos los registros del conjunto de test, imprimiendo únicamente el resultado final"]},{"cell_type":"code","metadata":{"id":"-dP-0dPv0CfS"},"source":["# Recorremos todos los elementos de y_test y comprobamos con y_val\n","\n","## TU CÓDIGO AQUÍ **"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnWk8Vj4GByW"},"source":["## Guardando el modelo entrenado\n","Recordemos que podemos salvar un modelo entrenado como un fichero en disco de forma que podamos recuperarlo posterormente para realizar predicciones o continuar entrenandolo con más datos. \n","Habitualmente lo que suele hacerse es salvar las matrices de pesos y biases y no toda la definición. Podemos encontrar más información sobre el proceso de guardado y sus recomendaciones en  <a href='https://pytorch.org/tutorials/beginner/saving_loading_models.html'>https://pytorch.org/tutorials/beginner/saving_loading_models.html</a>"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"BLRKEkTxGByW"},"source":["# Aseguramos que lo guardao SOLO depués de haberlo entrenado!\n","\n","## TU CÓDIGO AQUÍ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcyMaXBYGByY"},"source":["\n","Ahora podemos definir una función que solicite los datos de entrada al usuario , realice todos los pasos de preprocesado definidos al principio del Notebook y los pase al modelo entrenado con el fin de obtener una predicción concreta. "]},{"cell_type":"markdown","metadata":{"id":"gI_FZ99X1MmT"},"source":["## Introduciendo nuevos datos a través del modelo\n","\n","Define una función para introducir los datos y que muestre la nueva predicción con ellos:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"9DjDv2VoGByY"},"source":["def test_data(mdl): # nombre del modelo que hemos instanciado\n","    # Nuevos datos:\n","   \n","   ## TU CÓDIGO AQUÍ\n","    \n","    # Preprocesado de lso datos:\n","    ## TU CÓDIGO AQUÍ\n","   \n","    \n","    # Recodificamos las categoricas:\n","    ## TU CÓDIGO AQUÍ\n","\n","    # Creamos y concatenamos tensores\n","    ## TU CÓDIGO AQUÍ\n","    \n","    # Pasamos los nuevos datos al modelo. Sin backpropagacion\n","    \n","    ## TU CÓDIGO AQUÍ\n","    print(f'\\nLa tarifa resultante es {z}')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9EvAoEKYGByZ"},"source":["\n","Como referencia, estos son los valores máximos y mínimos de las variables solicitadas:\n","<table style=\"display: inline-block\">\n","<tr><th>Column</th><th>Mínimo</th><th>Máximo</th></tr>\n","<tr><td>pickup_latitude</td><td>40</td><td>41</td></tr>\n","<tr><td>pickup_longitude</td><td>-74.5</td><td>-73.3</td></tr>\n","<tr><td>dropoff_latitude</td><td>40</td><td>41</td></tr>\n","<tr><td>dropoff_longitude</td><td>-74.5</td><td>-73.3</td></tr>\n","<tr><td>passenger_count</td><td>1</td><td>5</td></tr>\n","<tr><td>EDTdate</td><td>2010-04-11 00:00:00</td><td>2010-04-24 23:59:42</td></tr>"]},{"cell_type":"markdown","metadata":{"id":"-RIcbndgGByZ"},"source":["<strong>ATENCION!</strong> La distancia entre 1 grado de latitud (40 a 41) is 111km y en q grade de longitud (-73 a -74) is 85km. El viaje más largo en el dataset muestra una diferencia de solo 0.243 grados lat. y  0.284 de long. La iferencia media para ambos está en torno a 0.02. Para obtener una buena predicción hay que emplear valores cercanos. "]},{"cell_type":"markdown","metadata":{"id":"naKq89bzCN2Z"},"source":["Probamos la función instanciando el modelo en un nuevo objeto y cargando los pesos que hemos guardado"]},{"cell_type":"code","metadata":{"id":"DtCc5oS1CesS"},"source":["# Definimos antes los tamaños del embedding y luego instanciamos:\n","emb_szs = [(24, 12), (2, 1), (7, 4)]\n","model2 = TabularModel(emb_szs, 6, 2, [200,100], p=0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIH1SWR_CsR7"},"source":["# Cargamos los valores guardado en el nuevo objeto: \n","model2.load_state_dict(torch.load('/content/TaxiFareClssModel.pt'));\n","model2.eval() # No olvidar este paso"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wcp0_BvlGByZ"},"source":["z = test_data(model2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2QOf-HEltUP"},"source":["\n","Enhorabuena si has completado todas las secciones!\n","\n","## Fin del Notebook"]}]}