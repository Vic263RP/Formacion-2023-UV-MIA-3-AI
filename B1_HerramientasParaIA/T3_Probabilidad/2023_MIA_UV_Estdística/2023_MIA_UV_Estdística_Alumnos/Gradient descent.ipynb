{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Gradient descent.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"v0_w6fYlS-pM"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import seaborn as sns\n","import bokeh.plotting as bp\n","import matplotlib.pyplot as plt\n","from sklearn.datasets.samples_generator import make_regression \n","from scipy import stats \n","from bokeh.models import  WheelZoomTool, ResetTool, PanTool\n","%matplotlib inline "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1JrBVh3S-pp"},"source":["## What is \"learning from data\"?\n","\n",">In general **Learning from Data** is a scientific discipline that is concerned with the design and development of algorithms that allow computers to infer (from data) a  model that allows *compact representation* (unsupervised learning) and/or *good generalization* (supervised learning). \n","\n","This is an important technology because it enables computational systems to adaptively improve their performance with experience accumulated from the observed data. \n","\n","Most of these algorithms are based on the *iterative solution* of a mathematical problem that involves data and model. If there was an analytical solution to the problem, this should be the adopted one, but this is not the case for most of the cases.\n","\n","So, the most common strategy for **learning from data** is based on solving a system of equations as a way to find a series of parameters of the model that minimizes a mathematical problem. This is called **optimization**.\n","\n","The most important technique for solving optimization problems is **gradient descend**."]},{"cell_type":"markdown","metadata":{"id":"N_uGpFdiS-pz"},"source":["## Gradient descend (for *hackers*) for function minimization: 1-D\n","\n","Let's suppose that we have a function $f: \\Re \\rightarrow \\Re$. For example: \n","\n","$$f(x) = x^2$$\n","\n","Our objective is to find the argument  $x$ that minimizes this function (for maximization, consider $-f(x)$). To this end, the critical concept is the **derivative**.\n","\n","The derivative of $f$ of a variable $x$, $f'(x)$ or $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$,  is a measure of the rate at which the value of the function changes with respect to the change of the variable. It is defined as the following limit:\n","\n","\n","$$ f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h} $$\n","\n","The derivative specifies how to scale a small change in the input in order to obtain the corresponding change in the output: \n","\n","$$ f(x + h) \\approx f(x) + h f'(x)$$"]},{"cell_type":"code","metadata":{"id":"2S5c-9fYS-p5"},"source":["# numerical derivative at a point x\n","\n","def f(x):\n","    return x**2\n","\n","def fin_dif(x, f, h = 0.000001):\n","    return (f(x+h) - f(x))/h\n","\n","x = 2.0\n","print(fin_dif(x,f))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NHla7uYCS-p8"},"source":["The limit as $h$ approaches zero, if it exists, should represent the **slope of the tangent line** to $(x, f(x))$. \n","\n","For values that are not zero it is only an approximation."]},{"cell_type":"code","metadata":{"id":"JZo_acZLS-qA"},"source":["for h in np.linspace(0.0, 1.0 , 5):\n","    print(\"{:3.6f}\".format(f(5+h)), \"{:3.6f}\".format(f(5)+h*fin_dif(5,f)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Z25DANYuS-qH"},"source":["x = np.linspace(-1.5,-0.5, 100)\n","f = [i**2 for i in x]\n","plt.plot(x,f, 'r-')\n","plt.plot([-1],[1],'o')\n","plt.plot([-1.4],[1.96],'o') # h=0.4\n","plt.text(-1.0, 1.2,'$x,f(x)$')\n","plt.text(-1.4, 2.2,'$(x-h),f(x-h)$')\n","plt.plot([-1.4, -1.0], [1.96, 1.0], 'b-', lw=2) #the derivative with h=0.4\n","plt.plot([-1.5, -0.5], [2, 0.0], 'k-', lw=2) #tangent line (y=-2x-1)\n","plt.grid()\n","#plt.show"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmPLm0twS-qP"},"source":["It can be shown that the “centered difference formula\" is better when computing numerical derivatives:\n","\n","$$ \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h} $$\n","\n","The error in the \"finite difference\" approximation can be derived from Taylor's theorem and, assuming that $f$ is differentiable, is $O(h)$. In the case of “centered difference\" the error is $O(h^2)$."]},{"cell_type":"markdown","metadata":{"id":"QOLtprerS-qV"},"source":["The derivative tells us how steep the tangent line to the graph would be at a given point $(x, f(x))$. That steepness, or slope varies from point to point: some parts of the graph are flatter, and some parts slope up more steeply.\n","By examining the derivative at a given point $(x, f(x))$, we can know whether the function $f$ is increasing or decreasing at that point, and by how much."]},{"cell_type":"markdown","metadata":{"id":"PQFqwiMES-qe"},"source":["We can follow these steps to **decrease** the value of the function:\n","\n","+ Start from a random $x$ value.\n","+ Compute the derivative $f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h}$.\n","+ Walk a small step in the **opposite** direction of the derivative, because we know that $f(x - h \\mbox{ sign}(f'(x))$ < $f(x)$ for  small enough $h$. \n","\n","The search for the minima ends when the derivative is zero because we have no more information about which direction to move. $x$ is a critical o stationary point if $f'(x)=0$. \n","\n"," + A **minimum (maximum)** is a critical point where $f(x)$ is lower (higher) than at all neighboring points. \n"," + There is a third class of critical points: **saddle points**.\n","\n","If $f$ is a **convex function**, this should be the minimum (maximum) of our functions. In other cases it could be a local minimum (maximum) or a saddle point."]},{"cell_type":"code","metadata":{"id":"mNyP0X7XS-qi"},"source":["W = 400\n","H = 250\n","bp.output_notebook()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32-78lTfS-qm"},"source":["x = np.linspace(-15,15,100)\n","y = x**2\n","\n","TOOLS = [WheelZoomTool(), ResetTool(), PanTool()]\n","\n","s1 = bp.figure(width=W, plot_height=H, \n","               title='Local minimum of function',  \n","               tools=TOOLS)\n","s1.line(x, y, color=\"navy\", alpha=0.5, line_width=3)\n","s1.circle(0, 0, size =10, color=\"orange\")\n","s1.title.text_font_size = '12pt'\n","s1.yaxis.axis_label_text_font_size = \"14pt\"\n","s1.xaxis.axis_label_text_font_size = \"14pt\"\n","\n","bp.show(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmVHPCuWS-qq"},"source":["x = np.linspace(-15,15,100)\n","y = -x**2\n","\n","TOOLS = [WheelZoomTool(), ResetTool(), PanTool()]\n","\n","\n","s1 = bp.figure(width=W, plot_height=H, \n","               title='Local maximum of function',  \n","               tools=TOOLS)\n","s1.line(x, y, color=\"navy\", alpha=0.5, line_width=3)\n","s1.circle(0, 0, size =10, color=\"orange\")\n","s1.title.text_font_size = '12pt'\n","s1.yaxis.axis_label_text_font_size = \"14pt\"\n","s1.xaxis.axis_label_text_font_size = \"14pt\"\n","\n","bp.show(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-YROPwjS-qu"},"source":["x = np.linspace(-15,15,100)\n","y = x**3\n","\n","TOOLS = [WheelZoomTool(), ResetTool(), PanTool()]\n","\n","\n","s1 = bp.figure(width=W, plot_height=H, \n","               title='Saddle point of function',  \n","               tools=TOOLS)\n","s1.line(x, y, color=\"navy\", alpha=0.5, line_width=3)\n","s1.circle(0, 0, size =10, color=\"orange\")\n","s1.title.text_font_size = '12pt'\n","s1.yaxis.axis_label_text_font_size = \"14pt\"\n","s1.xaxis.axis_label_text_font_size = \"14pt\"\n","\n","bp.show(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tNG-W418S-qx"},"source":["There are two problems with numerical derivatives:\n","+ It is approximate.\n","+ It is very slow to evaluate (two function evaluations: $f(x + h) , f(x - h)$ ).\n","\n","Our knowledge from Calculus could help!"]},{"cell_type":"markdown","metadata":{"id":"4LF3Sr8PS-q0"},"source":["We know that we can get an **analytical expression** of the derivative for **some** functions. \n","\n","For example, let's suppose we have a simple quadratic function, $f(x)=x^2−6x+5$, and we want to find the minimum of this function. \n","\n","#### First approach\n","\n","We can solve this analytically using Calculus, by finding the derivate $f'(x)  =  2x-6$ and setting it to zero:\n","\n","\\begin{equation}\n","\\begin{split}\n","2x-6  & = & 0 \\\\\n","2x  & = & 6 \\\\\n","x & = & 3 \\\\\n","\\end{split}\n","\\end{equation} "]},{"cell_type":"code","metadata":{"id":"GBH_rxLaS-q2"},"source":["x = np.linspace(-10,20,100)\n","y = x**2 - 6*x + 5\n"," \n","TOOLS = [WheelZoomTool(), ResetTool(), PanTool()]\n","\n","\n","s1 = bp.figure(width=W, plot_height=H, \n","               tools=TOOLS)\n","s1.line(x, y, color=\"navy\", alpha=0.5, line_width=3)\n","s1.circle(3, 3**2 - 6*3 + 5, size =10, color=\"orange\")\n","s1.title.text_font_size = '12pt'\n","s1.yaxis.axis_label_text_font_size = \"14pt\"\n","s1.xaxis.axis_label_text_font_size = \"14pt\"\n","\n","bp.show(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLiLAGKKS-q5"},"source":["#### Second approach\n","\n","To find the local minimum using **gradient descend**: you start at a random point, and move into the direction of steepest **descent** relative to the derivative:\n","\n","+ Start from a random $x$ value.\n","+ Compute the derivative $f'(x)$ analitically.\n","+ Walk a small step in the **opposite** direction of the derivative. \n","\n","In this example, let's suppose we start at $x=15$. The derivative at this point is $2*15−6=24$. \n","\n","Because we're using gradient descent, we need to subtract the gradient from our $x$-coordinate: $f(x - f'(x))$. However, notice that $15−24$ gives us $−9$, clearly overshooting over target of $3$. "]},{"cell_type":"code","metadata":{"id":"eYGua194S-q7"},"source":["x = np.linspace(-10,20,100)\n","y = x**2 - 6*x + 5\n","start = 15\n","\n","TOOLS = [WheelZoomTool(), ResetTool(), PanTool()]\n","\n","\n","s1 = bp.figure(width=W, plot_height=H, \n","               tools=TOOLS)\n","s1.line(x, y, color=\"navy\", alpha=0.5, line_width=3)\n","s1.circle(start, start**2 - 6*start + 5, size =10, color=\"orange\")\n","\n","d = 2 * start - 6\n","end = start - d\n","\n","s1.circle(end, end**2 - 6*end + 5, size =10, color=\"red\")\n","s1.title.text_font_size = '12pt'\n","s1.yaxis.axis_label_text_font_size = \"14pt\"\n","s1.xaxis.axis_label_text_font_size = \"14pt\"\n","\n","bp.show(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xO8nklsBS-q-"},"source":["To fix this, we multiply the gradient by a step size. This step size (often called **alpha** or **learning rate**) has to be chosen carefully, as a value too small will result in a long computation time, while a value too large will not give you the right result (by overshooting) or even fail to converge. \n","\n","In this example, we'll set the step size to 0.01, which means we'll subtract $24×0.01$ from $15$, which is $14.76$. \n","\n","This is now our new temporary local minimum: We continue this method until we either don't see a change after we subtracted the derivative step size, or until we've completed a pre-set number of iterations."]},{"cell_type":"code","metadata":{"id":"6NUfB7rcS-q_"},"source":["old_min = 0\n","temp_min = 15 #initial random point\n","step_size = 0.01\n","precision = 0.0001\n"," \n","def f_derivative(x):\n","    return 2*x -6\n","\n","mins = []\n","cost = []\n","\n","while abs(temp_min - old_min) > precision:\n","    old_min = temp_min \n","    gradient = f_derivative(old_min) \n","    move = gradient * step_size\n","    temp_min = old_min - move\n","    cost.append((3-temp_min)**2) #squared distance from temp_min to the true local minimum\n","    mins.append(temp_min)\n","\n","# rounding the result to 2 digits because of the step size\n","print(\"Local minimum occurs at\", round(temp_min,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACldVhlvS-rB"},"source":["An important feature of gradient descent is that **there should be a visible improvement over time**: In this example, we simply plotted the squared distance from the local minima calculated by gradient descent and the true local minimum,  ``cost``, against the iteration during which it was calculated. As we can see, the distance gets smaller over time, but barely changes in later iterations. "]},{"cell_type":"code","metadata":{"id":"9TTppgGmS-rD"},"source":["TOOLS = [WheelZoomTool(), ResetTool(), PanTool()]\n","\n","x = list(range(len(cost)))\n","y = cost\n","#x, y = (zip(*enumerate(cost)))\n","s1 = bp.figure(width=W, \n","               height=H, \n","               title='Squared distance to true local minimum',  \n","#                title_text_font_size='14pt', \n","               tools=TOOLS,\n","               x_axis_label = 'Iteration',\n","               y_axis_label = 'Distance'\n",")\n","s1.line(x, y, color=\"navy\", alpha=0.5, line_width=3)\n","s1.title.text_font_size = '16pt'\n","s1.yaxis.axis_label_text_font_size = \"14pt\"\n","s1.xaxis.axis_label_text_font_size = \"14pt\"\n","bp.show(s1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WneXO2u2S-rF"},"source":["## From derivatives to gradient: $n$-dimensional function minimization.\n","\n","Let's consider a $n$-dimensional function $f: \\Re^n \\rightarrow \\Re$. For example: \n","\n","$$f(\\mathbf{x}) = \\sum_{n} x_n^2$$\n","\n","Our objective is to find the argument  $\\mathbf{x}$ that minimizes this function.\n","\n","The **gradient** of $f$ is the vector whose components are the $n$ partial derivatives of $f$. It is thus a vector-valued function. \n","\n","The gradient points in the direction of the greatest rate of **increase** of the function.\n","\n","$$\\nabla {f} = (\\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n})$$"]},{"cell_type":"code","metadata":{"id":"E0fLFIIWS-rI"},"source":["def f(x):\n","    return sum(x_i**2 for x_i in x)\n","\n","def fin_dif_partial_centered(x, f, i, h=1e-6):\n","    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n","    w2 = [x_j - (h if j==i else 0) for j, x_j in enumerate(x)]\n","    return (f(w1) - f(w2))/(2*h)\n","\n","def gradient_centered(x, f, h=1e-6):\n","    return[round(fin_dif_partial_centered(x,f,i,h), 10) for i,_ in enumerate(x)]\n","\n","x = [1.0,1.0,1.0]\n","\n","print(f(x), gradient_centered(x,f))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ViVLFew5S-rJ"},"source":["The function we have evaluated, $f({\\mathbf x}) = x_1^2+x_2^2+x_3^2$, is $3$ at $(1,1,1)$ and the gradient vector at this point is $(2,2,2)$. \n","\n","Then, we can follow this steps to maximize (or minimize) the function:\n","\n","+ Start from a random $\\mathbf{x}$ vector.\n","+ Compute the gradient vector.\n","+ Walk a small step in the opposite direction of the gradient vector.\n","\n","> It is important to be aware that this gradient computation is very expensive: if $\\mathbf{x}$ has dimension $n$, we have to evaluate $f$ at $2*n$ points."]},{"cell_type":"markdown","metadata":{"id":"WxUstdxWS-rL"},"source":["### How to use the gradient.\n","\n","$f(x) = \\sum_i x_i^2$, takes its mimimum value when all $x$ are 0. \n","\n","Let's check it for $n=3$: "]},{"cell_type":"markdown","metadata":{"id":"C07xLyu4S-rM"},"source":["Let's start by choosing a random vector and then walking a step in the opposite direction of the gradient vector. We will stop when the difference between the new solution and the old solution is less than a tolerance value."]},{"cell_type":"code","metadata":{"id":"Dpq7gJxtS-rN"},"source":["# choosing a random vector\n","\n","import random\n","import numpy as np\n","\n","x = [random.randint(-10,10) for i in range(3)]\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgF16PRLS-rO"},"source":["def step(x,grad,alpha):\n","    return [x_i - alpha * grad_i for x_i, grad_i in zip(x,grad)]\n","\n","def euc_dist(v1,v2):\n","    import numpy as np\n","    import math\n","    v = np.array(v1)-np.array(v2)\n","    return math.sqrt(sum(v_i ** 2 for v_i in v))\n","\n","tol = 1e-15\n","alpha = 0.01\n","while True:\n","    grad = gradient_centered(x,f)\n","    next_x = step(x,grad,alpha)\n","    if euc_dist(next_x,x) < tol: #calculate Euclidean distance in n dimensions\n","        break\n","    x = next_x\n","print([round(i,10) for i in x])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8SNgd6dS-rQ"},"source":["### Alpha\n","\n","The step size, **alpha**, is a slippy concept: if it is too small we will slowly converge to the solution, if it is too large we can diverge from the solution. \n","\n","There are several policies to follow when selecting the step size:\n","\n","+ Constant size steps. In this case, the size step determines the precision of the solution.\n","+ Decreasing step sizes with iterations.\n","+ At each iteration, select the optimal step.\n","\n","The last policy is good, but too expensive. In this case we will consider a fixed set of values (at each iteration):"]},{"cell_type":"code","metadata":{"id":"3TkHl98TS-rR"},"source":["step_size = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfZ8eTN6S-rS"},"source":["## Learning from data\n","\n","In general, we have:\n","\n","+ A dataset $(\\mathbf{x},y)$. \n","+ A target function $f_\\mathbf{w}$, that we want to minimize, representing the discrepancy between our data and the model we want to fit. The model is represented by a set of parameters $\\mathbf{w}$. \n","+ The gradient of the target function, $g_f$. \n","\n","\n","In the most common case $f_\\mathbf{w}$ represents the errors from a data representation model $M$. To fit the model is to find the optimal parameters $\\mathbf{w}$ that minimize the following expression:\n","\n","$$ f_\\mathbf{w} = \\sum_{i} (y_i - M(\\mathbf{x}_i,\\mathbf{w}))^2 $$\n","\n","For example, $(\\mathbf{x},y)$ can represent:\n","\n","+ $\\mathbf{x}$: the behavior of a \"Candy Crush\" player; $y$: monthly payments. \n","+ $\\mathbf{x}$: sensor data about your car engine; $y$: probability of engine error.\n","+ $\\mathbf{x}$: finantial data of a bank customer; $y$: customer rating.\n","\n","> If $y$ is a real value, it is called a *regression* problem.\n","\n","> If $y$ is binary/categorical, it is called a *classification* problem. \n","\n","Let's suppose that $M(x_i,\\mathbf{w}) = w * x_i$. \n","\n","### Batch gradient descent\n","\n","We can implement **gradient descent** in the following way (*batch gradient descent*):"]},{"cell_type":"code","metadata":{"id":"zGWKvseGS-rU"},"source":["#random.seed(10)\n","\n","# find the value of w that minimizes the target (objective) function f_w: Sum((y-wx)**2)\n","\n","x = range(10)\n","y = [2*i for i in x] # y = 2x\n","\n","# f_target = Sum ((y - wx)**2)\n","def target_f(x,y,w):\n","    import numpy as np\n","    return np.sum((np.array(y) - np.array(x) * w)**2.0)\n","\n","# gradient_f_target = Sum(2wx**2 - 2xy)\n","def gradient_f(x,y,w):\n","    import numpy as np\n","    return np.sum(2*w*(np.array(x)**2) - 2*np.array(x)*np.array(y))\n","\n","def step(w,grad,alpha):\n","    return w - alpha * grad\n","\n","def min_batch(target_f, gradient_f, x, y, toler = 1e-6):\n","    import random\n","    alphas = [100, 10, 1, 0.1, 0.001, 0.00001]\n","    w = random.random()\n","    val = target_f(x,y,w)\n","    print(\"First w:\", w, \"First Val:\", val, \"\\n\")\n","    i = 0\n","    while True:\n","        i += 1\n","        gradient = gradient_f(x,y,w)\n","        next_ws = [step(w, gradient, alpha) for alpha in alphas] #calculate the step for the different alphas\n","        next_vals = [target_f(x,y,w) for w in next_ws] #obtain next values per step\n","        min_val = min(next_vals) # select the minimum value\n","        next_w = next_ws[next_vals.index(min_val)] #save the step corresponding to the minimum value \n","        next_val = next_vals[next_vals.index(min_val)] #save the minimum function value\n","        alpha_val = alphas[next_vals.index(min_val)]   #save the alpha leading to minimum function value\n","        print(i, \"w: {:4.4f}\".format(w), \"Val:{:4.4f}\".format(val), \"Gradient:{:4.4f}\".format(gradient), \"Alpha:\",alpha_val)        \n","        if (abs(val - next_val) < toler) or (i>200):\n","            return w\n","        else:\n","            w, val = next_w, next_val\n","            \n","min_batch(target_f, gradient_f, x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Shbw6z3aS-rW"},"source":["### Exercise: \n","+ 1. Consider a set of 100 data points and explain the behavior of the algorithm. \n","+ 2. How could we fix this behavior?\n"]},{"cell_type":"code","metadata":{"id":"ExnUg3O_S-rY"},"source":["#Your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eaUGxQorS-ra"},"source":["### Stochastic Gradient Descent\n","\n","The last function evals the objective function $f_\\mathbf{w}$ with respect to each parameter/feature at every step.\n","\n","If the dataset is large, this strategy is too costly. In this case we will use a strategy called **SGD** (*Stochastic Gradient Descent*). It consists on replacing the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). \n","\n","Then, we compute the estimate of the gradient (and move towards the minimum) by using only **one data sample** (or a small data sample). We find the minimum by iterating this gradient estimation over the dataset.\n","\n","A full iteration over the dataset is called **epoch**. During an epoch, data must be used in a random order.\n","\n","If we apply this method we have some guarantees to find the minimum. "]},{"cell_type":"code","metadata":{"id":"YLYCvS3hS-rb"},"source":["import numpy as np\n","x = range(10)\n","y = [2*i for i in x]\n","data = list(zip(x,y))\n","\n","def in_random_order(data):\n","    import random\n","    indexes = [i for i,_ in enumerate(data)]\n","    random.shuffle(indexes)\n","    for i in indexes:\n","        yield data[i]\n","        \n","for (x_i,y_i) in in_random_order(data):\n","    print(x_i,y_i) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUB6z0iwS-rc"},"source":["def gradient_f_SGD(x,y,w):\n","    import numpy as np\n","    return 2*w*(np.array(x)**2) - 2*np.array(x)*np.array(y) #Note there is no sum of elements\n","\n","def SGD(target_f, gradient_f, x, y, alpha_0=0.001):\n","    import numpy as np\n","    import random\n","    data = list(zip(x,y))\n","    w = random.random()\n","    alpha = alpha_0\n","    min_w, min_val = float('inf'), float('inf')\n","    iteration_no_increase = 0\n","    alpha_vect = []\n","    min_w_vect = []\n","    while iteration_no_increase < 100: \n","        val = sum(target_f(x_i, y_i, w) for x_i,y_i in data) # full iteration of the global error with updated w\n","        if val < min_val: \n","            # the global error gets lower with updated w\n","            min_w, min_val = w, val\n","            iteration_no_increase = 0\n","            alpha = alpha_0\n","        else:\n","            # the global error is not improving -> decrease alpha temporarily (until out of possible local minima)\n","            iteration_no_increase += 1\n","            alpha *= 0.9 #decrease alpha\n","        for x_i, y_i in in_random_order(data): \n","            gradient_i = gradient_f(x_i, y_i, w)  #gradient evaluation at each point\n","            w = np.array(w) - (alpha *  np.array(gradient_i)) # update w\n","        min_w_vect.append(w)\n","        alpha_vect.append(alpha)\n","    return min_w_vect, alpha_vect"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKb7CN30S-re"},"source":["min_w_vect, alpha_vect = SGD(target_f, gradient_f_SGD, x, y, 0.001)\n","print(\"w= \",min_w_vect[-5:])\n","plt.plot(min_w_vect)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6NtrNPKS-rf"},"source":["plt.plot(alpha_vect)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_14Y6HTS-rg"},"source":["### Exercise: \n"," Run SDG considering a set of 100 data points and explain the behavior of the algorithm. \n"]},{"cell_type":"code","metadata":{"id":"Ax0ArTeES-rh"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kpSUvfeSS-ri"},"source":["## Gradient Descent and Linear Regression\n","\n","The linear regression model assumes a linear relationship between data:\n","\n","$$ y_i = w_1 x_i + w_0 $$\n","\n","Let's generate a more realistic dataset (with noise), where $w_1 = 2$ and $w_0 = 0$:"]},{"cell_type":"code","metadata":{"id":"0WYl2LQmS-rk"},"source":["import numpy as np\n","x = np.random.uniform(0,1,20)\n","\n","def f(x): return x*2\n","\n","noise_variance =0.2\n","noise = np.random.randn(x.shape[0])*noise_variance\n","y = f(x) + noise\n","\n","plt.plot(x, y, 'o', label='y')\n","plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\n","plt.xlabel('$x$', fontsize=15)\n","plt.ylabel('$y$', fontsize=15)\n","plt.ylim([0,2])\n","plt.title('inputs (x) vs targets (y)')\n","plt.grid()\n","plt.legend(loc=2)\n","plt.gcf().set_size_inches((10,6))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2siZb8fZS-rl"},"source":["# Our cost function\n","def cost(y, t): return ((t - y)**2).sum()\n","\n","ws = np.linspace(0, 4, num=100)  \n","cost_ws = np.zeros(ws.shape)\n","for ind,w in enumerate(ws):\n","    cost_ws[ind]=cost(x*w, y) \n","    \n","# Ploting the cost function for different values of w\n","plt.plot(ws, cost_ws, 'r-')\n","plt.xlabel('$w$', fontsize=15)\n","plt.ylabel('Cost', fontsize=15)\n","plt.title('Cost vs. $w$')\n","plt.grid()\n","plt.gcf().set_size_inches((10,6))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jk5OfA7nS-rm"},"source":["### Exercise\n","\n","Complete the following code and look at the plot of the first gradient descent updates. Explore the behavior of the proposed alpha or learning rate."]},{"cell_type":"code","metadata":{"id":"Iri83q-NS-ro"},"source":["#your solution here\n","\n","# gradient_f_target = Sum 2wx**2 - 2xy\n","def gradient_f(x,y,w):\n","    import numpy as np\n","    return np.sum(2*w*(np.array(x)**2) - 2*np.array(x)*np.array(y))\n","\n","# initial value for w\n","w = 0.1\n","\n","# define a learning_rate (alpha) \n","learning_rate = .1\n","\n","nb_of_iterations = 20  \n","w_cost = [(w, cost(x*w, y))] \n","for i in range(nb_of_iterations):\n","    # Here your code in 3 lines: \n","    #1) compute the gradient   \n","    \n","    #2) Walk a small step in the opposite direction of the derivative\n","    \n","    #3) save w and global error in w_cost\n","    \n","    \n","for i in range(0, len(w_cost)):\n","    print('w({}): {:.4f} \\t cost: {:.4f}'.format(i, w_cost[i][0], w_cost[i][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6vl220yS-rq"},"source":["Plot the gradient descent updates on the cost function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8riQkeblS-rr"},"source":["# your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3UdD5MUS-rs"},"source":["Plot the input data and linear regression results with the obtained w"]},{"cell_type":"code","metadata":{"id":"Xn66id8lS-rt"},"source":["# your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLtL8luiS-ru"},"source":["## Mini-batch Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"7aeSesE3S-rv"},"source":["In code, general batch gradient descent looks something like this:\n","\n","```python\n","nb_epochs = 100\n","for i in range(nb_epochs):\n","    grad = evaluate_gradient(target_f, data, w)\n","    w = w - learning_rate * grad\n","```\n","\n","For a pre-defined number of epochs, we first compute the gradient vector of the target function for the whole dataset w.r.t. our parameter vector. \n","\n","Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example and label:\n","\n","```python\n","nb_epochs = 100\n","for i in range(nb_epochs):\n","    np.random.shuffle(data)\n","    for example in data:\n","        grad = evaluate_gradient(target_f, example, w)\n","        w = w - learning_rate * grad\n","```\n","\n","Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of $n$ training examples:\n","\n","```python\n","nb_epochs = 100\n","for i in range(nb_epochs):\n","  np.random.shuffle(data)\n","  for batch in get_batches(data, batch_size=50):\n","    grad = evaluate_gradient(target_f, batch, w)\n","    w = w - learning_rate * grad\n","```\n","\n","Minibatch SGD has the advantage that it works with a slightly less noisy estimate of the gradient. However, as the minibatch size increases, the number of updates done per computation done decreases (eventually it becomes very inefficient, like batch gradient descent). \n","\n","There is an optimal trade-off (in terms of computational efficiency) that may vary depending on the data distribution and the particulars of the class of function considered, as well as how computations are implemented."]},{"cell_type":"code","metadata":{"id":"pAz2qLlRS-rw"},"source":[""],"execution_count":null,"outputs":[]}]}