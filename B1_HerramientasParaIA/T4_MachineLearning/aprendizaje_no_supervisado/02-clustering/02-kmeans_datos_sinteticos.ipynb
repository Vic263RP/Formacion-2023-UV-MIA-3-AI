{"cells":[{"cell_type":"markdown","metadata":{"id":"Fj06WDWPbLra"},"source":["<img src=\"figuras/mbit-logo.png\" align=\"right\" style=\"float\" width=\"200\">\n","<font color=\"#CA3532\"><h1 align=\"left\">Clustering</h1></font>\n","<h2 align=\"left\">Base de datos sintética. K-means (parte 2)</h2>"]},{"cell_type":"markdown","metadata":{"id":"O1Cw6XpDbLre"},"source":["Primero importamos las librerías que necesitaremos. También activaremos el modo *inline* para los gráficos generados por *matplotlib*. También inicializaremos la semilla del generador de números aleatorios."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcvbqF6vbLrf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn import metrics\n","from sklearn.datasets import make_moons, make_circles\n","from numpy import linalg\n","from matplotlib.patches import Ellipse"]},{"cell_type":"markdown","metadata":{"id":"UFXCY3zubLrg"},"source":["Ahora crearemos un problema no supervisado sintético. El objetivo es entender los conceptos principales con este prolema, luego analizaremos bases de datos reales. Usaremos la función \"make_blobs\" que genera datos con un número especificado de \"blobs\". Esta función escoge aleatoriamente los centros de los blobs.\n","\n","Los parámetros que cambiaremos se comentan aquí:"]},{"cell_type":"markdown","metadata":{"id":"yexTzuexbLrg"},"source":["## Funciones para crear y dibujar los datos en el problema de los clusters elipsoidales"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNbZzY4EbLrh"},"outputs":[],"source":["def rota_datos_alrededor_origen(X, angulo):\n","    angle_rad = angulo*2.0*np.pi/360.0\n","\n","    s = np.sin(angle_rad)\n","    c = np.cos(angle_rad)\n","    matriz_rotacion = np.array([[c,-s],[s,c]])\n","    X_rotados = np.dot(matriz_rotacion, X) # se rotan los puntos generados\n","\n","    return X_rotados\n","\n","def genera_puntos_gaussiana2D(Npuntos, media, std1, std2, angulo):\n","    X = np.random.randn(2, Npuntos)\n","    X[0,:] = X[0,:] * std1 # se multiplica cada atributo por la\n","    X[1,:] = X[1,:] * std2 # desviación estándar deseada\n","    X = rota_datos_alrededor_origen(X, angulo)\n","    X[0,:] = media[0] + X[0,:]\n","    X[1,:] = media[1] + X[1,:]\n","\n","    return X.T\n","\n","def genera_puntos_disco2D(Npuntos, media, radio):\n","    radios  = radio*np.sqrt(np.random.rand(Npuntos))\n","    angulos = (2*np.pi)*np.random.rand(Npuntos)\n","    X = np.zeros((Npuntos,2))\n","    X[:,0] = media[0] + radios*np.cos(angulos)\n","    X[:,1] = media[1] + radios*np.sin(angulos)\n","    return X\n","\n","def plot_ellipsoid(mean, cov, color, splot, alpha=0.5):\n","    v, w = linalg.eigh(cov)\n","    u = w[0] / linalg.norm(w[0])\n","    angle = np.arctan(u[1] / u[0])\n","    angle = 180 * angle / np.pi  # convert to degrees\n","    ell = Ellipse(mean, v[0]*2, v[1]*2, 180 + angle, color=color)\n","    ell.set_clip_box(splot.bbox)\n","    ell.set_alpha(alpha)\n","    splot.add_artist(ell)\n","\n","def plot_ellipsoid_from_data(X, color, splot, alpha=0.5):\n","    plot_ellipsoid(np.mean(X.T, axis=1), np.cov(X.T), color, splot, alpha)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0It_S-1TbLri"},"outputs":[],"source":["problema = 6\n","\n","if problema==1:\n","    Nblobs = 3 # número de \"blobs\"\n","    X, y = make_blobs(random_state = 21,  # si cambiamos este parámetro cambiarán los \"blobs\" de ubicación\n","                      n_samples = 1000,   # número de puntos que serán generados en la base de datos\n","                      n_features = 2,     # número de dimensiones\n","                      centers = Nblobs,   # número de \"blobs\" (nubes)\n","                      cluster_std = 0.75) # ancho de cada \"blob\"\n","\n","elif problema==2: # blobs de diferente tamanyo\n","    Nblobs = 3\n","    stds = [0.1, 3, 0.1]\n","    X, y = make_blobs(random_state = 21,  # si cambiamos este parámetro cambiarán los \"blobs\" de ubicación\n","                      n_samples = 2000,   # número de puntos que serán generados en la base de datos\n","                      n_features = 2,     # número de dimensiones\n","                      centers = Nblobs,   # número de \"blobs\" (nubes)\n","                      cluster_std = stds) # ancho de cada \"blob\"\n","\n","elif problema==3: # dos \"lunas\" acopladas\n","    X = make_moons(n_samples=2000, noise=.1, random_state=21)[0]\n","\n","elif problema==4: # círculo dentro de un círculo\n","    X = make_circles(n_samples=2000, factor=0.5, noise=.05, random_state=21)[0]\n","\n","elif problema==5: # \"Mickey Mouse\"\n","    n_samples = 1000\n","    X1 = genera_puntos_disco2D(n_samples, [0, 5], 2)\n","    X2 = genera_puntos_disco2D(n_samples, [10, 5], 2)\n","    X3 = genera_puntos_disco2D(int(5*5/2/2*n_samples), [5, 0], 5)\n","    Xs = (X1, X2, X3)\n","    X = np.concatenate(Xs)\n","\n","elif problema==6:\n","    n_samples = 1000\n","    #np.random.seed(0)\n","    X1 = genera_puntos_gaussiana2D(n_samples, [0, 0], 1, 1, 45)\n","    X2 = genera_puntos_gaussiana2D(n_samples, [10, 0], 3.5, 1, 45)\n","    X3 = genera_puntos_gaussiana2D(n_samples, [5, 5], 2, 1, 45)\n","    Xs = (X1, X2, X3)\n","    X = np.concatenate(Xs)\n","    colors = ['lightgreen', 'orange', 'yellow', 'magenta', 'lightblue']\n","    markers = ['s', 'v', 'o', 'd', 's']\n","\n","\n","    plt.figure(figsize=(6,6))\n","    splot = plt.subplot(1, 1, 1)\n","    for Xi,color in zip(Xs,colors):\n","        plot_ellipsoid_from_data(Xi, 'b', splot, alpha=0.2)\n","\n","plt.scatter(X[:,0], X[:,1], color = 'b', s=2, alpha=0.1)\n","plt.grid()\n","plt.axis('equal')\n","plt.title('Datos sintéticos (no etiquetados)');"]},{"cell_type":"markdown","metadata":{"id":"UmtDQkDJbLrj"},"source":["Ahora usaremos el algoritmo de clustering k-means.\n","El parámetro principal es el número de clusters. Los otros parámetros se comentan en la celda siguiente y se visualizan los resultados obtenidos por este algoritmo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z81-LlN6bLrk"},"outputs":[],"source":["X.mean(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4HZZa1YbLrk"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","scaler.fit(X)\n","X = scaler.transform(X)\n","print(np.mean(X,axis=0))\n","print(np.std(X,axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8Bq84hwbLrl"},"outputs":[],"source":["Nclusters = 3    # número de clusters\n","Nrepetitions = 10 # número de repeticiones (para evitar mínimos locales). Número mínimo: 1\n","kmeans = KMeans(n_clusters=Nclusters,\n","                init='random', # random o kmeans++ (más eficiente)\n","                n_init=Nrepetitions,\n","                max_iter=500,\n","                random_state=2)\n","kmeans.fit(X)\n","\n","ax = plt.subplot(1,1,1)\n","colors = ['lightgreen', 'orange', 'yellow', 'magenta', 'lightblue', 'darkblue']\n","markers = ['s', 'v', 'o', 'd', 's']\n","for i in range(Nclusters):\n","    plt.scatter(X[kmeans.labels_ == i, 0],\n","                X[kmeans.labels_ == i, 1],\n","                s = 60, c = colors[i], marker = markers[i],\n","                label = 'cluster %d' % (i))\n","\n","plt.scatter(kmeans.cluster_centers_[:,0],\n","            kmeans.cluster_centers_[:,1],\n","            s = 100, marker = 'd',\n","            c='red', label='centroides')\n","plt.legend()\n","plt.grid()\n","plt.axis('equal')\n","plt.title('clustering k-means con %d clusters' % Nclusters)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"mKnQhhIcbLrl"},"source":["Uno de los problemas principales es cómo ajustar el número de clusters. Hay muchas estrategias. Una es usando el índice de calidad de clustering \"Silhouette\" para encontrar este número. Otra es usar otra métrica como el score de Calinski-Harabasz. La siguiente celda implementa estas estrategias:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hRc5eCobLrl"},"outputs":[],"source":["Nclusters_max = 15\n","Nrepetitions = 10\n","#qmetric = metrics.silhouette_score\n","qmetric = metrics.calinski_harabasz_score\n","\n","qualities = []\n","inertias = []\n","models = []\n","for k in range(1,Nclusters_max+1):\n","    kmeans = KMeans(n_clusters=k,\n","                    init='k-means++', n_init=Nrepetitions,\n","                    max_iter=500, random_state=1)\n","    kmeans.fit(X)\n","    models.append(kmeans)\n","    inertias.append(kmeans.inertia_)\n","    if k >1:\n","        qualities.append(qmetric(X, kmeans.labels_))\n","    else:\n","        qualities.append(0)\n","\n","fig = plt.figure(figsize=(15,4))\n","\n","ax = plt.subplot(1,3,1)\n","plt.plot(range(1,Nclusters_max+1), inertias, marker='o')\n","plt.xlabel('número de clusters')\n","plt.title('Inertia del clustering (SSE)')\n","\n","ax = plt.subplot(1, 3, 2)\n","plt.plot(range(1,Nclusters_max+1), qualities, marker='o')\n","plt.xlabel('número de clusters')\n","plt.title('Calidad del clustering')\n","\n","ax = plt.subplot(1, 3, 3)\n","best = pd.Series(qualities).idxmax() # encuentra el índice del mejor modelo\n","kmeans = models[best]\n","n_clusters = kmeans.get_params()['n_clusters']\n","for i in range(n_clusters):\n","    inds = np.where(kmeans.labels_ == i)[0]\n","    plt.scatter(X[inds, 0],\n","                X[inds, 1],\n","                s = 60,\n","                c = colors[i%len(colors)], marker = markers[i%len(markers)],\n","                label = 'cluster %d' % (i))\n","\n","plt.legend()\n","plt.grid()\n","plt.axis('equal')\n","plt.tight_layout()\n","plt.title('Clustering óptimo')\n","plt.show()\n","\n","print('Número de clusters óptimo: %d' % n_clusters)"]},{"cell_type":"markdown","metadata":{"id":"6at-n2tzbLrm"},"source":["Prueba diferentes parámetros y problemas sintéticos para comprender cómo funciona k-means en estos problemas de dos dimensiones.\n","\n","- ¿Descubre siempre k-means los clusters \"reales\"?\n","- ¿Qué ocurre si los \"blobs\" están \"demasiado\" cerca?\n","- ¿Encuentra siempre la estrategia basada en el índice Silhouette el número \"real\" de clusters?\n","- ¿Y la basada en el índice Calinski-Harabasz?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPQsgShjbLrm"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}